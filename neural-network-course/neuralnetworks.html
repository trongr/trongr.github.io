<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Neural Networks</title>
<link rel="stylesheet" href="../css/global.css">

<!-- this config must be before MathJax.js: -->
<script src="../js/mathjax.config.js"></script>
<script src="../js/MathJax/MathJax.js"></script>
<!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script> -->

<script src="../js/jquery-3.1.0.min.js"></script>
<script src="../js/smartquotes.js"></script>
<script src="../js/global.js"></script>

</head>
<body>
<div id="content">

<h1>Neural Networks</h1>

<div class="epigraph">
    <img src="images/tumblr_o93093O4Uo1qzbd7vo1_500.jpg">
    <div class="quote"><span class="normal">Pipey.</span>  Looks like
    you want to compress a movie file, can I help? You know with Pied
    Piper's revolutionary neural network optimized sharded data
    distribution system, it's just six clicks away. Follow
    meeee!</div><hr>
    <div class="author">Silicon Valley</div>
</div>

<h1>Biological inspiration / biomimicry</h1>

<div class="bside">
    <img src="images/neuron.png">
    <img src="images/neuron_model.jpeg">
</div>

<h1>Perceptron Learning Algorithm</h1>

<h1>Binary Threshold Neurons / McCulloch-Pitts</h1>

<p><b>Question.</b> <i>Used in Perceptrons. Also called Linear Threshold Neurons?</i></p>

<div class="bside">
    <img src="images/mccullochpitts.jpg">
</div>
<p><b>Definition.</b> <i>First compute $z = w^T x,$ then output $$y = \begin{cases}
1 \quad\text{ if } z \geq 0 \\
0 \quad\text{ otherwise,}
\end{cases}$$ representing "all VS none" activation. Here we implicitly added the threshold as a bias unit: $w = (b, w_1, w_2, \ldots).$
The function $y(z)$ is also called the Heaviside / unit step function.
</i></p>

<h1>Limitations of the Binary Threshold Neuron</h1>

<h1>Group Invariance Theorem</h1>

<div class="bside">
    <img src="images/linSepXor.png">
</div>

<p><b>Proposition.</b> <i>A single binary threshold neuron cannot
learn the XOR function, because geometrically its truth table
represented on a plane is not linearly separable.</i></p>

<p>
    <b>Proposition.</b> <i>Perceptrons can't learn patterns if they're
    subject to transformations that form a group, e.g. translations
    with wrap-around.</i>
</p>

<p>TODO. Details?</p>

<h1>Linear Neuron Learning Algorithm</h1>

<p><b>Definition.</b> <i>Given a training case $x_n$ and a weight
vector $w,$ the neuron's estimate $y_n$ of the desired output is $$y_n
= \sum_{i}^{} w_i x_{ni} = w^T x_n.$$ Define the cost function $E_n$
to be the squared difference error $$E_n = \frac{1}{2}(t_n - y_n)^2,$$
where $t_n$ is the target output, i.e. the "ground truth", and
define the total error to be $$E = \sum_{n}^{} E_n.$$ Finally the goal
of learning is to minimize $E$: $$\min_{w} E.$$</i></p>

<h1>Delta Rule: Learning by Gradient Descent</h1>

<p>The error partials are $$\frac{\partial E}{\partial w_i} = \sum_n
\frac{dE_n}{dy_n} \frac{\partial y_n}{\partial w_i} = - \sum_n (t_n -
y_n) x_{ni}.$$ The Delta Rule / Gradient Descent says that we should
change $w_i$ in the opposite direction as the change in error along
$w_i,$ give or take a learning rate $\alpha$: $$\Delta w_i = - \alpha
\frac{\partial E}{\partial w_i} = \sum_n \alpha (t_n - y_n) x_{ni},$$
i.e. $\alpha$ tells us how much to change, and the negative sign tells
us which direction to go, namely the opposite direction. E.g. if
$\frac{\partial E}{\partial w_i} > 0,$ that means the error goes up as
$w_i$ increases, so we want to decrease $w_i$ to make it go down, and
vice versa.</p>

<h1>Error Surface of a Linear Neuron</h1>

<div class="bside">
    <img src="images/slowlearrning.jpeg">
</div>

<p><b>Question.</b> <i>If I recall correctly feature normalization
should help with slow learning due to unscaled data? What about
pathological cases like this?</i></p>

<h1>Logistic Neurons</h1>

<h1>Learning Rule</h1>

<div class="bside">
    <img src="images/Logistic-curve.png">
    A logistic curve
</div>
<p><b>Definition.</b> <i>The estimator for a logistic neuron is given by
$$y = \frac{1}{1 + e^{-z}}$$ where $z = w^T x.$ The function $y(z)$ is also known as a logistic / sigmoid function, and $z$ is sometimes called the logit. As before, the error is the squared difference
$$E = \frac{1}{2} \sum_n (t_n - y_n)^2.$$
</i></p>

<p><b>Proposition.</b> <i>The estimator derivatives are
$$\frac{\partial y}{\partial w_i} = \frac{dy}{dz} \frac{\partial z}{\partial w_i}
= y(1 - y) x_i,$$ and so the error derivatives are
$$\frac{\partial E}{\partial w_i} = \sum_n \frac{dE_n}{dy_n} \frac{\partial y_n}{\partial w_i} = - \sum_n (t_n - y_n)(1 - y_n) y_n x_{ni}.$$
</i></p>

<h1>Learning with Hidden Units</h1>

<h1>Simplifying Notations</h1>

<div class="bside">
    <img src="images/backprop.png">
</div>
<p>In a neural networking using logistic neurons with hidden layers,
the neurons $y_n$ are arranged in layers, with neurons in each layer
receiving input from every neuron in the layer below, and outputting
to every neuron in the layer above it. To simplify notations, $y_i$ is
used to denote any neuron in a fixed row $i,$ and $y_j$ to denote any
neuron in the layer $j$ above it, i.e. we're reusing $j$ to refer to
both a layer and a particular neuron in that layer.</p>

<p>The weights controlling how neurons in row $i$ act on neurons in row
$j$ are $w_{ij},$ where $i$ ranges over the neurons in row $i$ and $j$
ranges over neurons in row $j.$ Furthermore, $w_{ij}$ is also used to
denote the weight of neuron $y_i$ on neuron $y_j,$ so that e.g. the
logit for neuron $y_j$ is $$z_j = w_{-} y_{-} + \cdots + w_{ij} y_{i}
+ \cdots + w_{-} y_{-},$$ where each unnamed $w_{-} y_{-}$ is a weight
and neuron in row $i,$ and we're only interested in the weight and
neuron $w_{ij} y_{i},$ so we give them names $ij$ and $i.$</p>

<h1>Backpropagation Algorithm</h1>

<p>Given an error $E$ and a training case $x = y_1,$ we want to
compute the errors of $E$ due to $y_n$ for all neurons $y_n,$ i.e. we
want to compute $\frac{\partial E}{\partial w_{ij}}$ for all $ij.$
With these partial derivatives calculated over all training cases, we
can then apply Gradient Descent or some other optimization algorithm
to <i>compute</i> the minimum weights $W.$
</p>

<p><b>Question.</b> <i>Is there any other optimization algorithm?
Numerical ones?</i></p>

<p>Keep in mind that there is a weight matrix $w$ for each pair of
layers $i$ and $j,$ where each entry $w_{ij}$ corresponds to the
connection between neuron $y_i$ in layer $i$ and neuron $y_j$ in layer
$j.$ Therefore the weights of all connections together form a
three-dimensional matrix $W.$</p>

<h1>Analytic Solution vs Random Perturbation</h1>

<p>An alternative to using Backpropagation is to randomly perturb the
weights and see how each change affects the error (a kind of
reinforcement learning), but that's a lot slower than solving for the
optimal solution analytically, since you have to test a single weight
change on <i>all</i> training cases to see if that change made an
improvement on the error.</p>

<h1>Deriving the Error Derivatives</h1>

<p><b>Definition.</b> <i>For quick reference, once again the logit and estimator of neuron $y_j$ in layer $j$ are:
\begin{align*}
z_j &= w^T x = w_- y_- + \cdots + w_{ij} y_i + \cdots + w_- y_- \\
y_j &= \frac{1}{1 + e^{-z_j}}.
\end{align*}
And the total error of the neural network is:
$$E = \frac{1}{2} \sum_n (t_n - y_n)^2.$$</i></p>

<hr>
<p><b>Proposition.</b> <i>The error derivatives for a logistic neural network with hidden units are:
\begin{align}
\frac{\partial E}{\partial z_j} &= \frac{\partial E}{\partial y_j} \frac{dy_j}{dz_j} = y_j (1 - y_j)  \frac{\partial E}{\partial y_j} \\
\frac{\partial E}{\partial y_i} &= \sum_j \frac{\partial E}{\partial z_j} \frac{\partial z_j}{\partial y_i} = \sum_j w_{ij} \frac{\partial E}{\partial z_j} \\
\frac{\partial E}{\partial w_{ij}} &= \frac{\partial E}{\partial z_j} \frac{\partial z_j}{\partial w_{ij}} = y_i \frac{\partial E}{\partial z_j}.
\end{align}
</i></p>
<hr>

<p>These equations are simple applications of the Chain Rule. But what
could they possibly mean? It's kinda like a recursive Chain Rule with
many interdependent variables.</p>

<h1>Meaning of the Error Derivatives and How to use them</h1>

<p>At layer $i,$ consider $E$ as a function of the $z_j$'s in layer
$j.$ Since $z_j$ depend on $y_i,$ by the Chain Rule we have the second
equation: $$\frac{\partial E}{\partial y_i} = \sum_j \frac{\partial
E}{\partial z_j} \frac{\partial z_j}{\partial y_i} = \sum_j w_{ij}
\frac{\partial E}{\partial z_j}.$$ To calculate $ \frac{\partial
E}{\partial z_j},$ we use equation (1): $$\frac{\partial E}{\partial
z_j} = \frac{\partial E}{\partial y_j} \frac{dy_j}{dz_j} = y_j (1 -
y_j) \frac{\partial E}{\partial y_j},$$ which requires $\frac{\partial
E}{\partial y_j},$ so we use equation (2) again to level up, and so on
until the top output layer.</p>

<p>In practice we'd start at the top and calculate down, and along the
way down we use the third equation to calculate $\frac{\partial
E}{\partial w_{ij}}.$ Once we have all of those we use the Delta Rule
to calculate the necessary weight changes: $$\Delta w_{ij} = - \alpha
\frac{\partial E}{\partial w_{ij}}.$$</p>

<h1>Optimization Issues</h1>

<p><b>Question.</b> <i>How often to update weights: online, full
batch, mini batch.</i></p>

<p><b>Question.</b> <i>How much to update weights via $\alpha$
learning rate: fixed, adaptive, adaptive per connection, alternatives
to Steepest Descent.</i></p>

<h1>Generalization Issues / overfitting</h1>

<p>Ways to reduce overfitting: weight decay, weight sharing, early
stopping, model averaging, Bayesian fitting of neural nets, dropout,
generative pre-training.</p>

<h1>Application: learning to predict the next word</h1>

<p><b>Example.</b> Learn $ArB$ to guess $B$ given $A$ and $r.$</p>

<p><b>Example.</b> Another application of $ArB$ is to guess the
probability of $ArB$ being correct given $A, r,$ and $B.$ Need both
correct and incorrect training cases for learning.</p>

<h1>Digression: Philosophy of Learning</h1>

<p>Feature theory: a concept is a set of semantic
features. Structuralist theory: meaning of a concept lies in its
relationships to other concepts.</p>

<p>Hinton: why not both?</p>

<p><b>Question.</b> <i>How to implement relational knowledge in a
neural net? Maybe many neurons per concept and one neuron reused in
multiple concepts.</i></p>

<p><b>Question.</b> <i>Why not all neurons in all concepts? Maybe
cause then you'd have no relations? Kinda woolly at the
moment.</i></p>

<h1>Digression: Softmax Output Function and Cross-Entropy Cost Function</h1>

<div class="bside">
    <img src="images/softmaxgroup.png">
</div>
<p><b>Definition.</b> <i>A softmax group $G$ is a group of output neurons whose outputs use the softmax activation function defined by $$y_i = \frac{e^{z_i}}{\sum\limits_{j\in G} e^{z_j}},$$ so that the outputs sum to $1.$ The cost function is given by $$C = - \sum_j t_j \log y_j.$$
</i></p>

<hr>

<p><b>Proposition.</b> <i>By the Quotient Rule, the derivatives are
\begin{align*}
\frac{\partial y_i}{\partial z_i} &= y_i(1 - y_i) \\
\frac{\partial y_i}{\partial z_j} &= -y_i y_j,
\end{align*}
or more fancy-like using the Kronecker Delta:
$$\frac{\partial y_i}{\partial z_j} = y_i(\delta_{ij} - y_j).$$
</i></p>

<hr>

<p><b>Proposition.</b> <i>The derivatives of the cost function are:
$$\frac{\partial C}{\partial z_i} = y_i - t_i.$$
</i></p>

<hr>

<p><i>Proof.</i> Apply the Chain Rule:
$$\frac{\partial C}{\partial z_i} = - \sum_j t_j \frac{\partial \log y_j}{\partial z_i} = - \sum_j t_j \frac{\partial \log y_j}{\partial y_j} \frac{\partial y_j}{\partial z_i}.$$
Using our formula for $\frac{\partial y_j}{\partial z_i},$ we get
$$\frac{\partial C}{\partial z_i} = - \sum_j \frac{t_j}{y_j} y_j(\delta_{ij} - y_i) = - \sum_j t_j (\delta_{ij} - y_i).$$
Recall that this is a multiclass classification problem, and so exactly one of the $t_j$'s is 1 and the rest are zero. Therefore:
$$\frac{\partial C}{\partial z_i} = - t_i (1 - y_i) + \sum_{j\neq i} t_j y_i = - t_i + y_i \sum_j t_j = y_i - t_i.$$
</p>

<h1>Application: Multiclass Classification using Softmax and Cross Entropy</h1>

<p>Suppose an input $x$ belongs to class $i \in G.$ That means $t_i =
1$ and $t_j = 0$ for all other $j \in G.$ It also means that
$y_i$---which represents the probability of the input belonging to
class $i$---should be high, and the cost $$C = - \sum_j t_j \log y_j = - \log y_i$$
encapsulates that idea: if $y_i$ is high, then we've guessed correctly
and $C$ is low; otherwise $y_i$ is low, and we've guessed incorrectly
and $C$ should be high---hence the negative sign.</p>

<p><b>Question.</b> <i>Why $\log$?  Maybe cause it gives nice derivatives like
$\frac{\partial C}{\partial z_i} = y_i - t_i.$</i></p>

<p>The reason we use $\log$ is that when the answer is $t_i = 1,$ and
we guess $y_i$ very close to zero, i.e. we've guessed very wrong, we
want the cost to be very high, and we can see that from the graph of
log: when $y_i$ is close to zero, $\log y_i$ is very negative and so
$- \log y_i$ is very large, i.e. it's a big cost. And vice versa, if
the answer is $1$ and we guess close to 1, then the cost is very
nearly zero.</p>

<div class="bside">
    <img src="images/figure_logarithmic_function.png">
</div>

<p><b>Question.</b> <i>What happens when $t_i = 0,$ and we guess
right with $y_i$ close to zero, or we guess wrong with $y_i$ close to
one? The cost function $C$ seems to ignore those guesses. Shouldn't we
punish those neurons that produce false positives and award ones that
produce true negatives?</i></p>

<h1>Application: Speech Recognition: word prediction</h1>

<div class="epigraph">
    <div class="quote">People use their understanding of the meaning
    of the utterance to hear the right words. We do this unconsciously
    when we wreck a nice beach.</div><hr>
    <div class="author">Geoffrey Hinton</div>
</div>

<p><b>Problem.</b> <i>Speech recognizers have to know which words are likely to come next
and which are not.</i></p>

<h1>Trigram method</h1>

<p>Take a huge amount of text and count frequencies of all triples of
words. Use these frequencies to calculate $$\frac{p(c|a,b)}{p(d|a,b)}
= \frac{\mathrm{count}(abc)}{\mathrm{count}(abd)}.$$</p>

<p>Fall back to digram if trigram frequencies too low.</p>

<p><b>Drawback.</b> Trigram doesn't understand similarity between words.</p>

<p>Need to do extra readings on the rest of the speech recognition
lectures. Not a lot of details there.</p>

<div class="epigraph">
    <div class="quote">
        I will travel across the land,<br>
        Searching far and wide,<br>
        Each pokemon to understand<br>
        The power that's inside.<br>
        Pokemon! Gotta catch 'em all!
    </div><hr>
    <div class="author">Pokemon</div>
</div>

<p>
    Turns out it's part of the assignment:
</p>

<h1>Assignment 1: Learning Distributed Word Representations</h1>

<p>Gonna use the Winter 2015 version of CSC321 cause they use Python instead of MATLAB.</p>

<p><b>Problem.</b> <i>Predict a word given a sequence of preceding
words. Will need to build a neural net that can represent words
well.</i></p>

<figure>
    <img src="images/zacefron.png">
    <figcaption></figcaption>
</figure>

<h1>Network Architecture / Language Model</h1>

<div class="bside">
    <img src="images/neuralnetlanguagemodel.png">
</div>

<p>Let $K = 250$ be the number of words in our dictionary, $D = 16$ be
the number (also called the Embedding Dimension) of neurons for each
word in the Embedding Layer, and let $H = 180$ be the number of
neurons in the Hidden Layer. Note that the final output layer has $K$
neurons, with each output $y_k$ corresponding to the probability that
the $k$-th word is the answer we're looking for, i.e. the $k$-th word
is the word following the three input words.</p>

<h2>An example of trade off between space and time</h2>

<p>In the word embedding layer, we can just train one embedding model
instead of three, one for each word. So in total the trainable
parameters in the network is $$KD + (3DH + H) + (HK + K) = 58070,$$
with $H$ extra bias parameters for the hidden layer and $K$ bias
parameters for the output layer. On the other hand, if we wanted to
store all $4$-gram frequencies explicitly, then we'd need $$K^4 =
250^4 = 3906250000$$ entries in the lookup table.</p>

<h1>Mini-batch Backpropagation</h1>

<p>The general backpropagation algorithm remains the same, but since
our architecture is made up of different kinds of neurons (using
different activation functions: unweighted linear in the embedding
layer, logistic in the hidden layer, and softmax in the output layer)
our derivatives are slightly different. In addition, the starter
assignment code uses mini-batch, so we'll also show vectorized
implementations of the derivatives.</p>

<h2>Output Layer</h2>

<p>Let $y_j$ be a neuron in the output layer and $z_j$ be its
logit. Then the loss derivatives are the same as before:
$$\frac{\partial C}{\partial z_j} = y_j - t_j.$$ We can denote the
matrix form as a row vector
$$\frac{\partial C}{\partial Z_j} = Y_j - T_j \sim 1 \times K$$ of
length $K,$ where the subscript just tells us which layer it is, in
this case $j$ for output. Mini-batch requires that we calculate this
derivative for $m = 100$ training cases, say, so the loss derivative
$$\frac{\partial C}{\partial Z_j} = Y_j - T_j \sim m \times K$$ is in
fact an $m \times K$ matrix, where $K = 250$ is the number of neurons
in the output layer, which is also the number of words in our
dictionary.</p>

<p><b>Notation.</b> It's often pretty useful to check matrix dimensions, so we'll
denote them by $\sim.$</p>

<h2>Hidden Layer</h2>

<p>Let $y_h$ be a neuron in the hidden layer, $z_h$ be its logit, and
$w_{jh}$ be its weight to one of the output neurons $y_j.$ In
particular we have \begin{align*} z_j &= \sum_h w_{jh} y_h\\ y_h &=
\frac{1}{1 + e^{-z_h}}\\ z_h &= \sum_e w_{he} y_e \end{align*} where
$y_e$ and $w_e$ are some neuron and its weight in the embedding
layer. Then $$\frac{\partial C}{\partial z_h} = \frac{\partial
C}{\partial y_h} \frac{\partial y_h}{\partial z_h},$$ and apply the
Chain Rule, remembering that all the $z_j$'s in the output layer
depend on $y_h$: $$\frac{\partial C}{\partial z_h} = \frac{\partial
y_h}{\partial z_h} \sum_j \frac{\partial C}{\partial z_j}
\frac{\partial z_j}{\partial y_h} = y_h (1 - y_h) \sum_j w_{jh}
\frac{\partial C}{\partial z_j}.$$ Here we also used a very nice and
useful derivative so it's worth writing down:</p>

<hr>

<p><b>Proposition.</b> <i>If $y = \frac{1}{1 + e^{-z}},$ then $$\frac{\partial
y}{\partial z} = y (1 - y).$$ Sometimes we'll also confuse $d$ with
$\partial.$</i></p>

<hr>

<p>Back on topic: let $W_h$ be the $K \times H$ weight matrix from the
hidden layer to the output layer, so e.g. the first column of $W_h$
would be weights over the hidden layer neurons going into neuron 1 in
the output layer. Let $Y_h$ be the $m\times H$ matrix of activations
of the hidden layer for the $m$ mini-batch, so e.g. the first row of
$Y_h$ contains the $y_h$ values of the hidden layer neurons on the
first training case. Then the matrix form of $\frac{\partial
C}{\partial z_h}$ is $$\frac{\partial C}{\partial Z_h} = Y_h * (1 -
Y_h) * \left(\frac{\partial C}{\partial Z_j}W_h\right) \sim m \times
H,$$ where $*$ denotes element-wise multiplication.</p>

<p>The derivatives of $C$ with respect to the hidden to output weights
are $$\frac{\partial C}{\partial w_{jh}} = \frac{\partial C}{\partial
z_j} \frac{\partial z_j}{\partial w_{jh}} = y_h \frac{\partial
C}{\partial z_j}.$$ In matrix form, this becomes $$\frac{\partial
C}{\partial W_h} = \frac{\partial C}{\partial Z_j}^T Y_h \sim K \times
H.$$ This last formula needs a bit of explanation: recall that
$\frac{\partial C}{\partial Z_j}$ is an $m\times K$ matrix, and that
$Y_h$ is an $m\times H$ matrix. Therefore the entries of
$\frac{\partial C}{\partial W_h}$ are of the form
$$\left(\frac{\partial C}{\partial W_h}\right)_{jh} = \sum_m y_h
\frac{\partial C}{\partial z_j},$$ i.e. we're summing all the partial
derivatives $\frac{\partial C}{\partial w_{jh}}$ over the
mini-batch. This is because when we do Gradient Descent, the Delta
Rule says that $$\Delta w_{jh} = - \alpha \frac{\partial C}{\partial
w_{jh}}$$ for one training case, so the change in $w_{jh}$ over all
training cases is $$\Delta w_{jh} = - \alpha \sum_m \frac{\partial
C}{\partial w_{jh}}.$$ As a sanity check: $\frac{\partial C}{\partial
W_h}$ is $K \times H,$ the same size as $W_h.$</p>

<h2>Hidden Layer Bias Units</h2>

<p>Let $b_h$ be the bias unit in the hidden layer going into neuron
$y_j$ in the output layer. Then $$\frac{\partial C}{\partial b_h} =
\frac{\partial C}{\partial z_j} \frac{\partial z_j}{\partial b_h} =
\frac{\partial C}{\partial z_j},$$ since $\frac{\partial z_j}{\partial
b_h} = 1.$ Similar to the weight derivatives, we also want to sum
these over the $m$ training cases, so $$\frac{\partial C}{\partial
b_h} = \sum_m \frac{\partial C}{\partial z_j},$$

<div class="bside">
    <b>Notation.</b> Denote by $\textsum_\text{rows} A$ the sum of a
    matrix $A$ over its rows, so in particular it's a row
    vector. Similarly $\textsum_\text{cols} A$ is a column vector. In
    NumPy this is <code>sum(A, 0)</code> and <code>sum(A, 1)</code>
    respectively.
</div>
which in matrix form is

$$\frac{\partial C}{\partial B_h} =
\left(\textsum_\text{rows}\frac{\partial C}{\partial Z_j}\right)^T
\sim K \times 1.$$</p>

<p><b>Note.</b> An alternative to treating bias units separately from normal
neurons is to add a constant feature 1 in each input layer going in to
the next layer.</p>

<h2>Embedding Layer</h2>

<div class="bside">
    <b>* Why?</b>
</div>

<p>Similar to the previous layer, let $y_e$ be a neuron in the
embedding layer, $z_e$ be its logit, $w_{he}$ be its weight to neuron
$y_h$ in the hidden layer. There is one difference: in this layer the
starter code uses a linear activation function:<b>*</b> $$y_e = z_e = \sum_i
w_i y_i,$$ where $i$ ranges over the neurons per context word in the
input layer. In particular this means $\frac{\partial y_e}{\partial
z_e} = 1,$ and so $$\frac{\partial C}{\partial z_e} = \frac{\partial
C}{\partial y_e} \frac{\partial y_e}{\partial z_e} = \sum_h
\frac{\partial C}{\partial z_h} \frac{\partial z_h}{\partial y_e} =
\sum_h w_{he} \frac{\partial C}{\partial z_h}.$$ In matrix form this
is $$\frac{\partial C}{\partial Z_e} = \frac{\partial C}{\partial Z_h}
W_e \sim m \times 3D.$$</p>

<p><b>Question.</b> <i>Why is a linear activator better than other
activation functions for word embedding?</i></p>

<p>The weight derivatives are $$\frac{\partial C}{\partial w_{he}} =
\frac{\partial C}{\partial z_h} \frac{\partial z_h}{\partial w_{he}} =
y_e \frac{\partial C}{\partial z_h},$$ which in matrix form is
$$\frac{\partial C}{\partial W_e} = \frac{\partial C}{\partial Z_h}^T
Y_e \sim H \times 3D,$$ which is the same size as $W_e.$</p>

<h2>Embedding Layer Bias Units</h2>

<p>Let $b_e$ be the bias weight going into neuron $y_h$ in the hidden
layer. Then as usual $$\frac{\partial C}{\partial b_e} =
\frac{\partial C}{\partial z_h} \frac{\partial z_h}{\partial b_e} =
\frac{\partial C}{\partial z_h},$$ which has matrix form
$$\frac{\partial C}{\partial B_e} = \left(\textsum_{\text{rows}}
\frac{\partial C}{\partial Z_h}\right)^T \sim H \times 1.$$</p>

<h1>Some Unanswered Questions</h1>

<div class="aside">
    <img src="images/deeper.jpg">
</div>

<p><b>Question.</b> <i>What exactly is each layer doing?</i></p>

<p>From the bottom up: each word embedding encodes a word in a new
distributed representation. The hidden layer then learns those trigram
representations, i.e. it's encoding the entire trigram. Finally the
output layer associates each trigram with the fourth target word.</p>

<p><b>Question.</b> <i>OK, but how exactly are they doing it?</i></p>

<p>
    Let's take a closer look at what each layer is doing. From the top down:
</p>

<h2>Output layer</h2>

<div class="aside">
    <img src="images/words_hidden_to_output.png">
    Note that $w_j$ is used to denote a single weight vector going
    into $y_j,$ while $W_h$ denotes all weight vectors going into the
    output layer as a matrix.
</div>

<p>
    Let's assume a particular training case belongs in class $j,$ i.e.
    that $t_j = 1$ and $t_i = 0$ for all other $i$'s. In that case
    the cross entropy function

    $$C = - \sum_j t_j \log y_j$$

    is optimized, i.e. close to zero, if $\log y_j$ is close to zero,
    and therefore if $y_j$ is close to 1---which makes all the other
    $y_i$'s in the output layer close to 0, because we defined $y_i =
    \frac{e^{z_i}}{\sum\limits_{j\in G} e^{z_j}}$ so they would have
    unit probability measure.
</p>

<p>
    This means that $e^{z_j}$ and therefore

    $$z_j = \sum_h w_{jh} y_h = w_j^T Y_h$$

    must be large, where $y_h$ are the neurons in the hidden layer and
    $w_{jh}$ are their weights going into the output
    layer. Geometrically, this means that the weight vector $w_j =
    (w_{jh})$ must be close to the hidden layer activation $Y_h.$
</p>

<div class="box">
    <b>Aside.</b> This is a property of the inner / dot product of two
    vectors: vectors that are similar, i.e. close together, have
    roughly bigger dot products, and vectors that are farther apart
    have roughly smaller dot products. E.g.

    $$(10, 0) \cdot (11, 0) = 110$$

    is large positive, perpendicular vectors have zero dot product:

    $$(10, 0) \cdot (0, 10) = 0,$$

    while

    $$(10, 0) \cdot (-10, 0) = -100$$

    is small / large negative. You can see why this is so by
    considering the definition of the dot product for the case $n =
    1,$ and generalize component-wise.
</div>

<p>
    To summarize, to optimize $C,$ some weights in the hidden to
    output layer must move towards the activation $Y_h.$ In
    particular, if we feed the network many similar trigrams and
    target words, a certain set of weights $w_j$ in the hidden layer
    will move towards those activation vectors $Y_h$ and come to
    resemble their mean.
</p>

<h2>Hidden Layer / Trigram Embedding</h2>

<div class="bside">
    <img src="images/words_embed_to_hidden.png">
</div>

<p>
    In the previous layer, we assumed that the hidden layer faithfully
    represents trigram embeddings. If it didn't, training the network
    will have to take care of that. In other words, to optimize $C,$
    at the hidden layer we have to optimize trigram embedding, i.e. by
    training the embedding to hidden layer weights to make sure that
    correct inputs from the embedding layer map to correct hidden
    layer activation.
</p>

<p>
    The hidden layer activation

    $$y_h = \frac{1}{1 + e^{-z_h}}$$

    says that neuron $y_h$ is activated, i.e. close to 1, if

    $$z_h = \sum_e w_{he} y_e = w_h^T Y_e$$

    is large, if $w_h$ is close to $Y_e.$ In other words, in order to
    make a trigram map to the correct activation vector $Y_h,$ the
    weights $w_h$ for certain hidden neurons must move closer to the
    embedding activation $Y_e.$
</p>

<h2>Embedding layer</h2>

<div class="bside">
    <img src="images/words_input_to_embed.png">
</div>

<p>
    This layer is a little different because the activation is
    unweighted linear:

    $$y_e = z_e = \sum_k w_{ek} y_k = w_e^T Y_k,$$

    and in addition each module takes a separate word as input. But
    other than that, the idea is the same: $y_e$ is high if $w_e$ is
    similar to $Y_k,$ and so in order to properly represent a word
    $Y_k,$ certain weights $w_e$ must move towards the vector $Y_k.$
</p>

<p>
    <b>Note.</b> We've only talked about the high neurons and what
    happens to the weights going into them. The low neurons are also
    similar: the weights going into those must move away from the
    input activation.
</p>

<hr>

<p>
    Now to answer the question:
</p>

<p><b>Question.</b> <i>What does it mean when two words are close
together in the embedding layer, i.e. have similar distributed word
representations?</i></p>

<p>
    As we've seen, training the neural network simply means making a
    certain set of weights in a layer move closer to those inputs that
    give the right answer, while at the same time making the remaining
    weights move away from those inputs. So let's consider what
    happens when we train a network on a set of trigrams ABC whose
    target word is a fixed word D. The hidden to output weights $W_h$
    will want to map all of these ABC's to the same class D, so $W_h$
    will move towards their mean encoding. In the embedding layer, the
    embedding to hidden weights $W_e$ will want to map these word
    embeddings to the same trigram representation, so $W_e$ will move
    towards their mean encoding.
</p>

<h2>Position-aware Trigram!</h2>

<div class="bside">
    * Hinton mentioned earlier the inability to recognize relative
    position as one of the drawbacks of the trigram method without
    using a neural network, and this is the fix.
</div>

<p>
    In the input layer, the input to embedding weights $W_k$ will be
    slightly different: in addition to having to encode each word,
    $W_k$ will also have to encode the word's position in the trigram
    sequence. This is because we're making the embedding layer take
    the letters A, B, C as input in order, and so the weights $W_k$
    are aware of this ordering by definition.*
</p>

<div class="bside">
    <img src="images/Original_Doge_meme.jpg">
    t-SNE plot of Doge's vocabulary. Just kidding.
</div>

<p>
    Therefore the answer to the question is: proximity in embedding
    space represents how often words appear together, and in which
    order before a target word. E.g. if we feed the neural network the
    two phrases <i>wow Doge much cute</i> and <i>all cats are cute</i>
    over and over, the words <i>wow</i> and <i>Doge</i> will be close
    to each other because <i>wow</i> and <i>Doge</i> appear together
    often before the target word <i>cute</i>, as well as the
    words <i>Doge</i> and <i>cats</i> because <i>Doge</i>
    and <i>cats</i> appear often in the same position before
    <i>cute</i>.
</p>

<div class="bside">
    <img src="images/tsneplot.png">
    t-SNE plot of word embeddings
</div>

<p>
    <b>Note.</b> In natural language, positional proximity will have
    more of an effect than how often two words appear
    together. E.g. in the t-SNE plot above, the words <i>very</i>
    and <i>too</i> are very close together, because they appear in the
    same position (3rd) before the target word <i>much,</i> as
    in <i>something something very much</i> and <i>something something
    too much.</i> Aside from the target word <i>much,</i> they also
    appear in the same position (3rd) before other words,
    like <i>long, well, good,</i> etc., which is why <i>very</i>
    and <i>too</i> are closer to each other than <i>very</i>
    and <i>much,</i> relatively speaking.
</p>

<p>
    It's usually the 3rd word in a trigram that has its position
    learned and associated, because it's the word closest to the
    target word. It's rarer for the first and second words to be
    learned, because you rarely have 3 or 4-word long phrases that
    occur again and again.
</p>

<p><b>Question.</b> <i>How does using an unweighted linear activator
in the embedding layer differ from using say a logistic activator? Can
we replace one by the other and still achieve similar results?</i></p>

<p>
    They differ at least in how fast a neuron learns / changes its output
    when it's very high or very low, also called saturated: e.g. the
    logistic neuron

    $$y = \frac{1}{1 + e^{-z}}$$

    has derivative

    $$\frac{\partial y}{\partial z} = y(1 - y),$$

    which is close to zero when $y$ is either maxed out at 1 or min'd
    out at 0, meaning that it won't change much when $z$ changes,
    i.e. it learns slowly when saturated.
</p>

<hr>

<h1>How to Train your Neural Network using Gradient Descent</h1>

<img src="images/M6O8AbL.gif" style="width:100%">

<div class="bside">
    * Hey, eigenvalues!
</div>
<p>
    Symmetry breaking by randomly initializing weights. Mean normalize
    data. Use PCA to decorrelate input components / remove redundant
    data.*
</p>

<p>
    Momentum method: use gradients to change velocity of weight
    particle instead of its position. Adaptive learning rate per
    weight. rmsprop: need details.
</p>

<h1>Momentum Method</h1>

<p>
    <b>Intuition: rolling ball.</b> Modify the Delta Rule

    $$\Delta w(t) = - \e \frac{\partial E}{\partial w}(t)$$

    to include a "momentum" term:

    $$\Delta w(t) = \alpha \Delta w(t - 1) - \e \frac{\partial E}{\partial w}(t),$$

    where $\alpha$ is a factor slightly less than 1. All it's saying
    is that $\Delta w(t)$ remembers a little bit of its previous
    direction via $\alpha \Delta w(t - 1)$, and is pushed along the
    gradient as usual by $-\e \frac{\partial E}{\partial
    w}(t).$
</p>

<p>
    <b>Question.</b> <i>What's a good value for $\e$?</i>
</p>

<div class="bside">
    <img src="images/planerollingball.png">
</div>

<p>
    <b>Example.</b> As a simple example, suppose you have a planar
    error surface.
</p>

<p>
    Then the weights will reach a terminal velocity

    $$\Delta w_\infty = -\frac{1}{1 - \alpha} \e \frac{\partial E}{\partial w},$$

    which can be shown as follows: since this is a flat plane,
    $\frac{\partial E}{\partial w}(t)$ is constant, so recursively
    expand

    $$\Delta w(t) = \alpha \Delta w(t - 1) - \e \frac{\partial E}{\partial w}(t)$$

    to get

    $$\Delta w(t) = \alpha^t \Delta w(0) - (\alpha^{t-1} + \cdots + 1)\e \frac{\partial E}{\partial w}(t).$$

    The first term $\alpha^t \Delta w(0)$ goes to zero because $0 <
    \alpha < 1,$ and the second term contains the geometric series, so
    as $t\longrightarrow 0,$

    $$\Delta w(t) \longrightarrow \Delta w_\infty = -\frac{1}{1 - \alpha} \e \frac{\partial E}{\partial w}.$$
</p>

<p>
    <b>Corollary.</b> <i>If $\frac{\partial E}{\partial w}$ is
    bounded, then so is $\Delta w,$ i.e. $\Delta w$ doesn't
    accumulate and go all crazy.</i>
</p>

<h2>Nesterov Method</h2>

<p>
    <b>Old momentum method:</b> <i>calculate gradient, i.e. correct
    your previous mistake, at current point, then jump.</i>
</p>

<div class="bside">
    <img src="images/nesterov.png">
    Hinton. <i>It's better to correct a mistake after you've made
    it.</i>
</div>

<p>
    <b>New and improved momentum method:</b> <i>jump first, then
    correct your mistake at the destination.</i> This method is
    captured by the formula for $\Delta w$ above:

    $$\Delta w(t) = \alpha \Delta w(t - 1) - \e \frac{\partial E}{\partial w}(t).$$

    The corresponding formula for the old momentum method would be

    $$\Delta w(t) = \alpha \Delta w(t - 1) - \e \frac{\partial E}{\partial w}(t - 1).$$
</p>

<h1>Per Connection Adaptive Learning Rate</h1>

<p class="box">
    <b>Definition.</b> <i>Define a gain parameter $g_{ij}$ for each
    connection:

    $$\Delta w_{ij} = -\e g_{ij} \frac{\partial E}{\partial w_{ij}},$$

    and increase $g_{ij}$ if the gradient does not change signs, otherwise
    decrease it:

    $$g_{ij}(t) = \begin{cases}
    g_{ij}(t-1) + \alpha &\text{ if } \frac{\partial E}{\partial w_{ij}}(t) \frac{\partial E}{\partial w_{ij}}(t-1) > 0 \\
    g_{ij}(t-1) \times \beta &\text{ otw,}
    \end{cases}$$

    where $\alpha$ is close to $0$ and $\beta$ is close to $1,$
    e.g. $0.05$ and $0.95$ resp.</i>
</p>

<div class="aside">
    <img src="images/keep-calm-and-make-all-kinds-of-gains-12.png">
</div>

<p>
    <b>Question.</b> <i>Why keep the $\e$? Some kind of global gain / learning rate?</i>
</p>

<p>
    The effect of these gain parameters is to add damping when
    oscillation occurs, i.e. if the gradient changes sign often,
    $\Delta w_{ij}$ will become small, so it'll sway less. On the
    other hand, if the gradient doesn't change sign, the weight will
    keep going down hill and gain more and more speed, unless we bound
    gains. According to lecture reasonable gain limits are

    $$g_{ij} \in [0.1, 10] \quad \text{ or } \quad g_{ij} \in [0.01, 100].$$
</p>

<p>
    <b>Example.</b> Say $\frac{\partial E}{\partial w}$ changes sign
    each time. So e.g. (using subscripts to denote time):

    \begin{align*}
    g_1 &= g_0 + \alpha \\
    g_2 &= g_1 \times \beta = g_0 \beta + \alpha \beta \\
    g_3 &= g_2 + \alpha = g_0 \beta + \alpha \beta + \alpha \\
    &\cdots \\
    g_{2n} &= g_0 \beta^n + \alpha \beta^n + \cdots + \alpha \beta \\
    g_{2n+1} &= g_0 \beta^n + \alpha \beta^n + \cdots + \alpha.
    \end{align*}

    Therefore as $n\longrightarrow \infty,$ we have $g_0 \beta^n
    \longrightarrow 0$ and

    $$g_{2n} \longrightarrow \alpha \frac{1}{1 - \beta} = 1,$$

    i.e. if the gradient changes sign each time, the gain will settle
    around 1.
</p>

<p>
    Geometric series again! Keeps showing up cause we always use
    simple examples. Would be nice if we can see what happens when the
    gradient changes not each time, but randomly. In that case,

    \begin{align*}
     g_{n_{2k+1}} &= n_1 \alpha \beta^{n_2 + n_4 + \cdots + n_{2k}}
              + n_3 \alpha \beta^{n_2 + n_4 + \cdots + n_{2k-2}}
              + \cdots
              + n_{2k+1} \alpha \\
             &\approx \alpha n \left((\beta^n)^{N/2} + \cdots + \beta^n + 1\right) \\
             &\approx \alpha n \frac{1}{1 - \beta^n},
    \end{align*}

    where $n_i$ is the number of times the gradient keeps its sign on
    the $i$-th change of signs, $n$ is the expected value of the
    $n_i$'s, and $N$ is the total number of times the gradient changes
    signs. Since the gradient changes signs randomly, $n$ should on
    average be 1, and so this reduces to the value we had earlier:

    $$g_\infty = 1.$$
</p>

<h2>RPROP: resilient backpropagation</h2>

<p>
    <b>Definition.</b> <i>Instead of relying on the gradient and a
    learning rate as in the Delta Rule, RPROP keeps track of a single
    step size $\Delta_{ij}$ per weight:

    $$\Delta_{ij}(t) = \begin{cases}
    \eta^+ \Delta_{ij}(t-1) \quad &\text{ if } \frac{\partial E}{\partial w_{ij}}(t) \frac{\partial E}{\partial w_{ij}}(t-1) > 0 \\
    \eta^- \Delta_{ij}(t-1) \quad &\text{ if } \frac{\partial E}{\partial w_{ij}}(t) \frac{\partial E}{\partial w_{ij}}(t-1) < 0 \\
    \Delta_{ij}(t-1) \quad &\text{ else,}
    \end{cases}$$

    where $0 < \eta^- < 1 < \eta^+,$ usually $0.5$ and $1.2.$ Next, the weight update is

    $$\Delta w_{ij}(t) = \begin{cases}
    -\Delta_{ij}(t) &\quad \text{ if } \frac{\partial E}{\partial w_{ij}}(t) > 0 \\
    +\Delta_{ij}(t) &\quad \text{ if } \frac{\partial E}{\partial w_{ij}}(t) < 0 \\
    0 &\quad \text{ else. }
    \end{cases}$$

    In other words, if the gradient changes sign, then our last step
    size $\Delta_{ij}(t-1)$ was too big, so we need to scale it back
    by $\eta^-.$ On the other hand, if the gradient keeps its sign,
    then we're going in the right direction, and we scale up the step
    size by $\eta^+$ to go faster. Otherwise the gradient is zero and
    there's no need to change the weight.
    </i>
</p>

<p>
    <b>Note.</b> Even when $\frac{\partial E}{\partial w_{ij}}(t) =
    0,$ that doesn't mean we've reached a minimal $w_{ij},$ because at
    time $t+1$ the weights might move somewhere else and
    $\frac{\partial E}{\partial w_{ij}}(t+1)$ might be nonzero.
</p>

<p>
    <b>Disadvantage.</b> Only works in full batch, can't use in
    mini-batch because e.g. if a weight gets a gradient of $+0.1$ on
    nine mini-batches and $-0.9$ on the tenth, this weight should stay
    roughly where it is. But RPROP will increase it nine times and
    decrease it only once, so the step size will grow by a factor of

    $$1.2^9 \cdot 0.5 \approx 2.5,$$

    i.e. the frequency and magnitude of the mini batch gradients
    matter but the magnitude isn't reflected in RPROP.
</p>

<h2>RMSPROP: root mean square backpropagation / mini-batch RPROP</h2>

<div class="aside">
    <img src="images/contours_evaluation_optimizers.gif">
    Visualization of different optimization algorithms
</div>

<p>
    <b>Definition.</b> <i>Note that RPROP is equivalent to regular
    per-weight adaptive learning rate with the gradient magnitude
    normalized out by dividing by itself. So in order to make the
    magnitude relevant again, we divide successive gradients not by
    themselves individually but by their root mean square: first
    define the running mean square

    $$\m_{ij}(t) = \g \m_{ij}(t-1) + (1-\g) \left(\frac{\partial E}{\partial w_{ij}}\right)^2,$$

    where $\g$ is the forgetting factor: bigger $\g \in (0, 1)$ makes
    $\m_{ij}$ remember more of its previous values. Then the weight update is

    $$\D w_{ij} = - \frac{\a}{\sqrt{\m_{ij}}} \frac{\partial E}{\partial w_{ij}}.$$

    </i>
</p>

<p>
    <b>Random TODO.</b> Look up meta neural network.
</p>

<hr>

<div class="epigraph">
     <img src="images/ff512d50be094b0d3e06e813553e9842-650-80.jpg">
     <div class="quote">
          On your travels you may encounter some very powerful
          foes: return to them once you've gained more experience.
     </div><hr><div class="author">The Witcher 3</div>
</div>

<h1>TODO!</h1>

<h1>Object Recognition: Convolutional Neural Networks for Digit
Recognition</h1>

<h1>Replicated Features</h1>

<h1>Backpropagation with Weight Constraints</h1>

<h1>Assignment 2: Convolutional Neural Networks for Digit Recognition</h1>

<h1>Overview: Modeling Sequences</h1>

<h1>Nearest Neighbour Classifier</h1>

<div class="aside">
    Circles in Manhattan space: points equidistance from a
    center. There's also $L^2,$ which is just normal Euclidean
    distance, as well as $L^p.$
    <p>
        <b>Fun note.</b> By the triangle inequality:

        $$||x||_1 \geq ||x||_2.$$
    </p>
    <img src="images/214px-TaxicabGeometryCircle.svg.png">
</div>

<h2>Manhattan Distance</h2>

<p>
    <b>Definition: Manhattan Metric.</b> <i>The Manhattan, also known
    as $L^1$ distance between two vectors $v = (v_i)$ and $w = (w_i)$
    is

    $$d_1(v, w) = ||v - w||_1 = \sum_i |v_i - w_i|.$$</i>
</p>

<p>
    <b>Definition: accuracy.</b> <i>Fraction of accurate
    predictions.</i>
</p>

<h1>k-Nearest Neighbour Classifier</h1>

<h1>Reference</h1>

<ol>
<li>Geoffrey E. Hinton's Neural Networks video lectures.</li>
<li>http://www.cs.toronto.edu/~rgrosse/csc321/</li>
<li>http://www.cs.toronto.edu/~tijmen/csc321/</li>
<li>http://cs231n.stanford.edu/</li>
<li>Everything else from the web.</li>
</ol>

</div>
</body>
</html>
