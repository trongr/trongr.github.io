<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />
<title>Reinforcement Learning</title>
<link rel="stylesheet" href="../css/global.css" />
<link rel="stylesheet" href="../css/obsidian.min.css" />
<script src="../js/highlight.min.js"></script>
<!-- this config must be before MathJax.js: -->
<script src="../js/mathjax.config.js"></script>
<script src="../js/MathJax/MathJax.js"></script>
<!--
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
-->
<script src="../js/jquery-3.1.0.min.js"></script>
<script src="../js/smartquotes.js"></script>
<script src="../js/global.js"></script>
</head>
<body>
<div id="content">
<h1>Markov processes</h1>

<h2>Markov property</h2>

<p>
  <i>The future is independent of the past given the present, i.e. you only need
  to know the present to predict the future. Specifically, a state $ s_t $ is
  Markov if

      $$
        p(s_{t+1} | s_t, a_t) = p(s_{t+1} | h_t, a_t).
      $$

    In other words, if the state is Markov, then you don't need to know the full
    history in order to predict the future, only the current state.</i>
</p>

<p>
  <i>Example.</i> If we define the state $ s_t $ as the full history $ h_t $, then it is Markov.
</p>

<h2>Markov process or Markov chain</h2>

<p>
  <i>Definition. A Markov chain is a pair $ (S, P) $ where $ S $ is a finite set
  of states, and $ P $ is a dynamics / transition model that specifies the
  probability of moving from state $ s $ to $ s': $

  $$
    p(s_{t+1} = s' | s_t = s).
  $$
  </i>
</p>

<p>
  If $ S $ has $ N $ states, then one way to represent the dynamics model is as
  a transition matrix $ P $ of probabilities:

  $$
    P =
    \begin{bmatrix}
      p(s_{1}|s_{1}) &  p(s_{1}|s_{2}) & \cdots & p(s_{1}|s_{N}) \\
      p(s_{2}|s_{1}) & p(s_{2}|s_{2}) & \cdots  & p(s_{2}|s_{N}) \\
      \vdots  & \vdots  & \ddots  & \vdots \\
      p(s_{N}|s_{1}) & p(s_{N}|s_{2}) & \cdots  & p(s_{N}|s_{N})
    \end{bmatrix}.
  $$

  Given a one-hot encoding of states, e.g.

  $$
    s_1 = \begin{bmatrix}
    1 \\
    0 \\
    \vdots \\
    0
    \end{bmatrix}
  $$

  for the first state, we can calculate the probabilities that we'll move from $
  s_1 $ to the other states as

  $$
    \begin{align*}
    P s_1 &= \begin{bmatrix}
      p(s_{1}|s_{1}) &  p(s_{1}|s_{2}) & \cdots & p(s_{1}|s_{N}) \\
      p(s_{2}|s_{1}) & p(s_{2}|s_{2}) & \cdots  & p(s_{2}|s_{N}) \\
      \vdots  & \vdots  & \ddots  & \vdots \\
      p(s_{N}|s_{1}) & p(s_{N}|s_{2}) & \cdots  & p(s_{N}|s_{N})
    \end{bmatrix}

    \begin{bmatrix}
      1 \\
      0 \\
      \vdots \\
      0
    \end{bmatrix} \\

    &= \begin{bmatrix}
      p(s_{1}|s_{1}) \\
      p(s_{2}|s_{1}) \\
      \vdots  \\
      p(s_{N}|s_{1}) \\
    \end{bmatrix}.
    \end{align*}
  $$

  <i>Note.</i> Sometimes people write $ P $ transposed, with the rows denoting
  the starting state and the columns denoting the ending state, in which case
  you'll have to do some transposing, and multiplying $ P $ and $ s $ backwards.
</p>

<p>
  <i>Exercise. Implement a Markov chain by defining a transition matrix $ P $
  and sample episodes starting from some state $ s_1. $</i>
</p>

<h2>Markov reward process (MRP)</h2>

<h2>Markov models</h2>

<h3>Full observability: Markov Decision Process (MDP)</h3>

<p>
  We can also define the state $ s_t = o_t, $ i.e. we're assuming that the
  current observation is enough to predict the future, i.e. what we're seeing
  right now gives us complete information about the world (e.g. as in chess).
</p>

<h3>Partial observability: Partially Observable Markov Decision Process (POMDP)</h3>

<p>
  Here the agent's state is not the same as the world state, because the agent
  doesn't have complete knowledge of the world (e.g. as in poker) and has to
  build its own model from observables, e.g. it might use the full history $ s_t
  = h_t, $ beliefs of world state, RNN, etc.
</p>

<h3>Bandits</h3>

<p>
  In a bandit game, actions have no influence on future observations, e.g.
  pulling on a slot machine does not affect how the slot machine will respond in
  the future. In addition, there is no delayed reward: the slot machine rewards
  immediately.
</p>

<p>
  In MDP's and POMDP's, actions influence future observations, and there may be
  delayed rewards, which makes credit assignment difficult (which previous
  actions caused the delayed reward).
</p>

</div>
</body>
</html>
