<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />
<title>Reinforcement Learning</title>
<link rel="stylesheet" href="../css/global.css" />
<link rel="stylesheet" href="../css/obsidian.min.css" />
<script src="../js/highlight.min.js"></script>
<!-- this config must be before MathJax.js: -->
<script src="../js/mathjax.config.js"></script>
<script src="../js/MathJax/MathJax.js"></script>
<!--
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
-->
<script src="../js/jquery-3.1.0.min.js"></script>
<script src="../js/smartquotes.js"></script>
<script src="../js/global.js"></script>
</head>
<body>
<div id="content">
<h1>Markov processes</h1>

<h2>Markov property</h2>

<p>
  <i>The future is independent of the past given the present, i.e. you only need
  to know the present to predict the future. Specifically, a state $ s_t $ is
  Markov if

      $$
        p(s_{t+1} | s_t, a_t) = p(s_{t+1} | h_t, a_t).
      $$

    In other words, if the state is Markov, then you don't need to know the full
    history in order to predict the future, only the current state.</i>
</p>

<p>
  <i>Example.</i> If we define the state $ s_t $ as the full history $ h_t $, then it is Markov.
</p>


<h2>Full v.s. partial observability</h2>

<p>
  We can  define the state $ s_t = o_t, $ i.e. we're assuming that the current
  observation is enough to predict the future, i.e. what we're seeing right now
  gives us complete information about the world (e.g. as in chess).
</p>

<p>
  In partially observable Markov processes, the agent's state is not the same as
  the world state, because the agent doesn't have complete knowledge of the
  world (e.g. as in poker) and has to build its own model from observables, e.g.
  it might use the full history $ s_t = h_t, $ beliefs of world state, RNN, etc.
</p>

<p>
  In MDP's and POMDP's, actions influence future observations, and there may be
  delayed rewards, which makes credit assignment difficult (which previous
  actions caused the delayed reward).
</p>

<h2>Bandits</h2>

<p>
  In a bandit game, actions have no influence on future observations, e.g.
  pulling on a slot machine does not affect how the slot machine will respond in
  the future. In addition, there is no delayed reward: the slot machine rewards
  immediately.
</p>


<h2>Markov process or Markov chain</h2>

<p>
  <i>Definition. A Markov chain is a pair $ (S, P) $ where $ S $ is a finite set
  of states, and $ P $ is a dynamics / transition model that specifies the
  probability of moving from state $ s $ to $ s': $

  $$
    p(s_{t+1} = s' | s_t = s).
  $$
  </i>
</p>

<p>
  If $ S $ has $ N $ states, then one way to represent the dynamics model is as
  a transition matrix $ P $ of probabilities:

  $$
    P =
    \begin{bmatrix}
      p(s_{1}|s_{1}) &  p(s_{1}|s_{2}) & \cdots & p(s_{1}|s_{N}) \\
      p(s_{2}|s_{1}) & p(s_{2}|s_{2}) & \cdots  & p(s_{2}|s_{N}) \\
      \vdots  & \vdots  & \ddots  & \vdots \\
      p(s_{N}|s_{1}) & p(s_{N}|s_{2}) & \cdots  & p(s_{N}|s_{N})
    \end{bmatrix}.
  $$

  Given a one-hot encoding of states, e.g.

  $$
    s_1 = \begin{bmatrix}
    1 \\
    0 \\
    \vdots \\
    0
    \end{bmatrix}
  $$

  for the first state, we can calculate the probabilities that we'll move from $
  s_1 $ to the other states as

  $$
    \begin{align*}
    P s_1 &= \begin{bmatrix}
      p(s_{1}|s_{1}) &  p(s_{1}|s_{2}) & \cdots & p(s_{1}|s_{N}) \\
      p(s_{2}|s_{1}) & p(s_{2}|s_{2}) & \cdots  & p(s_{2}|s_{N}) \\
      \vdots  & \vdots  & \ddots  & \vdots \\
      p(s_{N}|s_{1}) & p(s_{N}|s_{2}) & \cdots  & p(s_{N}|s_{N})
    \end{bmatrix}

    \begin{bmatrix}
      1 \\
      0 \\
      \vdots \\
      0
    \end{bmatrix} \\

    &= \begin{bmatrix}
      p(s_{1}|s_{1}) \\
      p(s_{2}|s_{1}) \\
      \vdots  \\
      p(s_{N}|s_{1}) \\
    \end{bmatrix}.
    \end{align*}
  $$

  <i>Note.</i> Sometimes people write $ P $ transposed, with the rows denoting
  the starting state and the columns denoting the ending state, in which case
  you'll have to do some transposing, and multiplying $ P $ and $ s $ backwards.
</p>

<p>
  <i>Exercise. Implement a Markov chain by defining a transition matrix $ P $
  and sample episodes starting from some state $ s_1. $</i>
</p>

<h2>Markov reward process (MRP)</h2>

<p>
  <i>Definition. A Markov chain with rewards: $ (S, P, R) $ where $ S $ and $ P $ are a Markov chain, and $ R: S \longrightarrow \bR $ is a reward function, e.g.

  $$
    R(s_t = s) = \E[r_t | s_t = s].
  $$
  </i>
</p>

<p>
  Note that in a Markov reward process, $ R $ is a function only of the state
  and not an action because there isn't any action.
</p>

<p>
  <i>Definition. The horizon is the number of time steps in each episode. Can be
  infinite. If it is finite, the process is called a finite Markov process.
  </i>
</p>

<p>
  <i>Definition. The return $ G_t $ for a Markov reward process is the
  discounted sum of rewards from $ t $ to the horizon:

  $$
    G_t = r_t + \g r_{t+1} + \g^2 r_{t+2} + \cdots.
  $$

  </i>
</p>

<p>
  <i>Definition. The state value function $ V(s) $ for an MRP is the expected
  return starting from $ s: $

  $$
    V(s) = \E[G_t | s_t = s] = \E[r_t + \g r_{t+1} + \g^2 r_{t+2} + \cdots | s_t = s].
  $$

  </i>
</p>

<p>
  The state value function $ V $ takes into account the future, whereas the
  reward function $ R $ only looks at the immediate reward.
</p>

<h2>Discount factor $ \g $</h2>

<p>
  Humans empirically act as if there is a discount factor $ \g < 1, $ i.e. the near future is more important than the distant future.
</p>

<h2> Markov Decision Process (MDP) </h2>

<p>
  <i>Definition. A Markov Decision Process is a tuple $ (S, P, R, A, \g) $ where $
  (S, P, R) $ is a Markov Reward Process and $ A $ is a set of actions. $ P $ is a dynamics / transition model for each state action pair $ (s, a): $

  $$
  P \left( s _ { t + 1 } = s ^ { \prime } | s _ { t } = s , a _ { t } = a \right).
  $$

  $ R $ is the reward function on $ (s, a): $

  $$
  R \left( s _ { t } = s , a _ { t } = a \right) = \mathbb { E } \left[ r _ { t } | s _ { t } = s , a _ { t } = a \right].
  $$

  $ \g \in [0, 1] $ is the discount factor used to calculate the return $ G_t. $
  </i>
</p>

<p>
  Note that actions add another dimension to the transition model. One way to
  represent this model is as a series of transition matrices, one for each
  action, e.g. the transition model for action $ a_1 $ is

  $$
  P \left( s ^ { \prime } | s , a _ { 1 } \right) = \left( \begin{array} { l l l l l l l } { 1 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } \\ { 1 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } \\ { 0 } & { 1 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } \\ { 0 } & { 0 } & { 1 } & { 0 } & { 0 } & { 0 } & { 0 } \\ { 0 } & { 0 } & { 0 } & { 1 } & { 0 } & { 0 } & { 0 } \\ { 0 } & { 0 } & { 0 } & { 0 } & { 1 } & { 0 } & { 0 } \\ { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 1 } & { 0 } \end{array} \right).
  $$

  with rows and columns representing starting and ending states, just like
  before. Similarly the transition model for action $ a_2 $ is

  $$
  P \left( s ^ { \prime } | s , a _ { 2 } \right) = \left( \begin{array} { l l l l l l l } { 0 } & { 1 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } \\ { 0 } & { 0 } & { 1 } & { 0 } & { 0 } & { 0 } & { 0 } \\ { 0 } & { 0 } & { 0 } & { 1 } & { 0 } & { 0 } & { 0 } \\ { 0 } & { 0 } & { 0 } & { 0 } & { 1 } & { 0 } & { 0 } \\ { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 1 } & { 0 } \\ { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 1 } \\ { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 1 } \end{array} \right).
  $$

  The full transition model would then be a 3-dimensional array.
</p>

</div>
</body>
</html>
