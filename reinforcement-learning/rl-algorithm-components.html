<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />
<title>Reinforcement Learning</title>
<link rel="stylesheet" href="../css/global.css" />
<link rel="stylesheet" href="../css/obsidian.min.css" />
<script src="../js/highlight.min.js"></script>
<!-- this config must be before MathJax.js: -->
<script src="../js/mathjax.config.js"></script>
<script src="../js/MathJax/MathJax.js"></script>
<!--
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
-->
<script src="../js/jquery-3.1.0.min.js"></script>
<script src="../js/smartquotes.js"></script>
<script src="../js/global.js"></script>
</head>
<body>
<div id="content">

<h1>Components of an RL algorithm</h1>

<h2>Model</h2>

<p>
  An agent's representation of how the world changes in response to agent's
  actions. A transition / dynamics model predicts the agent's next state given the current state and action:

  $$
    p(s_{t+1} = s' | s_t = s, a_t = a).
  $$

  A reward model predicts the immediate reward:

  $$
    r(s_t = s, a_t = a) = \E[r_t | s_t = s, a_t = a],
  $$

  i.e. it's the expected value of the reward given a state and action e.g. in a
  deterministic world, there would be a unique reward given $ (s, a): $ $ r(s_t
  = s, a_t = a) = \E[r_t | s_t = s, a_t = a] = r_t, $ but in a stochastic world,
  there would be more than one, each with an associated probability, and here
  we're taking their weighted average.
</p>

<h2>Policy function</h2>

<p>
  A policy $ \p: S \longrightarrow A $ determines how the agent chooses actions. We can have a deterministic policy, which maps a state to a single action:

  $$
    \p(s) = a,
  $$

  or a stochastic policy, which maps states to actions over a distribution:

  $$
    \p(a|s) = p(a_t = a | s_t = s).
  $$


</p>

<h2>Value function</h2>

<p>
  A value function $ V^\p $ tells us how good a state $ s $ is, by measuring the
  expected immediate plus discounted future rewards under a particular policy $
  \p $ acting from $ s: $

  $$
    V^\p(s) = \E_\p [r_t + \g r_{t+1} + \g^2 r_{t+2} + \cdots | s_t = s].
  $$

  The discount factor $ \g \in [0, 1] $ specifies how much we care about
  immediate v.s. future rewards, e.g. $ \g = 0 $ means we only care about the
  immediate reward $ r_t, $ and larger $ \g $ means we care more about future
  rewards.
</p>

<p>
  We can also define a value function on an action, or on a combination of state
  and action:

  $$
    V^\p(s, a) = \E_\p [r_t + \g r_{t+1} + \g^2 r_{t+2} + \cdots | s_t = s, a_t = a].
  $$
</p>

<p>
  Having a value function allows us to compare different policies, and choose
  the best one.
</p>


</div>
</body>
</html>
