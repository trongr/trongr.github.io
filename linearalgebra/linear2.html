<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Linear Algebra II</title>
<link rel="stylesheet" href="../css/global.css">

<!-- this config must be before MathJax.js: -->
<script src="../js/mathjax.config.js"></script>
<script src="../js/MathJax/MathJax.js"></script>
<!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script> -->

<script src="../js/jquery-3.1.0.min.js"></script>
<script src="../js/toc.js"></script>
<script src="../js/smartquotes.js"></script>
<script src="../js/global.js"></script>

</head>
<body>
<div id="content">
<a href="../index.html">HOME</a>

<h1>Linear Algebra II</h1>

<div id="toc"></div>

<h1>Reference</h1>
<ol>
<li>Linear Algebra by Friedberg, Insel, and Spence.</li>
<li>Linear Algebra video lectures by Gilbert Strang.</li>
<li>The Web.</li>
</ol>

<h1>Adjoint / complex conjugate</h1>

<p>
    <i>Exercise. Let $ A $ be an $ n \times n $ matrix. Then $ \det A^* = \overline{\det A}. $</i>
</p>
<p>
    Use Leibniz's formula for determinants.
</p>

<p>
    <i>Ex. Suppose that $ A $ is an $ m \times n $ matrix in which no two
    columns are identical. Then $ A^* A $ is diagonal iff every pair of columns
    of A is orthogonal.
    </i>
</p>
<p>
    Write out the entries of $ A^* A $ in terms of inner products of the rows of
    $ A^* $ and columns of $ A. $
</p>

<h3>Example of linear operator without an adjoint</h3>

<p>
    <i>Example. Let $ V $ be the vector space of infinite sequences with finitely
        many nonzero entries in $ F = \bR $ or $ F = \bC. $ Define $ T: V
        \longrightarrow V $ by

        $$ T(\s)_k = \sum_{i=k}^{\infty} \s_i $$

        for $ k \geq 0. $ Then

        <ol>
            <li>$ T $ is a linear operator on $ V. $</li>
            <li>For any $ n \geq 0, $ $ T(e_n) = \sum_{i=1}^n e_i. $</li>
            <li>$ T $ has no adjoint.</li>
        </ol></i>
</p>
<p>
    1. $ T $ is linear. Let $ \s, \m \in V. $ Then

    $$ T(\s + \m)_k = \sum_{i=k}^{\infty} (\s + \m)_i
    = \sum_{i=k}^{\infty} \s_i
    + \sum_{i=k}^{\infty} \m_i. $$

    Scalar multiple is proved similarly.
</p>
<p>
    2. Explicitly,

    \begin{align*}
        e_n &= (0, 0,..., 0, 1, 0,...) \\
        T(e_n) &= \left(\sum_{i=1}^{\infty} (e_n)_i, \sum_{i=2}^{\infty} (e_n)_i,...,
        \sum_{i=n}^{\infty} (e_n)_i,...\right) \\
        &= \left(1, 1,..., 1, 0, 0,...\right) \\
        &= \sum_{i=1}^n e_i.
    \end{align*}
</p>
<p>
    3. Suppose for contradiction that $ T^* $ exists. Then for any $ n, k $ we
    have

    $$ \< T^*(e_n), e_k \> = \< e_n, T(e_k) \> = \< e_n, \sum_{i=1}^k e_i \>
    = \begin{cases}
    0 \text{ if } k < n \\
    1 \text{ if } k \geq n.
    \end{cases}. $$

    Fixing $ n $ and varying $ k $ over the positive integers, we find that $ \<
    T^*(e_n), e_k \></n> $ has infinitely many nonzero entries. In fact

    $$ T^*(e_n) = (0, 0,...,1,1,1,1,...). $$

    Therefore $ T^* $ is not a map on $ V $ and we have a contradiction.
</p>

<h1>Normal and self-adjoint operators</h1>

<p>
    <i>Lemma. Let $ T $ be a linear operator on a finite-dimensional inner
    product space $ V. $ If $ T $ has an eigenvector, then so does $ T^*. $ If $
    \l $ is an eigenvalue of $ T, $ then $ \bar \l $ is an eigenvalue of $ T^*.
    $
    </i>
</p>

<h1>Schur's Theorem</h1>

<p>
    <i>Theorem. Let $ T $ be a linear operator on a finite-dimensional inner
    product space $ V. $ Suppose that the characteristic polynomial of $ T $
    splits. Then there exists an orthonormal basis $ \b $ for $ V $ s.t. the
    matrix $ [T]_\b $ is upper triangular.</i>
</p>

<h3>Normal operator</h3>

<p>
    <i>Definition. Let $ V $ be an inner product space and $ T $ be a linear
    operator on $ V. $ Then $ T $ is called normal if $ TT^* = T^*T. $ Similarly
    for an $ n\times n $ matrix $ A. $</i>
</p>
<p>
    <i>Corollary. Let $ V $ be an inner product space and $ T $ be a linear
    operator on $ V. $ Then $ T $ is normal iff $ [T]_{\b} $ is normal, where $
    \b $ is an orthonormal basis for $ V. $</i>
</p>
<p>
    <i>Proof.</i> $ (\Longrightarrow) $ Suppose that $ TT^* = T^*T. $ By theorem
    6.10,

    $$ [T^*]_\b = [T]_\b^*. $$ Use this fact to expand

    \begin{align*}
        [TT^*]_{\b} &= [T^*T]_{\b} \\
        [T]_{\b} [T^*]_{\b} &= [T^*]_{\b} [T]_{\b} \\
        [T]_{\b} [T]^*_{\b} &= [T]^*_{\b} [T]_{\b}. \\
    \end{align*}

    Therefore $ [T]_{\b} $ is normal. The converse is similar.
</p>

<h2>Theorem 6.18. Characterizing unitary / orthogonal / isometric operators on a
fin dim inner product space</h2>

<p>
<i>
    Let $ T, n, \left\langle \right\rangle ,V,F. $ Then the following statements
    are equivalent:

    <ol>
        <li>$ TT^{*}=T^{*}T=I. $ In particular, T is normal and there exists an
        orthonormal basis for V consisting of eigenvectors of T.</li>
        <li>$ \left\langle T(x),T(y)\right\rangle =\left\langle x,y\right\rangle
        $  for all x,y.</li>
        <li>If $ \beta $ is an orthonormal basis, then $ T(\beta) $ is an
        orthonormal basis.</li>
        <li>There exists an orthonormal basis $ \beta $ s.t. $ T(\beta) $ is an
        orthonormal basis.</li>
        <li>$ \left|\left|T(x)\right|\right|=\left|\left|x\right|\right| $ for
        all x, i.e. T is unitary / orthogonal.</li>
    </ol>
</i>
</p>

<h2>Schur's Theorem Variant. Let $ A\in M_{n\times n}(F) $ be a matrix whose
characteristic polynomial splits over F. If $ F=C, $ then A is unitarily eq. to
a complex upper triangular matrix. IF $ F=R, $ then A is orthogonally eq. to a
real upper triangular matrix.</h2>

<h2>Theorem 6.20. Let A be a real $ n\times n $ matrix. Then A is selfadjoint
i.e. symmetric iff A is orthogonally equivalent to a diagonal matrix D.</h2>

<p>
<i>Proof.</i>
The forward direction is already proved in Note 6.18.3. Conversely, suppose that
A is ortho. eq. to a diagonal matrix D. Then there exists an orthogonal matrix P
s.t. $ A=P^{T}DP. $ We want to show that A is symmetric:

$$
A^{T}=(P^{T}DP)^{T}=P^{T}D^{T}P=P^{T}DP=A,
$$

since D is diagonal.
</p>

<h2>Isometry / Rigid Motions</h2>

<p>
    <i>Definition. Let VR. A function $ f:V\longrightarrow V $ is called an
    isometry / rigid motion if

    $$ \|f(x)-f(y)\|=\|x-y\| $$

    for all x, y in V.
</i>
</p>

<p>
    (Rigid motions are examples of affine transformations, which include
    translation, scaling, homothety (homogeneous dilation), similarity
    transformation, reflection, rotation, shear mapping, and compositions of
    them in any combination and sequence.)
</p>

<h3>Examples of rigid motions</h3>

<p>
    E.g. Any orthogonal operator ($ \left|\left| f(x) \right|\right| =
    \left|\left| x \right|\right| $) on a finite dimensional real inner product
    space is a rigid motion, e.g. rotations, reflection by a line through the
    origin.
</p>
<p>
    E.g. any translation is a rigid motion. A function $ g:V\longrightarrow V $
    where $ V $ is a real inner product space, is called a translation if there
    exists a vector $ v_{0} \in V $ s.t. $ g(x) = x + v_0 $ for all $ x\in V. $
    We say that $ g $ is a translation by $ v_0. $
</p>

<h3>Proposition. Translations are rigid motions</h3>

<p>
    <i> Proof. </i> We want to show that

    $$ \|g(x)-g(y)\| = || x + v_0 - y - v_0 || =\|x-y\|, $$

    which is true.
</p>

<h3>Proposition. Compositions of rigid motions are also rigid motions</h3>

<p>
    <i> Proof. </i> Let $ f,g:V\longrightarrow V $ be rigid motions. We want to
    show that $$ \|f \circ g(x)-f \circ g(y)\| = | f(g(x)) - f(g(y)) | =_1 | g(x)
    - g(y) | =_2 \|x-y\|. $$

    $ =_1 $ is true because $ f $ is a rigid motion, and $ =_2 $ is true because
    $ g $ is a rigid motion.
</p>

<h2>Theorem 6.22. Every rigid motion is an orthogonal operator followed by a
translation</h2>

<p>
    <i> Let $ f:V\longrightarrow V $ be a rigid motion on a finite-dimensional
    real inner product space V. Then there exists a unique orthogonal operator T
    and a unique translation g on V s.t. $ f = g \circ T. $ </i>
</p>
<p>
    NOTE. Any translation is a special case of this composite, where the
    operator is the identity, and any orthogonal operator is also a special case
    where the translation is by 0.
</p>
<p>
    <i> Proof. </i> Let $ T: V\longrightarrow V $ be defined by

    $$ T(x) = f(x) - f(0). $$

    We show that T is an orthogonal operator, from which it follows that $ f = g
    \circ T, $ where g is the transaction by $ f(0). $ Observe that T is the
    composite of $ f $ and the translation by $ -f(0); $ hence T is a rigid
    motion. Furthermore, for any $ x \in V, $

    $$ | T(x) |^2 = | f(x) - f(0) |^2 = | x - 0 |^2 = | x |^2, $$

    and consequently $$ | T(x) | = | x |. $$ Thus for any $ x, y \in V, $

    \begin{align*}
    | T(x) - T(y) |^2 &= | T(x) |^2 - 2\< T(x), T(y) \> + | T(y) |^2 \\
        &= | x |^2 - 2\< T(x), T(y) \> + | y |^2
    \end{align*}

    and

    $$ | x - y |^2 = | x |^2 - 2\< x, y \> + | y |^2. $$

    But $$ | T(x) - T(y) |^2 = | f(x) - f(y) |^2 = | x - y |^2, $$ and so

    $$ \< T(x), T(y) \> = \< x, y \> $$

    for all $ x, y \in V. $
</p>
<p>
    Now we show that T is linear. Let $ x, y\in V, a \in \bbR. $ Then

    \begin{align*}
    | T(x+ay) - T(x) - aT(y) |^2 &= | (T(x+ay) - T(x)) - aT(y) | \\
        &= | T(x+ay) - T(x) |^2 - 2a\< T(x+ay) - T(x), T(y) \> + a^2| T(y) |^2 \\
        &= | (x+ay) - x |^2 - 2a\< T(x+ay) - T(x), T(y) \> + a^2| y |^2 \\
        &= | ay |^2 - 2a(\< T(x+ay), T(y) \> - \< T(x), T(y) \>) + a^2| y |^2 \\
        &= 2a^2| y |^2 - 2a(\< x+ay, y \> - \< x, y \>) \\
        &= 0.
    \end{align*}

    Therefore $ T(x+ay) = T(x) + aT(y) $ and T is linear. Since T also preserves
    inner products, it is also an orthogonal operator.
</p>
<p>
    To show uniqueness, let $ u_0, v_0 \in V, $ let T and U be orthogonal
    operators s.t.

    $$ f(x) = T(x) + u_0 = U(x) + v_0. $$

    Substituting $ x = 0 $ yields $ u_0 = v_0, $ and hence the translation is
    unique. This equation then reduces to $  T(x) = U(x), $ therefore $ T = U. $
</p>

<h2>Theorem 6.23. Orthogonal operators on $ \bbR^2 $ are rotations or
reflections with determinant $ \pm 1. $</h2>

<p>
    <i> Let T be an orthogonal operator on $ \bbR^2, $ and let $ A = [T]_\b $
    where $ \b $ is the standard basis for $ \bbR^2. $ Then exactly one of the
    following conditions is satisfied:

    <ol>
        <li>T is a rotation, and $ \det A = 1. $</li>
        <li>T is a reflection about a line through the origin, and $ \det A = -1. $</li>
    </ol></i>
</p>

<p>
<i>Proof.</i> By theorem 6.18, since
<i>
T is orthogonal, T maps orthonormal basis to orthonormal basis.
</i> In particular, $ T(\b) = \{ T(e_1), T(e_2) \} $ is an orthonormal basis for
$ \bR^2. $ Since $ e_1 $ is a unit vector, there exists a unique angle $ \th $
with $ 0 \leq \th \leq 2 \pi $ s.t. $ T(e1) = (\sin \theta, \cos \theta). $
Since $ T(e_2) $ is a unit vector and is orthogonal to $ T(e_1), $ there are
only two choices for $ T(e_2). $ Either

$$
T(e_2) = (- \sin \theta, \cos \theta)
$$

or

$$
T(e_2) = (\sin \theta, - \cos \theta).
$$

First suppose that

$$
T(e_2) = (- \sin \theta, \cos \theta).
$$

Then

$$
A = \begin{bmatrix}
    T(e_1) & T(e_2)
\end{bmatrix} = \begin{bmatrix}
    \cos \theta & - \sin \theta \\
    \sin \theta & \cos \theta
\end{bmatrix}.
$$

<i>Recall that this is a rotation by $ \theta, $</i> and

$$
\det A = \cos^2 \theta + \sin^2 \theta = 1.
$$
</p>

<p>

Now suppose that

$$
T(e_2) = (\sin \theta, - \cos \theta).
$$

Then

$$
A = \begin{bmatrix}
    T(e_1) & T(e_2)
\end{bmatrix} = \begin{bmatrix}
    \cos \theta & \sin \theta \\
    \sin \theta & - \cos \theta
\end{bmatrix}.
$$


<i>
Now recall that this is a reflection through a line through the origin at angle
$ \theta / 2. $
</i>

Furthermore,

$$
\det A = - \cos^2 \theta - \sin^2 \theta = -1.
$$

</p>

<h3>Corollary. Rigid motions in $ \bR^2 $ are compositions of rotations,
reflections, and translations. In fact, a rigid motion in $ \bR^2 $ is either a
rotation followed by a translation or a reflection about a line through the
origin followed by a translation.</h3>

<h3>Example. Reflection through a line in $ \bR^2: $

$
\begin{bmatrix}
\frac{1}{\sqrt{5}} & \frac{2}{\sqrt{5}} \\
\frac{2}{\sqrt{5}} & -\frac{1}{\sqrt{5}} \\
\end{bmatrix}
$

</h3>
<p>
Let

$$
A = \begin{bmatrix}
\frac{1}{\sqrt{5}} & \frac{2}{\sqrt{5}} \\
\frac{2}{\sqrt{5}} & -\frac{1}{\sqrt{5}} \\
\end{bmatrix}.
$$

<i>It's easy to verify that $ AA^* = A ^* A = I, $ hence A is orthogonal.
Alternatively, we can also verify that the columns of A form an orthonormal
basis of $ \bR^2. $ Furthermore, $ \det A = -1, $ so by Theorem 6.23, A must be
a reflection through a line through the origin.
</i>
</p>

<p>
Now we'll find the line L that A reflects through. Note that L is the
1-dimensional subspace where $ A x = x $ for all $ x \in L. $ To find L, it's
enough to find a vector $ x $ (in this case an eigenvector of A) and we'll have
$ L = \span\{ x \}. $
<i>
    One such eigenvector is $ x = (2, \sqrt{5} - 1), $ which we can
    find by solving

    $$ A x = x. $$
</i>


Alternatively, L is the line through the origin with slope $ m = \frac{\sqrt{5}
- 1}{2}, $ and hence is the line with the equation

$$ y = \frac{\sqrt{5} - 1}{2} x. $$
</p>

<h2>Question. What do orthogonal operators look like in $ \bR^3 $ and $ \bR^n? $</h2>

<h3>Example. Constructing orthogonal operators on $ \bR^3 $ from orthogonal operators on
$ \bR^2. $</h3>

<p>
    Let

    $$ A =
    \begin{bmatrix}
        \cos \th & -\sin \th \\
        \sin \th & \cos \th \\
    \end{bmatrix}
    : \bR^2 \longrightarrow \bR^2 $$

    be the rotation operator by $ \th $ in $ \bR^2. $ <i> Recall that this
    operator is orthogonal because its columns and rows form an orthonormal
    basis for $ \bR^2. $ </i> We can construct orthogonal operators on $ \bR^3 $
    as follows:

    $$ \begin{bmatrix}
        \cos \th & -\sin \th & 0 \\
        \sin \th & \cos \th & 0 \\
        0 & 0 & 1 \\
    \end{bmatrix},
    \begin{bmatrix}
        \cos \th  & 0 & -\sin \th \\
        0 & 1 & 0 \\
        \sin \th  & 0 & \cos \th \\
    \end{bmatrix},
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & \cos \th & -\sin \th \\
        0 & \sin \th & \cos \th \\
    \end{bmatrix}, $$

    and more by moving the 1 around. It's easy to see that these are orthogonal
    because the columns and rows are orthonormal.
</p>

<h4>Question. Does moving the 1 around correspond to orthogonal equivalence,
i.e. the three matrices above are orthogonally equivalent? How to find the
orthogonal transformation?</h4>

<h4>Question. Can all orthogonal operators in $ \bR^3 $ and higher dimensions be
formed like this?</h4>

<p>
Something like that.
</p>

<h2>Theorem. Any orthogonal matrix is orthogonally equivalent to a diagonal
block matrix of the form

$$ \begin{bmatrix}
    D_{\pm 1} & O & \cdots & O \\
    O & R_1 & \cdots & O \\
    \vdots & \vdots & \ddots & \vdots \\
    O & O & \cdots & R_k \\
\end{bmatrix} $$

where $ D_{\pm 1} $ is a diagonal matrix whose diagonal entries are $ \pm 1, $
and

$$ R_j = \begin{bmatrix}
    \cos \th_j & -\sin \th_j \\
    \sin \th_j & \cos \th_j \\
\end{bmatrix} $$ and $ \th_j \in \bR. $</h2>

<p>
    Proof TODO.
</p>

<h3>Example. Orthogonal matrices in $ \bR^3 $</h3>

<p>
    $$ \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1 \\
    \end{bmatrix},
    \begin{bmatrix}
        -1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1 \\
    \end{bmatrix}, \ldots $$
</p>

<h1>Conic sections</h1>



</div>
</body>
</html>
