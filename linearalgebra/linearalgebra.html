<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Linear Algebra</title>
<link rel="stylesheet" href="../css/global.css">

<!-- this config must be before MathJax.js: -->
<script src="../js/mathjax.config.js"></script>
<script src="../js/MathJax/MathJax.js"></script>
<!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script> -->

<script src="../js/jquery-3.1.0.min.js"></script>
<script src="../js/smartquotes.js"></script>
<script src="../js/global.js"></script>

</head>
<body>
<div id="content">

<h1>Linear Algebra</h1>

<img src="images/journey.jpg">
<div class="epigraph">
     <div class="quote">
        Just keep going, faith will come later.
     </div><hr><div class="author">
        d'Alembert to his students
     </div>
</div>

<h1>Subspaces</h1>

<p>
    <b>Theorem: subspace test.</b> <i>A subset $W$ of a vector space
    $V$ is a subspace of $V$ iff the following hold:

    <ul>
    <li>$W$ contains the zero vector.</li>
    <li>$W$ is closed under scalar multiplication and vector addition.</li>
    </ul></i>
</p>

<h1>Linear Transformations and Matrices</h1>

<h3>Dimension / rank nullity theorem</h3>

<div class="epigraph">
     <div class="quote">
          Imagine $\RR^n,$ then let $n = 42.$
     </div><hr><div class="author">Unknown</div>
     <img src="images/760px-Rank-nullity.svg.png">
</div>

<p>
    <b>Theorem.</b> <i>Let V and W be vector spaces, and let
    $T:V\longrightarrow W$ be linear. If V is finite-dimensional,
    then

    $$\dim \ker T + \rank T = \dim V.$$</i>
</p>

<h1>Special Matrix Functions</h1>

<p>
    <b>Proposition.</b> <i>If $A$ and $B$ are square matrices, then
    $\tr(AB) = \tr(BA)$ and $\tr A = \tr A^t.$</i>
</p>

<h1>Change of Coordinate Matrix</h1>

<div class="bside">
    <img src="images/377px-3d_basis_transformation.png">
</div>

<p>
    <b>Proposition.</b> <i>Let $B$ be an $n \times n$ invertible
    matrix. Then the map

    $$\Phi_B: M_{n\times n}(F) \longrightarrow M_{n\times n}(F)$$

    defined by

    $$\Phi_B(A) = B^{-1} A B$$

    is an isomorphism.</i>
</p>

<p>
    <b>Proposition.</b> <i>For any invertible matrix $B$ there exist
    bases $\beta, \gamma$ s.t. $B = [I]^\beta_\gamma,$ i.e. every
    invertible matrix is a change of coordinates matrix.</i>
</p>

<h1>Eigenvectors and Eigenvalues</h1>

<figure>
    <img src="images/5415731794_083b334b01_o.jpg">
    <figcaption></figcaption>
</figure>

<p id="nuhefgsd74862">
    <b>Theorem: Diagonalizability.</b> <i>A linear operator $T$ on a
    finite-dimensional vector space $V$ is diagonalizable iff there
    exists an ordered basis $\beta$ for $V$ consisting of eigenvectors
    of $T.$ Furthermore, if $T$ is diagonalizable, $\beta =
    \{v_1,\ldots, v_n\}$ is an ordered basis of eigenvectors of $T,$
    and $D = [T]_\beta,$ then $D$ is a diagonal matrix and $D_{jj}$ is
    the eigenvalue corresponding to $v_j$ for $1 \leq j \leq n.$</i>
</p>

Proof in the book.

<p>
    <b>Proposition.</b> <i>Let $T$ be a linear operator on a
    finite-dimensional vector space $V,$ and let $\beta$ be an ordered
    basis for $V.$ Then $\lambda$ is an eigenvalue of $T$ iff it is an
    eigenvalue of $[T]_\beta.$</i>
</p>

<p>
    <b>Corollary.</b> <i>
Similar matrices have the same eigenvalues, but not necessarily the same eigenvectors.
</i>
</p>

<p>
    <b>Proposition.</b> <i>If $v$ is an eigenvector of $A$
    corresponding to eigenvalue $\lambda,$ and $B$ is similar to $A$
    under change of coordinates matrix $Q,$ then $Qv$ is an
    eigenvector of $B$ corresponding to the same eigenvalue $\lambda.$
    Another way of saying this is that change of coordinates preserves
    eigenvalues and eigenvectors.</i>
</p>

<p>
    <i>Proof.</i> Let $A = Q^{-1}BQ.$ Then \begin{align*} Av &=
    Q^{-1}BQv \\ QAv &= BQv \\ \lambda Qv &= BQv, \end{align*} so $Qv$
    is an eigenvector of $B$ corresponding to $\lambda.$</p>

<p>
    <b>Definition.</b> <i>Let $T$ be a linear operator on a
    finite-dimensional vector space $V.$ Define the determinant of $T$
    to be $\det T = \det([T]_\beta)$ for any ordered basis $\beta$ for
    $V.$</i>
</p>

<p>
    <b>Note.</b> Since the determinant is multiplicative, for any two bases $\beta$ and $\alpha$ we have
\begin{align*}
\det([T]_\beta) &= \det(Q^{-1} [T]_\alpha Q) \\
&= \det Q^{-1} \det([T]_\alpha) \det Q \\
&= \det([T]_\alpha),
\end{align*}
where $Q = [I]^\alpha_\beta$ is the change of basis matrix from $\beta$ to $\alpha,$ and therefore $\det T$ is well defined, i.e. it's independent of the choice of basis.
</p>

<p>
    <b>Proposition.</b> <i>Representation of a matrix with respect to
    a basis is a linear operation, i.e. $$[T + \lambda U]_\beta =
    [T]_\beta + \lambda[U]_\beta.$$ In fact it's an isomorphism. For a
    fixed basis $\beta$ this transformation is usually written
    $\Phi_\beta: \mathcal{L}(V) \longrightarrow M_{n\times n}(F).$</i>
</p>

<p>
    <i>Proof.</i> Let $v$ be a vector, $\beta = \{v_1,\ldots, v_n\},$ and $T(v) = \sum a_i v_i, U(v) = \sum b_i v_i.$ We want to show that

    \begin{align*}
    [T + \lambda U]_\beta [v]_\beta &= ([T]_\beta + \lambda[U]_\beta)[v]_\beta \\
    [T(v) + \lambda U(v)]_\beta &= [T(v)]_\beta + \lambda[U(v)]_\beta.
    \end{align*}

    The RHS is $[a_i] + \lambda [b_i],$ which is the same as the LHS: $[a_i + \lambda b_i].$
</p>

<p>
    <b>Note.</b> Analogously, the standard representation of a vector space $V$ with respect to a basis $\beta$ is $\phi_\beta: V \longrightarrow F^n.$ And it's also an isomorphism.
</p>

<p>
    <b>Proposition.</b> <i>For any scalar $\lambda$ and any ordered
    basis $\beta$ for $V,$ $\det(T - \lambda I_V) = \det([T]_\beta -
    \lambda I).$</i>
</p>

<p id="skdnsut164">
    <b>Proposition: Eigenvalues and invertibility.</b> <i>A linear
    operator $T$ on a finite-dimensional vector space is invertible
    iff its eigenvalues are nonzero.</i>
</p>

<p>
    <i>Proof.</i> If zero is an eigenvalue of $T,$ then $\det(T - 0
    \cdot I) = 0,$ and $T$ is not invertible. Conversely, if $T$ is
    not invertible, then there exists a nonzero vector $v$ s.t. $T(v)
    = 0 = 0 \cdot v,$ and so 0 is an eigenvalue and $v$ is an
    eigenvector of $T.$
</p>

<p>
    <b>Proposition.</b> <i>Let $T$ be an invertible linear
    operator. Then a scalar $\lambda$ is an eigenvalue of $T$ iff
    $\lambda^{-1}$ is an eigenvalue of $T^{-1}.$ Note that by the
    previous proposition $\lambda$ is nonzero, so $\lambda^{-1}$
    exists.</i>
</p>

<p>
    <i>Proof.</i> Apply $T^{-1}$ to both sides of $T(v) = \lambda v.$
</p>

<p>
    <b>Proposition.</b> <i>The eigenvalues of an upper triangular
    matrix $M$ are the diagonal entries of $M.$</i>
</p>

<p>
    <i>Proof.</i> Follows from the fact that the determinant of an
    upper triangular matrix is the product of its diagonal entries.
</p>

<p>
    <b>Proposition.</b> <i>Similar matrices have the same
    characteristic polynomial.</i>
</p>

<p>
    <i>Proof.</i> Let $A = Q^{-1} B Q.$ Then

    \begin{align*}
    \det(A - tI) &= \det(Q^{-1} B Q - Q^{-1} tI Q) \\
    &= \det(Q^{-1}(B - tI)Q) \\
    &= \det(Q^{-1})\det(B - tI)\det(Q) \\
    &= \det(B - tI).
    \end{align*}
</p>

<p>
    <b>Corollary.</b> <i>
The definition of the characteristic polynomial of a linear operator on a finite-dimensional vector space $V$ is independent of the choice of basis for $V.$
</i>
</p>

<p>
    <i>Proof.</i> Follows immediately from the fact that similar
    matrices are the same linear operator expressed under different
    bases.
</p>

<div class="bside">
    <img src="images/basisrepresentationeigenvector.png">
</div>

<p>
    <b>Proposition.</b> <i>Let $T$ be a linear operator on a finite
    dimensional vector space $V$ over a field $F.$ Let $\beta$ be an
    ordered basis for $V,$ and let $A = [T]_\beta.$ Then a vector $v
    \in V$ is an eigenvector of $T$ corresponding to $\lambda$ iff
    $\phi_\beta(v)$ is an eigenvector of $A$ corresponding to
    $\lambda.$</i>
</p>

<h1>Eigenvectors and Some Special Functions and Matrices</h1>

<p>
    <b>Lemma.</b> <i>A square matrix has the same determinant as its transpose.</i>
</p>

<p>
    <b>Proposition.</b> <i>A square matrix has the same characteristic
    polynomial as its transpose.</i>
</p>

<p>
    <b>Proposition.</b> <i>If $x$ is an eigenvector of $T$
    corresponding to $\lambda,$ then for any positive integer $m,$ $x$
    is an eigenvector of $T^m$ corresponding to $\lambda^m.$</i>
</p>

<p>
    <i>Proof.</i> Linearity of $T$ and induction.
</p>

<p>
    <b>Note.</b> The same holds for matrices. It's always the same!
</p>

<p>
    <b>Proposition.</b> <i>Similar matrices have the same trace.</i>
</p>

<p>
    <b>Corollary.</b> <i>
Define the trace of a linear operator $T$ on a finite dimensional vector space as $\tr [T]_\beta$ for any basis $\beta.$ This is well defined by the previous Proposition.
</i>
</p>

<p>
    <b>Proposition.</b> <i>Let $T$ be the linear operator on
    $M_{n\times n}(\mathbf R)$ defined by $T(A) = A^t.$ Then $\pm 1$ are the
    only eigenvalues of $T.$ The eigenvectors of $T$ corresponding to
    $\pm 1$ are symmetric and antisymmetric matrices,
    respectively.</i>
</p>

<p>
    <b>Example.</b> In two dimensions, an ordered basis for $M_{2 \times 2}(\mathbf R)$ consisting of eigenvectors of $T$ so that $[T]_\beta$ is a diagonal matrix is
$$\beta = \left\{
\begin{bmatrix}
1 & 0 \\
0 & 0
\end{bmatrix},
\begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix},
\begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix},
\begin{bmatrix}
0 & 1 \\
-1 & 0
\end{bmatrix}
\right\}.$$
The first three matrices are symmetric and correspond to the eigenvalue $+1,$ while the last is skew-symmetric and corresponds to $-1.$
</p>

<p>
    <b>Example.</b> More generally, in $n$ dimensions an eigenbasis
    consists of symmetric matrices of the form $A$ that has zeroes
    everywhere except a single $1$ along the diagonal, and symmetric
    matrices of the form $B$ with zeroes everywhere except a $1$ in
    two opposite entries $B_{ij}$ and $B_{ji},$ and finally
    anti-symmetric matrices of the form $C$ with zeroes everywhere
    except a $-1$ and a $+1$ in two opposite entries $C_{ij}$ and
    $C_{ji},$ where the $-1$ is in the lower left half and the $+1$ in
    the upper right half of $C.$ There are $n$ matrices of type $A,$
    $(n^2 - n) / 2$ each of type $B$ and $C,$ for a total of $n^2,$ as
    expected.
</p>

<h1>Scalar Matrices</h1>

<div class="bside">
    <img src="images/scalarmatrix.png">
</div>

<p>
    <b>Definition.</b> <i>A scalar matrix is a square matrix of the
    form $\lambda I$ for some scalar $\lambda.$</i>
</p>

<p>
    <b>Proposition.</b> <i>If a square matrix $A$ is similar to a
    scalar matrix $\lambda I,$ then $A = \lambda I.$</i>
</p>

<p>
    <i>Proof.</i> If $A$ is similar to $\lambda I,$ that means there
    is an invertible matrix $Q$ s.t. $A = Q^{-1} \lambda I Q = \lambda
    I.$
</p>

<p id="asdvab94656">
    <b>Proposition.</b> <i>A diagonalizable matrix $A$ having only one
    eigenvalue is a scalar matrix.</i>
</p>

<p>
    <i>Proof.</i> If $A$ is diagonalizable, this means $A$ is similar
    to a diagonal matrix, whose diagonal entries are its
    eigenvalues. Since $A$ only has one eigenvalue $\lambda,$ the
    diagonal entries are all equal to $\lambda.$
</p>

<div class="aside">
    <img src="images/rotation.jpg">
</div>

<p>
    <b>Example.</b> $\begin{bmatrix}
1 & 1 \\
0 & 1 \\
\end{bmatrix}$ is not diagonalizable.
</p>

<p><b>Example of a linear transformation with no eigenvalue and
eigenvector.</b> Rotation in the plane by $\theta \in (0, 2\pi).$</p>

<p><b>Proposition.</b> <i>The characteristic polynomial $f$ of an $n
\times n$ matrix $A$ has leading coefficient $(-1)^n$ and order
$n$:

$$f(t) = \det(A - tI) = (-1)^n t^n + a_{n-1} t^{n-1} + \cdots + a_0.$$

As a consequence, $A$ has at most $n$ eigenvalues.</i></p>

<p>
    <i>Proof.</i> Pretty easy
    induction. <a href="#oijadfa8923">Slightly more explicit formula
    here.</a>
</p>

<p>
    <b>Example.</b> There are 4 possible distinct characteristic
    polynomials of matrices in $M_{2\times 2}(Z_2)$:

    $$f(t) = t^2 + at + b,$$

    with 2 choices for $a$ and 2 for $b.$
</p>

<p><b>Proposition.</b> <i>Let $A, B \in \mathbf C.$ If $B$ is
invertible, then there is a scalar $c \in \mathbf C$ s.t. $A + cB$ is
not invertible.</i></p>

<p><i>Proof.</i> We want to show that $$\det(A + cB) = 0$$ for some
$c,$ which is equivalent to $$0 = \det(A + cB) \det(B^{-1}) =
\det(AB^{-1} + cI),$$ since $B$ is invertible and $\det(B^{-1}) \neq
0.$ But $$0 = \det(AB^{-1} + cI)$$ is the characteristic equation of
$AB^{-1},$ and we know that the characteristic polynomial of a matrix
$M$ always has leading coefficient 1 and degree $n$ the size of $M,$
and since this is over the complex field $\mathbf C,$ this equation
has a solution.</p>

<p><b>Note.</b> The fact that the characteristic polynomial has degree
$n$ is important because if $B$ weren't invertible, then $\det(A +
cB)$ is also a polynomial over $\mathbf C$ that might not have degree
$n,$ in particular it might be a constant polynomial $$\det(A + cB) =
K \neq 0,$$ which has no solution.</p>

<p><b>Example.</b> Let $$A = \begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix},\quad
B = \begin{bmatrix}
-1 & 1 \\
-1 & 1
\end{bmatrix}.$$ Then $$\det(A + cB) = 1 \neq 0,$$ and $A$ and $A + cB$ are
both invertible for all $c,$ but $B$ isn't. The way you come up with this example is to let $$A = \begin{bmatrix}
a & b \\
c & d
\end{bmatrix},\quad
B = \begin{bmatrix}
\alpha & \beta \\
\gamma & \delta
\end{bmatrix}$$ and calculate
$$\det(A + xB) = (\alpha \delta - \beta \gamma) x^2 +
(a\delta + \alpha d - c\beta - \gamma b) x + (ad - cb).$$ We want this
to be equal to a constant $K,$ say 1, so we need
$$\begin{align*}
\alpha \delta - \beta \gamma &= 0 \\
a\delta + \alpha d - c\beta - \gamma b &= 0 \\
ad - cb &= 1,
\end{align*}$$ which are easy to find.</p>

<p>
    <b>Proposition.</b> <i>Let A and B be similar $n \times n$
    matrices. Then there exist an $n$-dimensional vector space V, a
    linear operator T on V, and ordered bases $\beta$ and $\gamma$
    for V such that $A = [T]_\beta$ and $B = [T]_\gamma.$</i>
</p>

<p>
    <b>Proposition.</b> <i>Let A be an $n \times n$ matrix with
    characteristic polynomial

    $$f(t) = \det(A - tI) = (-1)^n t^n + a_{n-1} t^{n-1} + \cdots + a_0.$$

    Then $f(0) = a_0 = \det(A),$ and therefore A is invertible
    iff $a_0 \neq 0.$</i>
</p>

<p>
    <i>Proof.</i> By definition.
</p>

<p id="oijadfa8923">
    <b>Proposition.</b> <i>Let A and $f$ be as in the previous
    proposition. Then

    $$f(t) = (A_{11} - t)(A_{22} - t)\cdots (A_{nn} - t) + q(t)$$

    where $q(t)$ is a polynomial of degree at most $n - 2.$
    Furthermore, $$\tr(A) = (-1)^{n-1}a_{n-1}.$$</i>
</p>

<p>
    <i>Proof.</i> Easy induction on $n$ for the first part. To show
    the second part, let's multiply out:

    \begin{align*}
    f(t) &= (A_{11} - t)(A_{22} - t)\cdots (A_{nn} - t) + q(t) \\
         &= (-1)^n t^n \\
         &+ (-1)^{n-1} t^{n-1} (A_{11} + \cdots + A_{nn}) \\
         &+ A_{11}(\text{product involving at least another $A_{ii}$}) \\
         &+ A_{22}(\text{product involving at least another $A_{ii}$}) \\
         &+ \cdots \\
         &+ A_{nn}(\text{product involving at least another $A_{ii}$}) \\
         &+ q(t).
    \end{align*}

    The terms

    $$A_{nn}(\text{product involving at least another $A_{ii}$})$$

    have degrees at most $n - 2,$ as does $q(t),$ and so comparing
    coefficients with

    $$f(t) = (-1)^n t^n + a_{n-1} t^{n-1} + \cdots + a_0$$

    we get

    $$\tr(A) = A_{11} + \cdots + A_{nn} = (-1)^{n-1}a_{n-1}.$$
</p>

<p>
    Now we know 3 coefficients of a characteristic polynomial. Let's
    summarize:
</p>

<p id="stdfh1267">
    <b>Corollary.</b> <i>The characteristic polynomial of a matrix A is

    $$\det(A - tI) = (-1)^n t^n + (-1)^{n-1} \tr(A) t^{n-1} + a_{n-2} t^{n-2} + \cdots + \det(A).$$</i>
</p>

<div class="epigraph">
    <img src="images/pokemon.jpg">
     <div class="quote">
     </div><hr><div class="author">Pokemon</div>
</div>

<p>
    For the remaining coefficients,
    see <a href="#gasfdbnnd35616">Vieta's formulas below.</a>
</p>

<p>
    <b>Proposition.</b> <i>Let $g$ be a polynomial. If $x$ is an
    eigenvector of $T$ with eigenvalue $\lambda,$ then

    $$g(T)(x) = g(\lambda) x,$$

    i.e. $x$ is an eigenvector of $g(T)$ with eigenvalue $g(\lambda).$</i>
</p>

<p>
    <i>Proof.</i> Simple linearity argument.
</p>

<p>
    <b>Corollary.</b> <i>If $f$ is the characteristic polynomial of
    $T$ and $T$ is diagonalizable, then $f(T) = T_0,$ the zero
    operator.</i>
</p>

<p>
    <i>Proof.</i> Since $T$ is diagonalizable, let $\beta = \{v_i\}$
    be a basis for $V$ consisting of eigenvectors of $T.$ By the
    previous Proposition,

    $$f(T)(v_i) = f(\lambda_i) v_i = 0 \cdot v_i = 0,$$

<div class="bside">
    The book says it's not necessary for $T$ to be
    diagonalizable. Will prove later.
</div>

    because eigenvalues are roots of the characteristic
    polynomial. Since $f(T)$ is zero on a basis, it must be zero on
    all of $V.$
</p>

<hr>

<h1>Diagonalizability</h1>

<p>
    <b>Definition.</b> <i>A polynomial $f$ splits over $F$ if there
    are scalars $c,a_i$ s.t.

    $$f(t) = c(t - a_1) \cdots (t - a_n).$$</i>
</p>

<p>
    <b>Example.</b> By the Fundamental Theorem of Algebra, every
    nonconstant polynomial splits over $\CC.$
</p>

<p>
    <b>Theorem.</b> <i>If an operator is diagonalizable, then its
    characteristic polynomial splits.</i>
</p>

<p>
    <i><b>Converse isn't true: the characteristic polynomial splitting
    doesn't mean the operator is diagonalizable.</b></i>
</p>

<h2>Algebraic and Geometric Multiplicity</h2>

<p>
    <b>Definition: eigenspace.</b> <i>The eigenspace of $T$
    corresponding to the eigenvalue $\lambda$ is the set

    $$E_\lambda = \{x\in V: T(x) = \lambda x\} = N(T - \lambda
    I).$$</i>
</p>

<p>
    <b>Definition: algebraic and geometric multiplicity.</b> <i>The
    algebraic multiplicity of $T$ corresponding to $\lambda$ is the
    multiplicity of $\lambda$ as a root of the characteristic
    polynomial of $T.$ The geometric multiplicity of $T$ corresponding
    to $\lambda$ is $\dim E_\lambda.$ They are denoted
    $\mathrm{alg}(\lambda)$ and $\mathrm{geo}(\lambda)$ resp.</i>
</p>

<p>
    <b>Proposition.</b> <i>The geometric multiplicity of $T$
    corresponding to $\lambda$ is less than or equal to the algebraic
    multiplicity corresponding to $\lambda$:

    $$1 \leq \dim E_\lambda = \mathrm{geo}(\lambda) \leq \mathrm{alg}(\lambda).$$</i>
</p>

<p id="vnwegji124">
    <b>Proposition: linear independence of
    eigenspaces.</b> <i>Eigenvectors and therefore eigenspaces
    corresponding to distinct eigenvalues are linearly
    independent.</i>
</p>

<p>
    <i>Proof.</i> Easy to show for two eigenvectors, then use
    induction.
</p>

<p>
    <b>Corollary.</b> <i>If T has $n$ distinct eigenvalues, then it is
    diagonalizable.</i>
</p>

<p>
    <b>Corollary.</b> <i>A matrix $A$ is diagonalizable iff the
    dimensions of its eigenspaces---i.e. the geometric multiplicities
    over all its eigenvalues---add up to the size of $A.$ In this case
    the geometric multiplicity of each eigenvalue is equal to its
    algebraic multiplicity.</i>
</p>

<p>
    <i>Proof.</i> By <a href="#vnwegji124">this theorem,</a> the
    eigenspaces are linearly independent. If their dimensions added up
    to less than $n,$ we'd have too few eigenvectors to make a basis
    for $V,$ and by the Diagonalizability Theorem, $A$ would not be
    diagonalizable. If they added up to more than $n,$ we'd have too
    many linearly independent vectors in $V.$ Conversely if they do
    add up to $n,$ then the union of bases for the eigenspaces forms
    an eigenbasis for $V,$ and $A$ is diagonalizable.
</p>

<p>
    <b>Example.</b> Let $T$ be the linear operator on $P_2(\RR)$
    defined by

    $$T(f(x)) = f'(x).$$

    The matrix representation of $T$ with respect to the standard
    basis $\beta$ is

    $$[T]_\beta = \begin{bmatrix}
    0 & 1 & 0 \\
    0 & 0 & 2 \\
    0 & 0 & 0
    \end{bmatrix}$$

    and the characteristic polynomial of $T$ is

    $$\det(T - tI) = \det \begin{bmatrix}
    -t &  1 & 0 \\
     0 & -t & 2 \\
     0 &  0 & -t
    \end{bmatrix} = -t^3,$$

    so $T$ has a single eigenvalue $\lambda = 0$ with algebraic
    multiplicity 3.
</p>

<p>
    Here the book shows that $T$ is not diagonalizable by noting that

    $$E_\lambda = \{\text{constant polynomials}\} = \span(1),$$

    and so

    $$\dim E_\lambda = 1 < 3 = \mathrm{alg}(\lambda).$$
</p>

<p>
    Another way to see that $T$ is not diagonalizable is to recall
    <a href="#asdvab94656">this proposition,</a> which says that <i>a
    diagonalizable matrix having only one eigenvalue is a scalar
    matrix.</i> Since $[T]_\beta$ has only one eigenvalue but is not
    scalar, it cannot be diagonalizable.
</p>

<p>
    <b>Corollary.</b> <i>Any upper triangular matrix with nonzero
    upper part whose diagonal entries are all the same is not
    diagonalizable, e.g.

    $$\begin{bmatrix}
     1 &  2 & 3 \\
     0 &  1 & 0 \\
     0 &  0 & 1
    \end{bmatrix}.$$</i>
</p>

<p>
    <b>Question.</b> <i>What about upper triangular matrices with more
    than one eigenvalues? What about ones whose eigenvalue blocks are
    nontrivial upper triangular? E.g.

    $$\begin{bmatrix}
     1 &  3 & 0 & 0 \\
     0 &  1 & 0 & 4 \\
     0 &  0 & 2 & 0 \\
     0 &  0 & 0 & 2 \\
    \end{bmatrix}.$$</i>
</p>

<p>
    A partial answer in that direction:
</p>

<div class="bside">
    I don't think the converse is true, but something close.
</div>

<p>
    <b>Conjecture: diagonalizable blocks.</b> <i>Let $T$ be a linear
    operator whose characteristic polynomial splits, and let $\beta$
    be any basis for $V$ s.t.

    $$A \equiv [T]_\beta = \begin{bmatrix}
     A_1 &        &     \\
         & \ddots &     \\
         &        & A_k \\
    \end{bmatrix},$$

    where $A_i$ are square blocks and the remaining entries are all
    zeroes. If all $A_i$ are diagonalizable, then so is $A.$</i>
</p>

<p>
    <i>Proof.</i> Let $A_i$ be diagonalizable for all $i.$ Then
    there are invertible matrices $Q_i$ s.t.

    $$Q_i^{-1} A_i Q_i = D_i,$$

    a diagonal matrix. Let

    $$Q = \begin{bmatrix}
     Q_1 &        &     \\
         & \ddots &     \\
         &        & Q_k \\
    \end{bmatrix}.$$

    Then

    \begin{align*}
    Q^{-1}AQ &= \begin{bmatrix}
     Q_1^{-1} &        &     \\
              & \ddots &     \\
              &        & Q_k^{-1} \\
    \end{bmatrix} \begin{bmatrix}
     A_1 &        &     \\
         & \ddots &     \\
         &        & A_k \\
    \end{bmatrix} \begin{bmatrix}
     Q_1 &        &     \\
         & \ddots &     \\
         &        & Q_k \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
     Q_1^{-1} A_1 Q_1 &        &     \\
                      & \ddots &     \\
                      &        & Q_k^{-1} A_k Q_k \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
      D_1 &        &     \\
          & \ddots &     \\
          &        & D_k \\
    \end{bmatrix}.
    \end{align*}
</p>

<p class="box">
    <b>Question.</b> <i>What is the relationship between
    diagonalizability and invertibility, <a href="#skdnsut164">aside
    from this</a>?</i>
</p>

<div class="bside">
    Almost everything is easier with diagonal matrices.
</div>

<p>
    So far it's not clear why we care about diagonalizability, but I
    think one reason is that diagonal matrices are very easy to
    multiply: you just multiply their diagonal entries
    element-wise---which makes it easy to calculate any polynomial
    function of the original matrix:
</p>

<p class="box">
    <b>Proposition.</b> <i>Let $A$ be diagonalizable with

    $$Q^{-1}AQ = D,$$

    where $D$ is diagonal, and let $f$ be a polynomial. Then

    $$f(A) = Q f(D) Q^{-1}.$$</i>
</p>

<p>
    <i>Proof.</i> Exercise. \qed
</p>

<p class="box">
    <b>Question.</b> <i>What is the probability that a random matrix
    is diagonalizable? Invertible?</i>
</p>

<p>
    <b>Exercise.</b> <i>Let $T$ be a linear operator on a finite
    dimensional vector space $V,$ and suppose there is a basis $\b$
    for $V$ s.t. $[T]_\b$ is upper triangular. Then the characteristic
    polynomial of $T$ splits. Similar result for matrix.</i>
</p>

<p>
    <i>Proof.</i> We know that the characteristic polynomial for $T$
    is the characteristic polynomial for $[T]_\b \equiv A$ for any
    basis $\b,$ which is

    $$\char A = \det(A - tI).$$

    Since $A$ is upper triangular, this is just

    $$\det(A - tI) = (A_{11} - t) \cdots (A_{nn} - t),$$

    which splits. \qed
</p>

<p>
    <b>Exercise.</b> <i>Let $T$ be a linear operator on a
    finite-dimensional vector space $V$ with distinct eigenvalues
    $\l_1,\ldots,\l_k$ with multiplicities $m_1,\ldots,m_k.$ Let $\b$
    be a basis for $V$ s.t. $[T]_\b$ is upper triangular. Then the
    diagonal entries of $[T]_\b$ are $\l_1,\ldots,\l_k,$ with each
    $\l_i$ appearing $m_i$ times.</i>
</p>

<p>
    I.e. the diagonal of an upper triangular matrix is as good as it
    gets: diagonalizing, if it were possible, won't change the
    diagonal entries, except perhaps their positions.
</p>

<p>
    <i>Proof.</i> There's not much to prove here. By the previous
    exercise, the eigenvalues $\l_i$ are precisely the diagonal
    entries $A_{ii},$ and since they have multiplicities $m_i,$ they
    appear in the diagonal $m_i$ times. \qed
</p>

<p class="box">
    <b>Proposition.</b> <i>Let $A$ be an $n\times n$ matrix that is
    similar to an upper triangular matrix, with distinct eigenvalues
    $\l_1,\ldots,\l_k$ with multiplicities $m_1,\ldots,m_k.$ Then:

    \begin{align*}
    \tr A &= \sum_{i=1}^k m_i \l_i = m_1 \l_1 + \cdots + m_k \l_k \\
    \det A &= \prod_{i=1}^k \l_i^{m_i} = \l_1^{m_1} \cdots \l_k^{m_k}.
    \end{align*}</i>
</p>

<p>
    <i>Proof.</i> Note that if $A$ itself is upper triangular, or
    diagonal, then these are true by definition of trace and det. Now
    we'll show that trace and det are invariant under similarity:
</p>

<div>
    Since determinant is multiplicative,

    \begin{align*}
    \det(Q^{-1} A Q) &= \det Q^{-1} \det A \det Q = \det A.
    \end{align*}

    As for trace, recall that

    $$\tr(AB) = \tr(BA),$$

    and so

    $$\tr(Q^{-1} A Q) = \tr(AQQ^{-1}) = \tr A.\qed$$
</div>

<p id="gasfdbnnd35616">
    <i>Alternative proof.</i> Another fun way to show this is to use
    Vieta's formulas, which say that:
</p>

<hr>

<h2>Vieta's Formulas</h2>

<div class="bside">
    Looking at these formulas, it seems trace and det are just special
    cases of a more general concept. It's all here, I just don't know
    what it means.
</div>

<p>
    <i>Let

    $$P(x) = a_n x^n + \cdots + a_0$$

    be a polynomial of degree $n$ with complex roots $x_1,\ldots,
    x_n.$ Then

    \begin{align*}
    -\frac{a_{n-1}}{a_n} &= x_1 + x_2 + \cdots + x_n \\
    \frac{a_{n-2}}{a_n} &= (x_1 x_2 + x_1 x_3 + \cdots + x_1 x_n) + (x_2 x_3 + x_2 x_4 + \cdots + x_2 x_n) + \cdots + x_{n-1} x_n \\
    &\cdots \\
    (-1)^n \frac{a_0}{a_n} &= x_1 x_2 \cdots x_n.
    \end{align*}</i>
</p>
<hr>

<p>
    It just so happens that <a href="#stdfh1267">earlier</a> we found
    an expression for the characteristic polynomial of $A$:

    $$\det(A - tI) = (-1)^n t^n + (-1)^{n-1} \tr(A) t^{n-1} + a_{n-2} t^{n-2} + \cdots + \det(A).$$</i>

    The result now easily follows by comparing trace and det with the
    first and last formulas in Vieta. \qed
</p>

<p class="box">
    <b>Corollary.</b> <i>Vieta's formulas also give us the other
    coefficients in the characteristic polynomial for a matrix $A.$
    Since similar matrices have the same eigenvalues and
    characteristic polynomial, these quantities are also invariant
    under similarity.</i>
</p>

<p>
    So, once again we see that being able to diagonalize, or at least
    upper-triangulate a matrix makes calculations much easier.
</p>

<p class="box">
    <b>Summary.</b> Ranking for matrix goodness according to
    diagonalizability:

    $$\text{neither} < \text{upper / lower triangular} < \text{diagonalizable}.$$
</p>

<h1>Direct Sums</h1>

<p>
    <b>Definition: linear sum.</b> <i>Let $W_1,\ldots,W_k$ be subspaces of $V.$
    Define the sum of these subspaces to be

    $$W_1 + \cdots + W_k = \sum W_i = \left\{ v_1 + \cdots + v_k : v_i \in W_i \right\}.$$</i>
</p>

<p>
    <b>Corollary.</b> <i>$\sum W_i$ is a subspace of $V.$</i>
</p>

<p>
    <i>Proof.</i> Exercise. \qed
</p>

<div class="aside">
    This is looking more and more like witchcraft. Human transmutation
    circle from Fullmetal Alchemist:
    <img src="images/7bff8caea74841eae91954057f115e53.jpg">
</div>

<div class="aside">
    <b>Definition: linearly independent subspaces.</b> A collection of
    subspaces are said to be linearly independent if any collection of
    vectors, one from each subspace, is linearly independent.
</div>

<p>
    <b>Definition: direct sums.</b> <i>Let $W, W_1,\ldots,W_k$ be
    subspaces of $V.$ We call $W$ the direct sum of
    $W_1,\ldots,W_k$ and write

    $$W = \bigoplus_{i=1}^k W_i = W_1 \oplus \cdots \oplus W_k$$

    if

    $$W = \sum_{i=1}^k W_i \quad \text{and} \quad W_j \cap \sum_{i\neq j} W_i = \left\{ 0 \right\}$$

    for all $j.$ In other words, $W$ is the direct sum of the $W_i$'s
    if they are linearly independent and span $W.$</i>
</p>

<p>
    <b>Example.</b> The $x, y,$ and $z$ axes direct sum to $\RR^3.$
</p>

<p>
    <b>Theorem: equivalent formulations of direct sum.</b> <i>Let
    $W_i$ be subspaces of a finite dimensional vector space $V.$ Then
    the following are equivalent:

    <ol>
    <li>$V = \bigoplus W_i.$</li>
    <li>$V = \sum W_i$ and for any $v_i \in W_i,$ $\sum v_i = 0$
    implies $v_i = 0$ for all $i.$</li>
    <li>Each $v\in V$ can be written uniquely as $v = \sum v_i$ where $v_i\in W_i.$</li>
    <li>If $\g_i$ is a basis for $W_i,$ then $\cup \g_i$ is a basis for $V.$</li>
    </ol>
    </i>
</p>

<p>
    <i>Proof.</i> Pretty straight forward. \qed
</p>

<p>
    <b>Theorem.</b> <i>A linear operator $T$ on a finite dimensional
    vector space $V$ is diagonalizable iff $V$ is the direct sum of
    the eigenspaces of $T.$</i>
</p>

<p>
    <i>Proof.</i> By the <a href="#nuhefgsd74862">diagonalizability
    theorem,</a> $T$ is diagonalizable iff there is a basis for $V$
    consisting of eigenvectors of $V,$ i.e. iff the eigenspaces of $T$
    span $V.$ By <a href="#vnwegji124">this theorem,</a> eigenspaces
    are linearly independent, and so they direct sum to $V.$ \qed
</p>

<p>
    <b>Remark.</b> The book has an alternative proof that uses the
    previous theorem.
</p>

<p>
    <b>Proposition.</b> <i>Let $T$ be an invertible linear operator on
    a finite dimensional vector space $V,$ and $\l$ be an eigenvalue
    of $T.$ Then the eigenspace of $T$ corresponding to $\l$ is also
    the eigenspace of $T^{-1}$ corresponding to $\l^{-1}.$ In
    addition, if $T$ is diagonalizable, then so is $T^{-1}.$</i>
</p>

<div class="bside">
    <b>*</b> Ah we have. Earlier we showed that similar matrices have
    the same eigenvalues, but not necessarily the same
    eigenvectors. So invertible matrices, which are similar, take that
    a step further, having both the same eigenvectors and eigenvalues.
</div>

<p>
    <i>Proof.</i> Let $x$ be an eigenvector of $T$ corresponding to
    $\l.$ We'll show that $x$ is also an eigenvector of $T^{-1}.$
    Omitting parantheses for simplicity:

    \begin{align*}
    Tx &= \l x \\
    \l^{-1} Tx &= x \\
    T^{-1} \l^{-1} Tx &= T^{-1} x \\
    \l^{-1} T^{-1} Tx &= T^{-1} x \\
    \l^{-1} x &= T^{-1} x.
    \end{align*}

    This also shows that if $\l$ is an eigenvalue of $T,$ then
    $\l^{-1}$ is an eigenvalue of $T^{-1},$ which I think we've showed
    before.<b>*</b>
</p>

<p>
    Now let $T$ be diagonalizable, so that there is an invertible
    matrix $Q$ s.t.:

    \begin{align*}
    Q T Q^{-1} = D,
    \end{align*}

    a diagonal matrix. Then

    \begin{align*}
    (Q T Q^{-1})^{-1} = Q T^{-1} Q^{-1} = D^{-1},
    \end{align*}

    which is also diagonal (the inverse of a diagonal matrix is
    diagonal). Therefore $T^{-1}$ is diagonalizable. \qed
</p>

<p>
    <b>Proposition.</b> <i>Let $A \in M_{n\times n}(F).$ Recall that
    $A$ and $A^t$ have the same characteristic polynomial and hence
    the same eigenvalues with the same multiplicities. For any
    eigenvalue $\l$ of $A$ and $A^t,$ let $E_\l$ and $E_\l^t$ denote
    the corresponding eigenspaces for $A$ and $A^t.$ Then:

    <ol>
    <li>$E_\l$ and $E_\l^t$ need not be the same.</li>
    <li>For any eigenvalue $\l,$ their dimensions are the same: $\dim E_\l = \dim E_\l^t.$</li>
    <li>If $A$ is diagonalizable, then $A^t$ is also diagonalizable.</li>
    </ol></i>
</p>

<p>
    <i>1 Proof.</i> Let

    \begin{align*}
    A = \begin{bmatrix}
    1 & 1 \\
    0 & 1
    \end{bmatrix} \quad \text{and} \quad
    A^t = \begin{bmatrix}
    1 & 0 \\
    1 & 1
    \end{bmatrix},
    \end{align*}

    which have only one eigenvalue $\l = 1.$ It's easy to check that
    $A$ has eigenvector

    $$v = \begin{bmatrix}
    1 \\
    0
    \end{bmatrix},$$ which is not an eigenvector of $A^t.$ \qed
</p>

<p>
    <i>2 Proof.</i> By the Rank Nullity Theorem,

    \begin{align*}
    \dim V &= \dim \ker(A - \l I) + \rank(A - \l I) \\
           &= \dim \ker(A^t - \l I) + \rank(A^t - \l I).
    \end{align*}

    By properties of transpose,

    $$A^t - \l I = (A - \l I)^t,$$

    and since transpose matrices have the same rank, we have

    $$\rank(A - \l I) = \rank(A^t - \l I),$$

    therefore

    $$\dim E_\l = \dim \ker(A - \l I) = \dim \ker(A^t - \l I) = \dim E_\l^t. \qed$$
</p>

<p>
    <i>3 Proof.</i> Let $A$ be diagonalizable, so that there is an
    invertible matrix $Q$ s.t.

    $$QAQ^{-1} = D$$

    is diagonal. Then

    \begin{align*}
    (QAQ^{-1})^t = (Q^{-1})^t A^t Q^t = (Q^t)^{-1} A^t Q^t = D^t,
    \end{align*}

    which is diagonal, so $A^t$ is also diagonalizable. \qed
</p>

<h1>Reference</h1>

<ol>
<li>Linear Algebra by Friedberg, Insel, and Spence.</li>
<li>Linear Algebra video lectures by Gilbert Strang.</li>
<li>Everything else from the web.</li>
</ol>

</div>
</body>
</html>
