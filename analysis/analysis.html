<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>math notes and stuff</title>
<link rel="stylesheet" href="../css/global.css">

<!-- this config must be before MathJax.js: -->
<script src="../js/mathjax.config.js"></script>
<script src="../js/MathJax/MathJax.js"></script>
<!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script> -->

<script src="../js/jquery-3.1.0.min.js"></script>
<script src="../js/smartquotes.js"></script>
<script src="../js/global.js"></script>

</head>
<body>
<div id="content">

<h1>Analysis</h1>

<hr>

<div class="epigraph">
    <div class="quote">Be wary of gorgeous view.</div><hr>
    <div class="author">Dark Souls</div>
</div>
<div class="clearboth"></div>

<h1>Polynomial Approximation</h1>

<figure>
    <img src="images/MH8onJG.png">
    <figcaption>Mathematical Approximation</figcaption>
</figure>

<p>
    <b>Theorem: Taylor's Theorem.</b> <i>If $f', \ldots, f^{(n+1)}$
are defined on $[a, x],$ then $$f(x) = f(a) + f'(a)(x - a) + \cdots +
\frac{f^{(n)}(a)}{n!}(x - a)^n + R_n(x)$$ where $R_n(x) =
\frac{f^{(n+1)}(t)}{(n+1)!}(x - a)^{n+1}$ for some $t$ in $(a,
x).$</i>
</p>

<p>
    <b>Note.</b> The Mean Value Theorem is a special case of Taylor's Theorem:
$$f(b) = f(a) + f'(c)(b - a)$$
for some $c$ between $a$ and $b.$
</p>

<h1>Sequences</h1>

<figure>
    <img src="images/numbers.jpg">
    <figcaption>LOST</figcaption>
</figure>

<p>
    <b>Monotone Convergence Theorem.</b> <i>A sequence which is
    increasing and bounded above by a supremum converges to the
    supremum. Similarly a decreasing lower-bounded sequence
    converges to its infimum.</i>
</p>

<p>
    <b>Theorem: Uniform Limit Theorem.</b> <i>Uniform convergence of functions preserves
    continuity, i.e. if $f_n$ are continuous and approach $f$
    uniformly, then $f$ is continuous.</i>
</p>

<p>
    <b>Proposition.</b> <i>The uniform limit of uniformly continuous
    functions is uniformly continuous.</i>
</p>

<p>
    <b>Question.</b> <i>What about differentiability, i.e. if $f_n$
    are differentiable and approach $f$ uniformly, is $f$ always
    differentiable, and is $\lim f_n' = f'$?</i>
</p>

<p>
    <b>Example.</b> No to the second question: the functions $f_n(x) =
    \frac{1}{n} \sin(nx)$ converge uniformly to the zero function,
    which <i>is</i> differentiable. But, the limit of the
    derivatives don't exist. What about just differentiability?
</p>

<figure>
    <img src="images/uniformconvergence1.jpg">
    <figcaption></figcaption>
</figure>

<p>
    <b>Example.</b> Still No, e.g. the functions $f_n(x) = \sqrt{x^2 +
    1/n}$ converge uniformly to $f = |x|,$ which is not differentiable
    at zero.
</p>

<figure>
    <img src="images/uniformconvergence2.png">
    <figcaption></figcaption>
</figure>

<p>
    <b>Example.</b> The Weierstrass function $$f(x) =
    \sum^\infty_{k=0} a^k \cos(b^k \pi x),$$ for appropriate values
    $a$ and $b,$ is the uniform limit of $$f_n = \sum^n_{k=0} a^k
    \cos(b^k \pi x),$$ but is nowhere differentiable.
</p>

<figure>
    <img src="images/nodiff1.jpg">
    <figcaption></figcaption>
</figure>

<p>
    <b>Question.</b> <i>Is the Koch snowflake nowhere
    differentiable?</i>
</p>

Yes. Proof?

<figure>
    <img src="images/biomimicry-koch-snowflake.jpg">
    <figcaption></figcaption>
</figure>

<p>
    <b>Definition.</b> <i>Let $\{a_n\}$ be a sequence, and $0 \leq a <
    b \leq 1.$ Let $N(n; a, b)$ be the number of integers $j \leq n$
    s.t. $a_j \in [a, b].$ A sequence $\{a_n\}$ of numbers in $[0, 1]$
    is called uniformly distributed in $[0, 1]$ if $$\lim_{n \to
    \infty} \frac{N(n; a, b)}{n} = b - a$$ for all $a, b,$ s.t. $0
    \leq a < b \leq 1.$</i>
</p>

<p>
    <b>Proposition.</b> <i>If $s$ is a step function on $[0, 1],$ and
    $\{a_n\}$ is uniformly distributed in $[0, 1],$ then $$\int_0^1 s
    = \lim_{n\to\infty} \frac{s(a_1) + \cdots + s(a_n)}{n}.$$</i>
</p>

<p>
    <i>Proof.</i>
Let $\Delta_1, \ldots, \Delta_m$ be a partition of $[0, 1]$ corresponding to the steps in $s.$ Then (with a slight abuse of notation) we have
\begin{align*}
\int_0^1 s &= \sum_{i=1}^m s(\Delta_i) \Delta_i \\
&= \sum_{i=1}^m s(\Delta_i) \lim_{n \to \infty} \frac{1}{n} N(n; \Delta_i) \\
&= \lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^m s(\Delta_i) N(n; \Delta_i) \\
&= \lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n s(a_i).
\end{align*}
</p>

<p>
    <b>Proposition.</b> <i>If $f$ is integrable on $[0, 1],$ and
    $\{a_n\}$ is uniformly distributed in $[0, 1],$ then $$\int_0^1 f
    = \lim_{n\to\infty} \frac{f(a_1) + \cdots + f(a_n)}{n}.$$</i>
</p>

<p>
    <i>Proof.</i>  Since $f$ is integrable, there is a step function
$s$ such that $\int_0^1 f$ is close to $\int_0^1 s,$ which is close to
$\frac{s(a_1) + \cdots + s(a_n)}{n},$ which is close to $\frac{f(a_1)
+ \cdots + f(a_n)}{n}.$
</p>

<h1>Bolzano-Weierstrass Theorem</h1>

<p>
    <b>Theorem: Bolzano-Weierstrass Theorem.</b> <i>An infinite
    sequence contained in a closed interval $I$ has a limit point in
    $I.$</i>
</p>

<figure>
    <img src="images/Bolzanoâ€“Weierstrass_theorem_-_step_7.png">
    <figcaption></figcaption>
</figure>

Proof uses the Nested Interval Theorem:

<p>
    <b>Theorem: Nested Interval Theorem.</b> <i>The intersection of closed nested intervals is nonempty. If the
    interval lengths tend to zero, then the intersection is a
    point. Otherwise it's a closed interval.</i>
</p>

<figure>
    <img src="images/511px-Illustration_nested_intervals.png">
    <figcaption></figcaption>
</figure>

<p>
    <b>Definition.</b> <i>A function $f$ defined on an interval $I$ is
    called limitful if $\lim\limits_{y\to a} f(y)$ exists for all $a
    \in I.$</i>
</p>

<p>
    <b>Proposition.</b> <i>Let $f$ be a limitful function on $[0,
    1].$ Then for any $\e > 0$ there are only finitely many
    points $a \in [0, 1]$ with $$|\lim\limits_{y\to a} f(y) - f(a)| >
    \e.$$</i>
</p>

<p>
    <i>Proof.</i> Suppose that there are infinitely many such points
    $a.$ Then by the Bolzano-Weierstrass Theorem, these points have a
    limit $x \in [0, 1].$ Let $$L := \lim\limits_{y \to x} f(y) =
    \lim\limits_{a\to x} f(a).$$
</p>

<p>
    The condition $$|\lim\limits_{y\to a} f(y) - f(a)| > \e$$
    means that for $y$ close to $a,$ $f(y)$ is far from $f(a).$
    Similarly $\lim\limits_{a \to x} f(a) = L$ means that for $a$
    close to $x,$ $f(a)$ is close to $L.$ Together this means that for
    $y$ close to $x$ and $y$ close to $a$ for some $a,$ we have that
    $f(y)$ is far from $L,$ but this contradicts the fact that $L =
    \lim\limits_{y \to x} f(y),$ i.e. for all $y$ close to $x,$ $f(y)$
    is close to $L.$
</p>

<p>
    <i>Proof 2.</i> Another way to see this is to let $a_n$ be the
convergent subsequence given by Bolzano-Weierstrass, and choose $y_n$
close to $a_n$ so that $|f(y_n) - f(a_n)|$ is big. Since $|f(a_n) -
L|$ is small, the triangle inequality $$|f(y_n) - L| \geq |f(y_n) -
f(a_n)| - |f(a_n) - L|$$ says $|f(y_n) - L|$ is big, thus
contradiction.
</p>

<p>
    <b>Theorem.</b> <i>A limitful function on $[0, 1]$ has at most
    countably many discontinuities.</i>
</p>

<p>
    <i>Proof.</i> By the previous Proposition, for each $\e_q >
    0$ there are at most finitely many points $a$
    s.t. $$|\lim\limits_{y\to a} f(y) - f(a)| > \e_q.$$ Taking a
    sequence $\e_q \in \mathbf Q$ converging to zero, we have countably
    many such points $a.$
</p>

<p>
    <b>Corollary.</b> <i>
If $f$ has only removable discontinuities, then $f$ is continuous except at countably many points. In particular, $f$ cannot be discontinuous everywhere.
</i>
</p>

<h1>Infinite Series</h1>

<img src="images/quote-infinite-growth-of-material-consumption-in-a-finite-world-is-an-impossibility-e-f-schumacher-52-17-01.jpg">

<p><b>Definition.</b> <i>The sequence $a_n$ is summable if the
sequence $$s_n = a_1 + \cdots + a_n$$ converges. In this case
$\lim\limits_{n\to \infty} s_n$ is denoted by
$\sum\limits_{n=1}^\infty a_n,$ and we say $\sum\limits_{n=1}^\infty
a_n$ converges.</i></p>

<h1>Cauchy Criterion</h1>

<p><b>Proposition.</b> <i>The series $\sum\limits_{n=1}^\infty a_n$
converges iff its tail vanishes: $$\lim_{m,n\to \infty} a_n + \cdots +
a_m = 0.$$</i></p>

<figure>
    <img src="images/2000px-Cauchy_sequence_illustration.png">
    <figcaption>A Cauchy sequence.</figcaption>
</figure>

<h1>Vanishing Condition</h1>

<p><b>Proposition.</b> <i>If $a_n$ is summable, then
$$\lim_{n\to\infty} a_n = 0.$$</i></p>

<p><b>Example.</b> The sequence $1, -1, 1, -1, \ldots$ is not
summable.</p>

<p><b>Noncounterexample.</b> The vanishing condition is necessary, but
not sufficient for summability, e.g. the harmonic series $$\sum_{n}
\frac{1}{n}$$ is not convergent even though the terms go to zero.</p>

<figure>
    <img src="images/Integral_Test.png">
    <figcaption>Harmonic Series nonconvergent because log is unbounded.</figcaption>
</figure>

<p><b>Example of convergent series.</b> The geometric series is
convergent for $|r| < 1$: $$\sum_{n=0}^\infty r^n = \frac{1}{1 - r}.$$
For $|r| \geq 1,$ the series isn't convergent because the terms aren't
vanishing.</p>

<figure>
    <img src="images/proof-without-words-halves.gif">
    <figcaption>You can also visualize this on a line like in Zeno's Paradox.</figcaption>
</figure>

<figure>
    <img src="images/zeno-paradox-arrow.png">
    <figcaption></figcaption>
</figure>

<h1>Boundedness Criterion</h1>

<p><b>Proposition.</b> <i>A nonnegative sequence $a_n$ is summable iff
the partial sums $s_n$ is bounded.</i></p>

<h1>Comparison Tests</h1>

<p><b>Theorem.</b> <i>Suppose that $0 \leq a_n \leq b_n$ for all $n.$ If
$\sum\limits_n b_n$ converges, then so does $\sum\limits_n a_n.$</i></p>

<p>Useful for comparing series by orders of magnitude:</p>

<p><b>Example.</b> The sum $$\sum_n \frac{2 + \sin^3(n + 1)}{2^n +
n^2}$$ converges by comparison with $$\sum_n \frac{1}{2^n},$$ which is
a geometric series. Similarly $$\sum_n \frac{n+1}{n^2+1}$$ is not
convergent because its terms are of the same order of magnitude as
those of the harmonic series $$\sum_n \frac{1}{n}.$$</p>

<p><b>Theorem.</b> <i>If $a_n, b_n > 0$ and $\lim\limits_{n\to\infty}
\frac{a_n}{b_n}$ is a nonzero constant, then $\sum\limits_n a_n$ converges
iff $\sum\limits_n b_n$ converges.</i></p>

<p><b>Theorem: Ratio Test.</b> <i>Let $a_n > 0$ for all $n,$ and
suppose that $$\lim_{n\to\infty} \frac{a_{n+1}}{a_n} = r.$$ Then the
series

$$\sum\limits_{n} a_n \text{ is } \begin{cases}
\text{convergent if $r < 1$} \\
\text{divergent if $r > 1$} \\
\text{insurgent if $r = 1.$}
\end{cases}$$
</i></p>

<div class="epigraph">
     <div class="quote">
        <img src="images/divergent-trilogy-covers.png">
     </div><hr><div class="author">Sadly, insurgent is not the technical
     term for inconclusive convergence test. Yet.</div>
</div><div class="clearboth"></div>

<p>
    <b>Example.</b> The harmonic series has ratio

    $$\lim_{n\to\infty} \frac{a_{n+1}}{a_n} = 1,$$

    but is divergent. On the other hand the series

    $$\sum_n \frac{1}{n^2}$$

    also has ratio 1 but is convergent, by the integral test.
</p>

<p>
    <b>Theorem: Integral Test.</b> <i>Suppose $f$ is positive and
    decreasing on $[1, \infty],$ and that $f(n) = a_n$ for all
    n. Then $\sum a_n$ converges iff the limit

    $$\int_1^\infty f = \lim_{A\to\infty} \int_1^A f$$

    exists.</i>
</p>

<div class="aside">
    * <b>Question.</b> What is the sum of this series? The integral
    has such a nice form when it exists: $\int_1^\infty \frac{1}{x^p}
    dx = \frac{1}{p-1}.$ Not to be confused with the geometric series
    $\sum_n r^n = \frac{1}{1 - r}.$
</div>

<p>
    <b>Example.</b> Convergence of the series*

    $$\sum_n \frac{1}{n^p}$$

    for $p > 0$ depends on the existence of the integral

    \begin{align*}
    \int_1^\infty \frac{1}{x^p} dx &= \lim_{A\to\infty} \int_1^A \frac{1}{x^p} dx \\
                                   &= \lim_{A\to\infty} \begin{cases}
                                   -\frac{1}{p-1} \frac{1}{A^{p-1}} + \frac{1}{p-1} & \text{ if } p \neq 1 \\
                                   \log A                                           & \text{ if } p = 1
                                   \end{cases}
    \end{align*}

    which exists iff $p > 1.$ In particular the harmonic series
    diverges and $\sum_n \frac{1}{n^2}$ converges.
</p>

<h2>Absolute and Conditional Convergence</h2>

<p>
    <b>Definition.</b> <i>A series $\sum a_n$ is absolutely
    convergent if $\sum |a_n|$ converges.</i>
</p>

<p id="aksfjn12124">
    <b>Absolute Convergence Theorem.</b> <i>Every absolutely convergent
    series is convergent. Moreover, a series is absolutely convergent
    iff the series formed from its positive and negative terms both
    converge.</i>
</p>

<p id="nxm3m4">
    <b>Note.</b> Let $\sum p_n$ and $\sum q_n$ be the series made of
    positive and negative terms of $\sum a_n.$ If $\sum a_n$ is
    conditionally convergent, i.e. it converges but not absolutely,
    then by the Absolute Convergence Theorem, one of $\sum p_n$ and
    $\sum q_n$ must diverge. In fact, they must both diverge, because
    otherwise one converges and one diverges, and so one has bounded
    and the other has unbounded partial sums, so $\sum a_n$ has
    unbounded partial sums and $\sum a_n$ diverges, contradicting that
    it converges.
</p>

<div id="dsf87hh34" class="aside">
    <img src="images/alternatingseries.png">
</div>

<p>
    <b>Theorem: Leibniz's Alternating Series Theorem.</b> <i>Suppose that

    $$a_1 \geq a_2 \geq \cdots \geq 0$$

    and that

    $$\lim_{n\to\infty} a_n = 0.$$

    Then the series

    $$\sum_n (-1)^{n+1} a_n = a_1 - a_2 + a_3 - \cdots$$

    converges.</i>
</p>

<p>
    <i>Proof.</i> <a href="#ijfasdf09234">See note following the proof of this proposition:</a>
</p>

<p>
    <b>Proposition: alternating series remainder estimate.</b> <i>Let

    $$\sum_n (-1)^{n+1} a_n$$

    be an alternating series as in Leibniz's Theorem above. Then

    $$\left|\sum_n (-1)^{n+1} a_n - (a_1 - a_2 + \cdots \pm a_N)\right| \leq a_N$$

    for any $N,$ i.e. the remainder of the series is no larger than
    the term $a_N$ preceding it.</i>
</p>

<p>
    <i>Proof sketch.</i> Note that it's enough to show this for $a_1,$
    then it'll be true for any $N$ because we can just cut the series
    there and the remainder will be its own alternating series. So we
    need to show

    $$|a_2 - a_3 + \cdots| \leq a_1.$$
</p>

<p>
    Clearly, the LHS is greatest when its even terms $a_2, a_4, a_6,
    \ldots$ are at least as big as the odd terms preceding them. But
    the odd terms have to go to zero eventually, so we have the
    following picture:
</p>

<img src="images/leibnizalternatingseries.png">

<p>
    where the red bars represent $a_i,$ and the green bars represent

    $$|a_2 - \cdots \pm a_i|.$$

    Looking at the picture, we see that even when the even terms are
    as big as possible, the entire series can never get past its
    original value $a_1.$
</p>

<p id="ijfasdf09234">
    <b>Note.</b> Using this estimate we can give a quick proof of
    Leibniz's theorem above: since the remainder is bounded by $a_N
    \longrightarrow 0,$ it too approaches zero, and so the series
    converges by the Cauchy Criterion.
</p>

<h2>Rearrangement Theorems</h2>

<p>
    <b>Rearrangement Theorem for Conditionally Convergent
    Series.</b> <i>If $\sum a_n$ converges but not absolutely, then
    for any number $\alpha$ there is a rearrangement $b_n$ of
    $a_n$ s.t. $\sum b_n = \alpha.$</i>
</p>

<p>
    <i>Proof.</i> <a href="#nxm3m4">By a previous note,</a> $\sum a_n$
    contains infinitely many positive and negative terms, and
    furthermore the positive and negative sums are divergent, so they
    can reach any positive or negative number $\alpha.$ Once you're
    close to $\alpha$ you can rearrange the terms to make it go a
    little over, and then a little under, and so on, converging to
    $\alpha,$ a bit like <a href="#dsf87hh34">this picture.</a>
</p>

<p>
    Note that you need both the positive and negative series to be
    divergent, because otherwise say $\sum p_n = N$ is convergent,
    then you could pick $\alpha > N$ and any rearranged partial sums
    of $\sum a_n$ could never reach $\alpha.$
</p>

<p>
    <b>Rearrangement Theorem for Absolutely Convergent
    Series.</b> <i>If $\sum a_n$ converges absolutely and $b_n$ is any
    rearrangement of $a_n,$ then $\sum b_n$ also converges
    absolutely and $$\sum a_n = \sum b_n.$$</i>
</p>

<p>
    <b>Note.</b> If $\sum a_n$ converges absolutely, that doesn't mean
    $$\sum |a_n| = \sum a_n.$$ E.g. the series

    $$\frac{1}{2} - \frac{1}{4} + \frac{1}{8} - \cdots = \frac{1}{3}$$

    is absolutely convergent, with absolute sum

    $$\frac{1}{2} + \frac{1}{4} + \frac{1}{8} + \cdots = 1.$$

    The way you calculate the first sum is as follows: split the
    series into its positive and negative terms:

    \begin{align*}
    S \equiv \sum a_n^+ &= \frac{1}{2} + \frac{1}{2^3} + \frac{1}{2^5} + \cdots \\
    T \equiv \sum a_n^- &= -\frac{1}{2^2} - \frac{1}{2^4} - \frac{1}{2^6} - \cdots \\
                        &= -\frac{1}{2}\left(\frac{1}{2} + \frac{1}{2^3} + \frac{1}{2^5} + \cdots\right) \\
                        &= -\frac{S}{2}.
    \end{align*}

    Now compute the absolute series in terms of S:

    \begin{align*}
    1 = \sum |a_n| &= \sum a_n^+ - \sum a_n^- \\
    1 &= S - T = S + \frac{S}{2} \\
    S &= \frac{2}{3}.
    \end{align*}

    Next compute T:

    \begin{align*}
    1 &= S - T = \frac{2}{3} - T \\
    T &= -\frac{1}{3},
    \end{align*}

    and finally:

    \begin{align*}
    \sum a_n = S + T = \frac{2}{3} - \frac{1}{3} = \frac{1}{3}.
    \end{align*}
</p>

<p>
    <b>Note.</b> We used in the last example a fact that should
    probably be proved:
</p>

<div class="aside">
    * Just realized this result follows immediately from
    the <a href="#aksfjn12124">Absolute Convergence Theorem.</a> In
    fact $\sum a_n$ is absolutely convergent.
</div>

<p>
    <b>Proposition.*</b> <i>Let $\sum a_n$ be a series s.t. $\sum
    a_n^+$ and $\sum a_n^-$ are both convergent. Then $\sum a_n$ is
    convergent and

    $$\sum a_n = \sum a_n^+ + \sum a_n^-.$$</i>
</p>

<p>
    <i>Proof.</i> Let $s_n$ denote a partial sum and $s$ its limit. Since

    $$s_n = s_n^+ + s_n^-$$

    we have

    \begin{align*}
    |s_n - (s^+ + s^-)| &= |s_n^+ + s_n^- - (s^+ + s^-)| \\
    &= |s_n^+ - s^+ + s_n^- - s^-| \\
    &\leq |s_n^+ - s^+| + |s_n^- - s^-| \\
    &< 2 \e
    \end{align*}

    with appropriately defined $\e.$ Therefore $s_n
    \longrightarrow s^+ + s^-.$
</p>

<p>
    <b>Theorem: product of absolutely convergent series.</b> <i>If
    $\sum a_n$ and $\sum b_n$ are absolutely convergent, and $c_n$ is
    any sequence containing the products $a_i b_j$ for all $i, j,$ then

    $$\sum c_n = \sum a_n \cdot \sum b_n.$$</i>
</p>

<h2>Root Test</h2>

<p>
    <b>Proposition: root test.</b> <i>Let $a_n > 0$ for all $n,$ and
    suppose that $$\lim_{n\to\infty} \sqrt[n]{a_n} = r.$$ Then the
    series

    $$\sum\limits_{n} a_n \text{ is } \begin{cases}
    \text{convergent if } r < 1 \\
    \text{divergent if } r > 1.
    \end{cases}$$

    The test is inconclusive if $r = 1.$</i>
</p>

<p>
    <i>Proof.</i> If $r > 1,$ then $a_n > 1$ for sufficiently large
    $n,$ and so the terms aren't approaching zero and the series
    diverges. Let $r < 1.$ Then

    $$\lim_{n\to\infty} \sqrt[n]{a_n} = r < 1$$

    means that for any $s \in (r, 1)$ there exists an $N$ s.t. for all $n > N,$

    $$\sqrt[n]{a_n} \leq s, \quad \text{ or } \quad a_n \leq s^n.$$

    Therefore the remainder of the series after $a_N$ is

    $$\sum_{n> N} a_n \leq \sum_{n>N} s^n,$$

    which converges because it's a geometric series.
</p>

<p>
    <b>Remark.</b> The ratio test above can be proved similarly.
</p>

<p>
    <b>Note.</b> If the ratio test works, then the root test also
    works, by the following:
</p>

<p>
    <b>Lemma: limit of ratios is limit of roots.</b> <i>If $a_n >
    0$ and

    $$\lim_{n\to\infty} \frac{a_{n+1}}{a_n} = L,$$

    then

    $$\lim_{n\to\infty} \sqrt[n]{a_n} = L.$$</i>
</p>

<p>
    <i>Proof.</i> The limit

    $$\lim_{n\to\infty} \frac{a_{n+1}}{a_n} = L$$

    means that for any $\e,$ there is an $N$ s.t. for all $n \geq N,$ we
    have

    $$L - \e \leq \frac{a_{n+1}}{a_n} \leq L + \e.$$

    Therefore,

    $$(L - \e)^n \leq \frac{a_{N+1}}{a_N} \cdots \frac{a_{N+n}}{a_{N+n-1}}
    = \frac{a_{N+n}}{a_N} \leq (L + \e)^n \\

    L - \e \leq \sqrt[n]{ \frac{a_{N+n}}{a_N} } \leq L + \e.$$

    In other words,

    $$\lim_{n\to\infty} \sqrt[n]{ \frac{a_{N+n}}{a_N} } = L.$$

    Since $a_N > 0$ is fixed,

    $$\lim_{n\to\infty} \sqrt[n]{a_N} = 1,$$

    and so

    $$\lim_{n\to\infty} \sqrt[n]{ \frac{a_{N+n}}{a_N} }
    = \lim_{n\to\infty} \sqrt[n]{a_{N+n}}
    = \lim_{m\to\infty} \sqrt[m]{a_{m}} = L.$$
</p>

<p>
    <b>Proposition.</b> <i>If $\Sum a_n$ converges absolutely, then
    for any subsequence $b_n$ of $a_n,$ $\Sum b_n$ also converges
    absolutely.</i>
</p>

<p>
    <i>Proof.</i> We want to show that for any $\e > 0,$ there is an
    $N$ s.t. for all $M > N,$

    $$|b_N| + \cdots + |b_M| < \e,$$

    so that $\Sum |b_n|$ converges by the Cauchy Criterion.

    But $\Sum a_n$ converges absolutely, so for any such $b_n$ tail we
    have an $a_n$ tail containing it, s.t.

    $$|b_N| + \cdots + |b_M| \leq |a_N| + \cdots + |a_M| < \e,$$

    and so $\Sum b_n$ converges absolutely.
</p>

<p>
    <b>Example.</b> This is false if $\sum a_n$ does not converge
    absolutely, because then we know that its positive and negative
    sums diverge, and so we can choose $\sum b_n$ to be either
    subseries. E.g. the alternating harmonic series

    $$\sum \frac{(-1)^{n+1}}{n}$$

    converges conditionally, but the subseries

    $$\sum_{\text{odd}} \frac{1}{n} = 1 + \frac{1}{3} + \frac{1}{5} + \cdots
    \quad \text{and} \quad \sum_{\text{even}} \frac{1}{n} = -\frac{1}{2} - \frac{1}{4} - \cdots$$

    both diverge.
</p>

<img src="images/htvol3-pic4.jpg">

<p>
    <b>Corollary.</b> <i>If $\sum a_n$ converges absolutely, then

    $$\sum a_n = \sum_{\text{odd}} a_n + \sum_{\text{even}} a_n
               = (a_1 + a_3 + \cdots) + (a_2 + a_4 + \cdots).$$</i>
</p>

<p>
    <i>Proof.</i> By the previous proposition, we know that the even
    and odd sums both converge absolutely. Let $A, O, E$ be the sums,
    and let $A_n, O_n, E_n$ be the $n$-th partial sums for the whole,
    even, and odd series:

    \begin{align*}
    A_n &= a_1 + a_2 + \cdots + a_n \\
    O_n &= a_1 + a_3 + \cdots + a_{(n)} \\
    E_n &= a_2 + a_4 + \cdots + a_{[n]}, \\
    \end{align*}

    where $(n)$ and $[n]$ denote the largest integer less than or
    equal to $n$ which is odd and even, so that in particular

    $$A_n = O_n + E_n.$$

    Then

    \begin{align*}
    |A_n - (O + E)| &= |A_n - (O + E) + (O_n - O_n) + (E_n - E_n)| \\
                    &\leq |A_n - O_n - E_n| + |O - O_n| + |E - E_n| \\
                    &< 0 + \e + \e,
    \end{align*}

    for appropriately chosen $\e.$ Therefore $A_n \longrightarrow O + E.$
</p>

<p>
    <b>Corollary.</b> <i>In fact we can do a little bit better: if
    $\sum a_n$ converges absolutely, then for any partition $b_n, c_n$
    of $a_n,$

    $$\sum a_n = \sum b_n + \sum c_n.$$</i>
</p>

<div class="aside">
    It's stronger in the forward but weaker in the backward direction.
</div>

<p>
    <b>Corollary: Absolute Convergence Theorem II.</b> <i>Combining
    this with the Absolute Convergence Theorem, we get: $\sum a_n$
    converges absolutely iff $\sum b_n$ and $\sum c_n$ converge for
    any partition $b_n, c_n$ of $a_n.$ In that case,

    $$\sum a_n = \sum b_n + \sum c_n.$$</i>
</p>

<h1>Reference</h1>

<ol>
<li>Spivak's Calculus.</li>
<li>Everything else from the web.</li>
</ol>

</div>
</body>
</html>
