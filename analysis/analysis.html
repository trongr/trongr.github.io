<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Analysis</title>
<link rel="stylesheet" href="../css/global.css">

<!-- this config must be before MathJax.js: -->
<script src="../js/mathjax.config.js"></script>
<script src="../js/MathJax/MathJax.js"></script>
<!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script> -->

<script src="../js/jquery-3.1.0.min.js"></script>
<script src="../js/smartquotes.js"></script>
<script src="../js/global.js"></script>

</head>
<body>
<div id="content">

<h1>Analysis</h1>

<img src="images/23c4bdff2b3ebcb89382735a643aba37bdd5f79d.jpg">

<div class="epigraph">
    <div class="quote">Math is a lot like Dark Souls.</div>
    <div class="author"></div>
</div>

<h1>Fundamental Theorem of Calculus</h1>

<h3>First fundamental theorem of calculus</h3>

<p>
    <b>Theorem.</b> <i>Let $f$ be integrable on $[a, b]$ and define
    $F$ on $[a, b]$ by

    $$F(x) = \int_a^x f.$$

    If $f$ is continuous at $c \in [a, b],$ then $F$ is differentiable
    at $c,$ and

    $$F'(c) = f(c).$$</i>
</p>

<p>
    I.e. an antiderivative $F$ of an integrable function $f$ is
    differentiable with derivative $F'$ equal to $f.$
</p>

<p>
    <b>Corollary.</b> <i>If $f$ is continuous on $[a, b]$ and $f = g'$
    for some function $g,$ then

    $$\int_a^b f = g(b) - g(a).$$</i>
</p>

<h3>Second fundamental theorem of calculus</h3>

<p>
    <b>Theorem.</b> <i>If $f$ is integrable on $[a, b]$ and $f = g'$
    for some $g,$ then

    $$\int_a^b f = g(b) - g(a).$$</i>
</p>

<h1>Trigonometric Functions</h1>

<h3 id="hsdgb3662">Lagrange's Trigonometric Identities</h3>

<div class="aside">
    The functions

    $$1 + 2 \sum_{k=1}^n \cos kx = \frac{\sin(n + \frac{1}{2})x}{\sin \frac{x}{2}}$$

    are also called the Dirichlet kernel, used in Fourier analysis.

    <img src="images/Dirichlet_kernel_anime.gif">
</div>
<p>
    <b>Proposition.</b> <i>

    \begin{align*}
    \frac{1}{2} + \sum_{k=1}^n \cos kx &= \frac{\sin(n + \frac{1}{2})x}{2\sin \frac{x}{2}} \\
    \sum_{k=1}^n \sin kx &= \frac{\sin \frac{n+1}{2}x \sin \frac{n}{2}x}{\sin \frac{x}{2}}.
    \end{align*}
    </i>
</p>

<p>
    <i>Proof.</i> We'll just prove the first one, the second is
    similar. A well known trigonometric identity says that

    $$2 \cos \th \sin \ff = \sin(\th + \ff) - \sin(\th - \ff),$$

    so that

    $$2 \cos kx \sin \frac{x}{2} = \sin(k + \tfrac{1}{2})x - \sin(k - \tfrac{1}{2})x.$$

    Summing up both sides for $k = 1:n$ gives

    \begin{align*}
    \sum_{k=1}^n 2 \cos kx \sin \frac{x}{2} &= \sin(n+\tfrac{1}{2})x - \sin\frac{x}{2} \quad \text{ (RHS telescopes)} \\
    \frac{1}{2} + \sum_{k=1}^n \cos kx &= \frac{\sin(n + \frac{1}{2})x}{2\sin \frac{x}{2}}. \qed
    \end{align*}
</p>

<h1>Integrals</h1>

<h3 id="agdggi426">Abel's Lemma</h3>

<p>
    <b>Lemma.</b> <i>Suppose $b_n \geq 0$ is non-increasing, and
    that

    $$m \leq a_1 + \cdots + a_n \leq M$$

    for all $n.$ Then

    $$b_1 m \leq a_1 b_1 + \cdots + a_n b_n \leq b_1 M.$$</i>
</p>

<p>
    <i>Proof.</i> We'll prove the right inequality first cause it's
    easier:

    $$a_1 b_1 + \cdots + a_n b_n \leq a_1 b_1 + \cdots + a_n b_1 \leq b_1 M.$$

    To prove the left inequality we'll use induction, and to keep
    things simple we'll first show it for $n=1$ and $2.$ First:

    \begin{align}
    b_1 m \leq a_1 b_1.
    \label{hsdfb342}
    \end{align}

    Now we want to show that

    $$b_1 m \leq a_1 b_1 + a_2 b_2.$$

    There are two cases: either $a_2 \geq 0$ or $a_2 < 0.$ If $a_2
    \geq 0,$ then $a_2 b_2 \geq 0,$ and adding a non-negative number
    to the RHS of $\ref{hsdfb342}$ doesn't make it less:

    $$b_1 m \leq a_1 b_1 + a_2 b_2.$$

    On the other hand, suppose $a_2 < 0.$ Since $b_1 \geq b_2,$ we
    have $a_2 b_1 \leq a_2 b_2.$ Therefore

    $$b_1 m \leq a_1 b_1 + a_2 b_1 \leq a_1 b_1 + a_2 b_2.$$

    Keep going like that for $n = 3,4,5,\dots.$ \qed
</p>

<h1>Polynomial Approximation</h1>

<figure>
    <img src="images/MH8onJG.png">
    <figcaption>Mathematical approximation</figcaption>
</figure>

<p>
    <b>Theorem: Taylor's Theorem.</b> <i>If $f', \ldots, f^{(n+1)}$
    are defined on $[a, x],$ then $$f(x) = f(a) + f'(a)(x - a) +
    \cdots + \frac{f^{(n)}(a)}{n!}(x - a)^n + R_n(x)$$ where

    \begin{align*}
    R_n(x) &= \frac{f^{(n+1)}(t)}{n!}(x - t)^n (x - a) &&\text{ for some } t \in (a, x) \\
    R_n(x) &= \frac{f^{(n+1)}(t)}{(n+1)!}(x - a)^{n+1} &&\text{ for some } t \in (a, x).
    \end{align*}

    If $f^{(n+1)}$ is integrable on $[a, x],$ then

    $$R_n(x) = \int_a^x \frac{f^{(n+1)}(t)}{n!} (x - t)^n dt.$$

    The remainder forms are called Cauchy, Lagrange, and Integral,
    respectively.</i>
</p>

<div class="aside">
    <img src="images/Tayloranimation.gif">
</div>

<p>
    The Mean Value Theorem is a special case of Taylor's Theorem:
    $$f(b) = f(a) + f'(c)(b - a)$$ for some $c$ between $a$ and $b.$
</p>

<h1>Sequences</h1>

<div class="bside">
    <img src="images/numbers.jpg">
    LOST
</div>

<p>
    <b>Monotone Convergence Theorem.</b> <i>A sequence which is
    increasing and bounded above by a supremum converges to the
    supremum. Similarly a decreasing lower-bounded sequence
    converges to its infimum.</i>
</p>

<p>
    <b>Theorem: Uniform Limit Theorem.</b> <i>Uniform convergence of functions preserves
    continuity, i.e. if $f_n$ are continuous and approach $f$
    uniformly, then $f$ is continuous.</i>
</p>

<p>
    <b>Proposition.</b> <i>The uniform limit of uniformly continuous
    functions is uniformly continuous.</i>
</p>

<p>
    <b>Question.</b> <i>What about differentiability, i.e. if $f_n$
    are differentiable and approach $f$ uniformly, is $f$ always
    differentiable, and is $\lim f_n' = f'$?</i>
</p>

<div class="aside">
    <img src="images/uniformconvergence1.jpg">
</div>

<p>
    <b>Example.</b> No to the second question: the functions $f_n(x) =
    \frac{1}{n} \sin(nx)$ converge uniformly to the zero function,
    which <i>is</i> differentiable. But, the limit of the
    derivatives don't exist. What about just differentiability?
</p>

<div class="aside">
    <img src="images/uniformconvergence2.png">
</div>

<p>
    <b>Example.</b> Still No, e.g. the functions $$f_n(x) = \sqrt{x^2 +
    1/n}$$ converge uniformly to $f = |x|,$ which is not differentiable
    at zero.
</p>

<div class="aside">
    <img src="images/nodiff1.jpg">
</div>

<p>
    <b>Example.</b> The Weierstrass function $$f(x) =
    \sum^\infty_{k=0} a^k \cos(b^k \pi x),$$ for appropriate values
    $a$ and $b,$ is the uniform limit of $$f_n = \sum^n_{k=0} a^k
    \cos(b^k \pi x),$$ but is nowhere differentiable.
</p>

<div class="aside">
    <img src="images/biomimicry-koch-snowflake.jpg">
</div>
<p>
    <b>Question.</b> <i>Is the Koch snowflake nowhere
    differentiable?</i>
</p>

Yes. Proof?

<h3>Another example similar to the Weierstrass function</h3>

<p>
    <b>Example.</b> Let $\{ x \}$ denote the distance from
    $x$ to the nearest integer. Then the function

    $$f(x) = \sum_{n=1}^\infty \frac{\{ 10^n x \}}{10^n}$$

    is continuous everywhere but differentiable nowhere.
</p>

<p>
    <b>Definition.</b> <i>Let $\{a_n\}$ be a sequence, and $0 \leq a <
    b \leq 1.$ Let $N(n; a, b)$ be the number of integers $j \leq n$
    s.t. $a_j \in [a, b].$ A sequence $\{a_n\}$ of numbers in $[0, 1]$
    is called uniformly distributed in $[0, 1]$ if $$\lim_{n \to
    \infty} \frac{N(n; a, b)}{n} = b - a$$ for all $a, b,$ s.t. $0
    \leq a < b \leq 1.$</i>
</p>

<p>
    <b>Proposition.</b> <i>If $s$ is a step function on $[0, 1],$ and
    $\{a_n\}$ is uniformly distributed in $[0, 1],$ then $$\int_0^1 s
    = \lim_{n\to\infty} \frac{s(a_1) + \cdots + s(a_n)}{n}.$$</i>
</p>

<p>
    <i>Proof.</i>
Let $\Delta_1, \ldots, \Delta_m$ be a partition of $[0, 1]$ corresponding to the steps in $s.$ Then (with a slight abuse of notation) we have
\begin{align*}
\int_0^1 s &= \sum_{i=1}^m s(\Delta_i) \Delta_i \\
&= \sum_{i=1}^m s(\Delta_i) \lim_{n \to \infty} \frac{1}{n} N(n; \Delta_i) \\
&= \lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^m s(\Delta_i) N(n; \Delta_i) \\
&= \lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n s(a_i).
\end{align*}
</p>

<p>
    <b>Proposition.</b> <i>If $f$ is integrable on $[0, 1],$ and
    $\{a_n\}$ is uniformly distributed in $[0, 1],$ then $$\int_0^1 f
    = \lim_{n\to\infty} \frac{f(a_1) + \cdots + f(a_n)}{n}.$$</i>
</p>

<p>
    <i>Proof.</i>  Since $f$ is integrable, there is a step function
$s$ such that $\int_0^1 f$ is close to $\int_0^1 s,$ which is close to
$\frac{s(a_1) + \cdots + s(a_n)}{n},$ which is close to $\frac{f(a_1)
+ \cdots + f(a_n)}{n}.$
</p>

<h1>Bolzano-Weierstrass Theorem</h1>

<div class="aside">
    <img src="images/Bolzanoâ€“Weierstrass_theorem_-_step_7.png">
</div>
<p>
    <b>Bolzano-Weierstrass Theorem.</b> <i>An infinite sequence
    contained in a closed interval $I$ has a limit point in $I.$</i>
</p>

Proof uses the Nested Interval Theorem:

<div class="aside">
    <img src="images/511px-Illustration_nested_intervals.png">
</div>

<p>
    <b>Theorem: Nested Interval Theorem.</b> <i>The intersection of closed nested intervals is nonempty. If the
    interval lengths tend to zero, then the intersection is a
    point. Otherwise it's a closed interval.</i>
</p>

<p>
    <b>Definition.</b> <i>A function $f$ defined on an interval $I$ is
    called limitful if $\lim\limits_{y\to a} f(y)$ exists for all $a
    \in I.$</i>
</p>

<p>
    <b>Proposition.</b> <i>Let $f$ be a limitful function on $[0,
    1].$ Then for any $\e > 0$ there are only finitely many
    points $a \in [0, 1]$ with $$|\lim\limits_{y\to a} f(y) - f(a)| >
    \e.$$</i>
</p>

<p>
    <i>Proof.</i> Suppose that there are infinitely many such points
    $a.$ Then by the Bolzano-Weierstrass Theorem, these points have a
    limit $x \in [0, 1].$ Let $$L := \lim\limits_{y \to x} f(y) =
    \lim\limits_{a\to x} f(a).$$
</p>

<p>
    The condition $$|\lim\limits_{y\to a} f(y) - f(a)| > \e$$
    means that for $y$ close to $a,$ $f(y)$ is far from $f(a).$
    Similarly $\lim\limits_{a \to x} f(a) = L$ means that for $a$
    close to $x,$ $f(a)$ is close to $L.$ Together this means that for
    $y$ close to $x$ and $y$ close to $a$ for some $a,$ we have that
    $f(y)$ is far from $L,$ but this contradicts the fact that $L =
    \lim\limits_{y \to x} f(y),$ i.e. for all $y$ close to $x,$ $f(y)$
    is close to $L.$
</p>

<p>
    <i>Proof 2.</i> Another way to see this is to let $a_n$ be the
convergent subsequence given by Bolzano-Weierstrass, and choose $y_n$
close to $a_n$ so that $|f(y_n) - f(a_n)|$ is big. Since $|f(a_n) -
L|$ is small, the triangle inequality $$|f(y_n) - L| \geq |f(y_n) -
f(a_n)| - |f(a_n) - L|$$ says $|f(y_n) - L|$ is big, thus
contradiction.
</p>

<p>
    <b>Theorem.</b> <i>A limitful function on $[0, 1]$ has at most
    countably many discontinuities.</i>
</p>

<p>
    <i>Proof.</i> By the previous Proposition, for each $\e_q >
    0$ there are at most finitely many points $a$
    s.t. $$|\lim\limits_{y\to a} f(y) - f(a)| > \e_q.$$ Taking a
    sequence $\e_q \in \mathbf Q$ converging to zero, we have countably
    many such points $a.$
</p>

<p>
    <b>Corollary.</b> <i>
If $f$ has only removable discontinuities, then $f$ is continuous except at countably many points. In particular, $f$ cannot be discontinuous everywhere.
</i>
</p>

<h1>Infinite Series</h1>

<img src="images/quote-infinite-growth-of-material-consumption-in-a-finite-world-is-an-impossibility-e-f-schumacher-52-17-01.jpg">

<p><b>Definition.</b> <i>The sequence $a_n$ is summable if the
sequence $$s_n = a_1 + \cdots + a_n$$ converges. In this case
$\lim\limits_{n\to \infty} s_n$ is denoted by
$\sum\limits_{n=1}^\infty a_n,$ and we say $\sum\limits_{n=1}^\infty
a_n$ converges.</i></p>

<h1>Cauchy Criterion</h1>

<div class="aside">
    <img src="images/2000px-Cauchy_sequence_illustration.png">
    A Cauchy sequence
</div>

<p><b>Proposition.</b> <i>The series $\sum\limits_{n=1}^\infty a_n$
converges iff its tail vanishes: $$\lim_{m,n\to \infty} a_n + \cdots +
a_m = 0.$$</i></p>

<h1>Vanishing Condition</h1>

<p><b>Proposition.</b> <i>If $a_n$ is summable, then
$$\lim_{n\to\infty} a_n = 0.$$</i></p>

<p><b>Example.</b> The sequence $1, -1, 1, -1, \ldots$ is not
summable.</p>

<div class="aside">
    <img src="images/Integral_Test.png">
    Harmonic Series nonconvergent because log is unbounded.
</div>

<p><b>Noncounterexample.</b> The vanishing condition is necessary, but
not sufficient for summability, e.g. the harmonic series $$\sum_{n}
\frac{1}{n}$$ is not convergent even though the terms go to zero.</p>

<p><b>Example of convergent series.</b> The geometric series is
convergent for $|r| < 1:$ $$\sum_{n=0}^\infty r^n = \frac{1}{1 - r}.$$
For $|r| \geq 1,$ the series isn't convergent because the terms aren't
vanishing.</p>

<div class="aside">
    <img src="images/zeno-paradox-arrow.png">
    Zeno's Paradox
</div>

<div class="aside">
    <img src="images/proof-without-words-halves.gif">
</div>

<h1>Boundedness Criterion</h1>

<p><b>Proposition.</b> <i>A nonnegative sequence $a_n$ is summable iff
the partial sums $s_n$ is bounded.</i></p>

<h1>Convergence Tests</h1>

<p><b>Theorem.</b> <i>Suppose that $0 \leq a_n \leq b_n$ for all $n.$ If
$\sum\limits_n b_n$ converges, then so does $\sum\limits_n a_n.$</i></p>

<p>Useful for comparing series by orders of magnitude:</p>

<p><b>Example.</b> The sum $$\sum_n \frac{2 + \sin^3(n + 1)}{2^n +
n^2}$$ converges by comparison with $$\sum_n \frac{1}{2^n},$$ which is
a geometric series. Similarly $$\sum_n \frac{n+1}{n^2+1}$$ is not
convergent because its terms are of the same order of magnitude as
those of the harmonic series $$\sum_n \frac{1}{n}.$$</p>

<p><b>Theorem.</b> <i>If $a_n, b_n > 0$ and $\lim\limits_{n\to\infty}
\frac{a_n}{b_n}$ is a nonzero constant, then $\sum\limits_n a_n$ converges
iff $\sum\limits_n b_n$ converges.</i></p>

<p><b>Theorem: Ratio Test.</b> <i>Let $a_n > 0$ for all $n,$ and
suppose that $$\lim_{n\to\infty} \frac{a_{n+1}}{a_n} = r.$$ Then the
series

$$\sum\limits_{n} a_n \text{ is } \begin{cases}
\text{convergent if $r < 1$} \\
\text{divergent if $r > 1$} \\
\text{insurgent if $r = 1.$}
\end{cases}$$
</i></p>

<div class="epigraph">
     <div class="quote">
        <img src="images/divergent-trilogy-covers.png">
     </div><hr><div class="author">Sadly, insurgent is not the technical
     term for inconclusive convergence test. Yet.</div>
</div>

<p>
    <b>Example.</b> The harmonic series has ratio

    $$\lim_{n\to\infty} \frac{a_{n+1}}{a_n} = 1,$$

    but is divergent. On the other hand the series

    $$\sum_n \frac{1}{n^2}$$

    also has ratio 1 but is convergent, by the integral test.
</p>

<p>
    <b>Example.</b> The series $\sum\limits_{n=1}^\infty n r^n$ converges for $r\in [0, 1).$
</p>

<p>
    <b>Theorem: Integral Test.</b> <i>Suppose $f$ is positive and
    decreasing on $[1, \infty],$ and that $f(n) = a_n$ for all
    n. Then $\sum a_n$ converges iff the limit

    $$\int_1^\infty f = \lim_{A\to\infty} \int_1^A f$$

    exists.</i>
</p>

<div class="bside">
    * <b>Question.</b> What is the sum of this series? The integral
    has such a nice form when it exists: $\int_1^\infty \frac{1}{x^p}
    dx = \frac{1}{p-1}.$ Not to be confused with the geometric series
    $\sum_n r^n = \frac{1}{1 - r}.$
</div>

<p>
    <b>Example.</b> Convergence of the series*

    $$\sum_n \frac{1}{n^p}$$

    for $p > 0$ depends on the existence of the integral

    \begin{align*}
    \int_1^\infty \frac{1}{x^p} dx &= \lim_{A\to\infty} \int_1^A \frac{1}{x^p} dx \\
                                   &= \lim_{A\to\infty} \begin{cases}
                                   -\frac{1}{p-1} \frac{1}{A^{p-1}} + \frac{1}{p-1} & \text{ if } p \neq 1 \\
                                   \log A                                           & \text{ if } p = 1
                                   \end{cases}
    \end{align*}

    which exists iff $p > 1.$ In particular the harmonic series
    diverges and $\sum_n \frac{1}{n^2}$ converges.
</p>

<h2>Reverse Integral Test</h2>

<p>
    <b>Example.</b> <i>$$\int_0^\infty \left| \frac{\sin x}{x} \right| dx \quad \text{diverges.}$$</i>
</p>

<p>
    <i>Proof.</i> First note that

    $$\int_0^\pi \left| \frac{\sin x}{x} \right| dx
        \geq \frac{1}{\pi} \int_0^\pi |\sin x| dx
        = \frac{1}{\pi},$$

    and similarly for the remaining intervals $\pi, 2\pi, 3\p,\ldots.$
    Therefore we can compare this integral with the harmonic series:

    \begin{align*}
    \int_0^\infty \left| \frac{\sin x}{x} \right| dx
        &= \int_0^\pi \left| \frac{\sin x}{x} \right| dx
        + \int_\p^{2\p} \left| \frac{\sin x}{x} \right| dx
        + \int_{2\p}^{3\p} \left| \frac{\sin x}{x} \right| dx
        + \cdots \\
        &\geq \frac{1}{\pi} + \frac{1}{2\pi} + \frac{1}{3\pi} + \cdots \\
        &= \frac{1}{\p}\left( 1 + \frac{1}{2} + \frac{1}{3} + \cdots \right),
    \end{align*}

    which diverges. \qed
</p>

<div class="aside">
    <b>Question.</b> <i>Is there a nice function that magically
    pinches intervals?</i>
</div>

<p>
    <b>Example: geometric sawtooth.</b> A continuous function $f \geq 0$
    s.t. $\int_0^\infty f$ exists, but $\lim\limits_{x\to\infty} f(x)$ does
    not:
</p>

<p>
    <img src="images/improperintegralexistsnolimit.png">
</p>

<h2>Root Test</h2>

<p>
    <b>Proposition: root test.</b> <i>Let $a_n > 0$ for all $n,$ and
    suppose that $$\lim_{n\to\infty} \sqrt[n]{a_n} = r.$$ Then the
    series

    $$\sum\limits_{n} a_n \text{ is } \begin{cases}
    \text{convergent if } r < 1 \\
    \text{divergent if } r > 1.
    \end{cases}$$

    The test is inconclusive if $r = 1.$</i>
</p>

<p>
    <i>Proof.</i> If $r > 1,$ then $a_n > 1$ for sufficiently large
    $n,$ and so the terms aren't approaching zero and the series
    diverges. Let $r < 1.$ Then

    $$\lim_{n\to\infty} \sqrt[n]{a_n} = r < 1$$

    means that for any $s \in (r, 1)$ there exists an $N$ s.t. for all $n > N,$

    $$\sqrt[n]{a_n} \leq s, \quad \text{ or } \quad a_n \leq s^n.$$

    Therefore the remainder of the series after $a_N$ is

    $$\sum_{n> N} a_n \leq \sum_{n>N} s^n,$$

    which converges because it's a geometric series.
</p>

<p>
    <b>Remark.</b> The ratio test above can be proved similarly.
</p>

<p>
    <b>Note.</b> If the ratio test works, then the root test also
    works, by the following:
</p>

<p>
    <b>Lemma: limit of ratios is limit of roots.</b> <i>If $a_n >
    0$ and

    $$\lim_{n\to\infty} \frac{a_{n+1}}{a_n} = L,$$

    then

    $$\lim_{n\to\infty} \sqrt[n]{a_n} = L.$$</i>
</p>

<p>
    <i>Proof.</i> The limit

    $$\lim_{n\to\infty} \frac{a_{n+1}}{a_n} = L$$

    means that for any $\e,$ there is an $N$ s.t. for all $n \geq N,$ we
    have

    $$L - \e \leq \frac{a_{n+1}}{a_n} \leq L + \e.$$

    Therefore,

    $$(L - \e)^n \leq \frac{a_{N+1}}{a_N} \cdots \frac{a_{N+n}}{a_{N+n-1}}
    = \frac{a_{N+n}}{a_N} \leq (L + \e)^n \\

    L - \e \leq \sqrt[n]{ \frac{a_{N+n}}{a_N} } \leq L + \e.$$

    In other words,

    $$\lim_{n\to\infty} \sqrt[n]{ \frac{a_{N+n}}{a_N} } = L.$$

    Since $a_N > 0$ is fixed,

    $$\lim_{n\to\infty} \sqrt[n]{a_N} = 1,$$

    and so

    $$\lim_{n\to\infty} \sqrt[n]{ \frac{a_{N+n}}{a_N} }
    = \lim_{n\to\infty} \sqrt[n]{a_{N+n}}
    = \lim_{m\to\infty} \sqrt[m]{a_{m}} = L.$$
</p>

<h3>Partial sums tail bound</h3>

<p>
    <b>Lemma.</b> <i>If the partial sums are bounded, then so are the
    tails. Concretely, if

    $$|a_1 + \cdots + a_n| \leq M$$

    for all $n,$ then

    $$|a_k + \cdots + a_n| \leq 2M$$

    for all $k$ and $n.$</i>
</p>

<p>
    <i>Proof.</i> Suppose for contradiction that

    $$|a_k + \cdots + a_n| > 2M.$$

    Then, by the reverse triangle inequality,

    \begin{align*}
    M &\geq |a_1 + \cdots + a_n| \\
      &\geq |a_k + \cdots + a_n| - |a_1 + \cdots + a_{k-1}| \\
      &> 2M - M = M,
    \end{align*}

    contradiction. \qed
</p>

<p>
    Combining this bound on the tail with <a href="#agdggi426">Abel's
    Lemma,</a> we get:
</p>

<h3>Abel's Lemma Reloaded</h3>

<p>
    <b>Corollary.</b> <i>Suppose $b_n \geq 0$ is non-increasing, and
    that

    $$|a_1 + \cdots + a_n| \leq M$$

    for all $n.$ Then

    $$|a_k b_k + \cdots + a_n b_n| \leq 2 b_k M,$$

    i.e. if the partial sums of a series are bounded, then the
    squeezed tails are also bounded.</i>
</p>

<h3>Dirichlet Test</h3>

<p>
    <b>Proposition.</b> <i>Suppose that the partial
    sums of $a_n$ are bounded and that $b_n$ is a sequence with $b_n
    \geq b_{n+1}$ and $\lim\limits_{n\to\infty} b_n = 0.$ Then $\sum\limits_{n=1}^\infty a_n
    b_n$ converges.</i>
</p>

<p>
    <i>Proof.</i> By the previous corollary, the tail is bounded:

    $$|a_k b_k + \cdots + a_n b_n| \leq 2 b_k M.$$

    Since $\lim\limits_{n\to\infty} b_n = 0,$ it also vanishes as
    $k\longrightarrow \infty,$ therefore by the cauchy criterion the
    series converges. \qed
</p>

<h3>Corollary. Leibniz's Alternating Series Theorem</h3>

<p>
    We can use Dirichlet's test to give another proof
    of <a href="#bsdjskb31">Leibniz's theorem below.</a>
</p>

<p>
    <i>Proof.</i> Take

    $$a_n = (-1)^{n+1}, \quad b_n \geq b_{n+1} \quad
        \text{ and } \quad \lim\limits_{n\to\infty} b_n = 0.$$

     Then the alternating series

    $$\sum_{n=1}^\infty (-1)^{n+1} b_n$$

    converges. \qed
</p>

<p>
    <b>Exercise.</b> <i>The series

    $$\sum_{n=1}^\infty \frac{\cos nx}{n}$$

    converges iff $x$ is not of the form $2k\pi.$</i>
</p>

<p>
    <i>Proof.</i> By <a href="#hsdgb3662">Lagrange's trigonometric
    identity,</a> the partial sums

    $$\sum_{n=1}^N \cos nx = \frac{\sin(N + \frac{1}{2})x}{2\sin \frac{x}{2}} - \frac{1}{2}$$

    are bounded, and $\frac{1}{n} \longrightarrow 0,$ so by
    Dirichlet's test the series converges if $x \neq 2k\pi.$ If $x =
    2k\pi,$ then $\cos nx = 1$ and this becomes the harmonic series,
    which we know diverges. \qed
</p>

<h3>Abel's test</h3>

<p>
    <b>Proposition.</b> <i>If $\sum a_n$ converges and $b_n$ is a
    bounded sequence which is either nonincreasing or nondecreasing,
    then $\sum_{n=1}^\infty a_n b_n$ converges.</i>
</p>

<p>
    <i>Proof.</i> We'll prove the case where $b_n$ is nonincreasing,
    the other one is similar. By the monotone convergence theorem,
    $b_n$ converges, so let $b = \lim\limits_{n\to\infty} b_n.$ Let $c_n =
    b_n - b.$ Then by Dirichlet's test, $\sum a_n c_n$ converges, and

    $$\sum_{n=1}^\infty a_n c_n = \sum_{n=1}^\infty a_n (b_n - b)
    = \sum_{n=1}^\infty a_n b_n - b \sum_{n=1}^\infty a_n,$$

    so $\sum a_n b_n$ converges too. \qed
</p>

<h2>Absolute and Conditional Convergence</h2>

<p>
    <b>Definition.</b> <i>A series $\sum a_n$ is absolutely
    convergent if $\sum |a_n|$ converges.</i>
</p>

<p id="aksfjn12124">
    <b>Absolute Convergence Theorem.</b> <i>Every absolutely convergent
    series is convergent. Moreover, a series is absolutely convergent
    iff the series formed from its positive and negative terms both
    converge.</i>
</p>

<p id="nxm3m4">
    <b>Note.</b> Let $\sum p_n$ and $\sum q_n$ be the series made of
    positive and negative terms of $\sum a_n.$ If $\sum a_n$ is
    conditionally convergent, i.e. it converges but not absolutely,
    then by the Absolute Convergence Theorem, one of $\sum p_n$ and
    $\sum q_n$ must diverge. In fact, they must both diverge, because
    otherwise one converges and one diverges, and so one has bounded
    and the other has unbounded partial sums, so $\sum a_n$ has
    unbounded partial sums and $\sum a_n$ diverges, contradicting that
    it converges.
</p>

<div id="dsf87hh34" class="bside">
    <img src="images/alternatingseries.png">
</div>

<p id="bsdjskb31">
    <b>Leibniz's Alternating Series Theorem.</b> <i>Suppose that

    $$a_1 \geq a_2 \geq \cdots \geq 0$$

    and that

    $$\lim_{n\to\infty} a_n = 0.$$

    Then the series

    $$\sum_n (-1)^{n+1} a_n = a_1 - a_2 + a_3 - \cdots$$

    converges.</i>
</p>

<p>
    <i>Proof.</i> <a href="#ijfasdf09234">See note following the proof of this proposition:</a>
</p>

<p>
    <b>Proposition: alternating series remainder estimate.</b> <i>Let

    $$\sum_n (-1)^{n+1} a_n$$

    be an alternating series as in Leibniz's Theorem above. Then

    $$\left|\ \sum_n (-1)^{n+1} a_n - (a_1 - a_2 + \cdots \pm a_N)\ \right| \leq a_N$$

    for any $N,$ i.e. the remainder of the series is no larger than
    the term $a_N$ preceding it.</i>
</p>

<p>
    <i>Proof sketch.</i> Note that it's enough to show this for $a_1,$
    then it'll be true for any $N$ because we can just cut the series
    there and the remainder will be its own alternating series. So we
    need to show

    $$|a_2 - a_3 + \cdots| \leq a_1.$$
</p>

<div class="bside">
    <img src="images/leibnizalternatingseries.png">
</div>

<p>
    Clearly, the LHS is greatest when its even terms $a_2, a_4, a_6,
    \ldots$ are at least as big as the odd terms preceding them. But
    the odd terms have to go to zero eventually, so we have the
    following picture, where the red bars represent $a_i,$ and the
    green bars represent

    $$|a_2 - \cdots \pm a_i|.$$

    Here we see that even when the even terms are as big as possible,
    the entire series can never get past its original value $a_1.$ \qed
</p>

<p id="ijfasdf09234">
    <i>Proof of Leibniz.</i> Using this estimate we can give a quick
    proof of Leibniz's theorem above: since the remainder is bounded
    by $a_N \longrightarrow 0,$ it too approaches zero, and so the
    series converges by the Cauchy Criterion. \qed
</p>

<h2>Rearrangement Theorems</h2>

<p>
    <b>Rearrangement Theorem for Conditionally Convergent
    Series.</b> <i>If $\sum a_n$ converges but not absolutely, then
    for any number $\alpha$ there is a rearrangement $b_n$ of
    $a_n$ s.t. $\sum b_n = \alpha.$</i>
</p>

<p>
    <i>Proof.</i> <a href="#nxm3m4">By a previous note,</a> $\sum a_n$
    contains infinitely many positive and negative terms, and
    furthermore the positive and negative sums are divergent, so they
    can reach any positive or negative number $\alpha.$ Once you're
    close to $\alpha$ you can rearrange the terms to make it go a
    little over, and then a little under, and so on, converging to
    $\alpha,$ a bit like <a href="#dsf87hh34">this picture.</a>
</p>

<p>
    Note that you need both the positive and negative series to be
    divergent, because otherwise say $\sum p_n = N$ is convergent,
    then you could pick $\alpha > N$ and any rearranged partial sums
    of $\sum a_n$ could never reach $\alpha.$
</p>

<p>
    <b>Rearrangement Theorem for Absolutely Convergent
    Series.</b> <i>If $\sum a_n$ converges absolutely and $b_n$ is any
    rearrangement of $a_n,$ then $\sum b_n$ also converges
    absolutely and $$\sum a_n = \sum b_n.$$</i>
</p>

<p>
    <b>Note.</b> If $\sum a_n$ converges absolutely, that doesn't mean
    $$\sum |a_n| = \sum a_n.$$ E.g. the series

    $$\frac{1}{2} - \frac{1}{4} + \frac{1}{8} - \cdots = \frac{1}{3}$$

    is absolutely convergent, with absolute sum

    $$\frac{1}{2} + \frac{1}{4} + \frac{1}{8} + \cdots = 1.$$

    The way you calculate the first sum is as follows: split the
    series into its positive and negative terms:

    \begin{align*}
    S \equiv \sum a_n^+ &= \frac{1}{2} + \frac{1}{2^3} + \frac{1}{2^5} + \cdots \\
    T \equiv \sum a_n^- &= -\frac{1}{2^2} - \frac{1}{2^4} - \frac{1}{2^6} - \cdots \\
                        &= -\frac{1}{2}\left(\frac{1}{2} + \frac{1}{2^3} + \frac{1}{2^5} + \cdots\right) \\
                        &= -\frac{S}{2}.
    \end{align*}

    Now compute the absolute series in terms of S:

    \begin{align*}
    1 = \sum |a_n| &= \sum a_n^+ - \sum a_n^- \\
    1 &= S - T = S + \frac{S}{2} \\
    S &= \frac{2}{3}.
    \end{align*}

    Next compute T:

    \begin{align*}
    1 &= S - T = \frac{2}{3} - T \\
    T &= -\frac{1}{3},
    \end{align*}

    and finally:

    \begin{align*}
    \sum a_n = S + T = \frac{2}{3} - \frac{1}{3} = \frac{1}{3}.
    \end{align*}
</p>

<p>
    <b>Note.</b> We used in the last example a fact that should
    probably be proved:
</p>

<div class="bside">
    * Just realized this result follows immediately from
    the <a href="#aksfjn12124">Absolute Convergence Theorem.</a> In
    fact $\sum a_n$ is absolutely convergent.
</div>

<p>
    <b>Proposition.*</b> <i>Let $\sum a_n$ be a series s.t. $\sum
    a_n^+$ and $\sum a_n^-$ are both convergent. Then $\sum a_n$ is
    convergent and

    $$\sum a_n = \sum a_n^+ + \sum a_n^-.$$</i>
</p>

<p>
    <i>Proof.</i> Let $s_n$ denote a partial sum and $s$ its limit. Since

    $$s_n = s_n^+ + s_n^-$$

    we have

    \begin{align*}
    |s_n - (s^+ + s^-)| &= |s_n^+ + s_n^- - (s^+ + s^-)| \\
    &= |s_n^+ - s^+ + s_n^- - s^-| \\
    &\leq |s_n^+ - s^+| + |s_n^- - s^-| \\
    &< 2 \e
    \end{align*}

    with appropriately defined $\e.$ Therefore $s_n
    \longrightarrow s^+ + s^-.$
</p>

<p>
    <b>Theorem: product of absolutely convergent series.</b> <i>If
    $\sum a_n$ and $\sum b_n$ are absolutely convergent, and $c_n$ is
    any sequence containing the products $a_i b_j$ for all $i, j,$ then

    $$\sum c_n = \sum a_n \cdot \sum b_n.$$</i>
</p>

<p>
    <b>Proposition.</b> <i>If $\sum\limits a_n$ converges absolutely, then
    for any subsequence $b_n$ of $a_n,$ $\sum\limits b_n$ also converges
    absolutely.</i>
</p>

<p>
    <i>Proof.</i> We want to show that for any $\e > 0,$ there is an
    $N$ s.t. for all $M > N,$

    $$|b_N| + \cdots + |b_M| < \e,$$

    so that $\sum\limits |b_n|$ converges by the Cauchy Criterion.

    But $\sum\limits a_n$ converges absolutely, so for any such $b_n$ tail we
    have an $a_n$ tail containing it, s.t.

    $$|b_N| + \cdots + |b_M| \leq |a_N| + \cdots + |a_M| < \e,$$

    and so $\sum\limits b_n$ converges absolutely.
</p>

<div class="bside">
    <img src="images/htvol3-pic4.jpg">
</div>

<p>
    <b>Example.</b> This is false if $\sum a_n$ does not converge
    absolutely, because then we know that its positive and negative
    sums diverge, and so we can choose $\sum b_n$ to be either
    subseries. E.g. the alternating harmonic series

    $$\sum \frac{(-1)^{n+1}}{n}$$

    converges conditionally, but the subseries

    $$\sum_{\text{odd}} \frac{1}{n} = 1 + \frac{1}{3} + \frac{1}{5} + \cdots$$

    and

    $$\sum_{\text{even}} \frac{1}{n} = -\frac{1}{2} - \frac{1}{4} - \cdots$$

    both diverge.
</p>

<p>
    <b>Corollary.</b> <i>If $\sum a_n$ converges absolutely, then

    $$\sum a_n = \sum_{\text{odd}} a_n + \sum_{\text{even}} a_n
               = (a_1 + a_3 + \cdots) + (a_2 + a_4 + \cdots).$$</i>
</p>

<p>
    <i>Proof.</i> By the previous proposition, we know that the even
    and odd sums both converge absolutely. Let $A, O, E$ be the sums,
    and let $A_n, O_n, E_n$ be the $n$-th partial sums for the whole,
    even, and odd series:

    \begin{align*}
    A_n &= a_1 + a_2 + \cdots + a_n \\
    O_n &= a_1 + a_3 + \cdots + a_{(n)} \\
    E_n &= a_2 + a_4 + \cdots + a_{[n]}, \\
    \end{align*}

    where $(n)$ and $[n]$ denote the largest integer less than or
    equal to $n$ which is odd and even, so that in particular

    $$A_n = O_n + E_n.$$

    Then

    \begin{align*}
    |A_n - (O + E)| &= |A_n - (O + E) + (O_n - O_n) + (E_n - E_n)| \\
                    &\leq |A_n - O_n - E_n| + |O - O_n| + |E - E_n| \\
                    &< 0 + \e + \e,
    \end{align*}

    for appropriately chosen $\e.$ Therefore $A_n \longrightarrow O + E.$
</p>

<p>
    In fact we can do a little bit better:
</p>

<p>
    <b>Corollary.</b> <i>If
    $\sum a_n$ converges absolutely, then for any partition $b_n, c_n$
    of $a_n,$

    $$\sum a_n = \sum b_n + \sum c_n.$$</i>
</p>

<p>
    Combining this with the Absolute Convergence Theorem we get:
</p>

<p>
    <b>Corollary: Absolute Convergence Theorem II.</b> <i> $\sum a_n$
    converges absolutely iff $\sum b_n$ and $\sum c_n$ converge for
    any partition $b_n, c_n$ of $a_n.$ In that case,

    $$\sum a_n = \sum b_n + \sum c_n.$$</i>
</p>

<p>
    It's stronger in the forward but weaker in the backward direction.
</p>

<div class="aside">
    <img src="images/illuminati-symbols-all-seeing-eye.gif">
</div>
<p>
    <b>Proposition: infinite triangle inequality.</b> <i>If $\sum a_n$
    is absolutely convergent, then

    $$\left|\ \sum_{n=1}^\infty a_n\ \right| \leq \sum_{i=1}^\infty |a_n|.$$</i>
</p>

<p>
    <i>Proof.</i> By the triangle inequality, for each $n,$

    \begin{align*}
    s_n &\equiv |a_1 + \cdots + a_n| \\
        &\leq |a_1| + \cdots + |a_n| \\
        &\leq \sum_{i=1}^\infty |a_n| \equiv S.
    \end{align*}

    Since the sequence $s_n$ is bounded above by $S,$ its limit must
    also be bounded by $S.$ Illuminati Confirmed \qed
</p>

<h2>Binomial Series</h2>

<p>
    <b>Definition.</b> <i>For any $\a$ and non-negative integer $n,$ define
    the binomial coefficient

    $${\a \choose n} = \begin{cases}
    \frac{\a(\a - 1) \cdots (\a - n + 1)}{n!} & \text{ if } n\neq 0\\
    1 & \text{ if } n = 0.
    \end{cases}$$
    </i>
</p>

<p>
    If $\a$ is an integer, then this is the old binomial coefficient
    for $n \leq \a$ and zero for $n > \a.$ Otherwise it is never zero
    and alternates in sign for $n > \a.$
</p>

<h3 id="bfvtr3491">Multiplicative and additive recursion</h3>

<p>
    <b>Corollary.</b> <i>For any real $\a$ and integer $k > 0,$

    \begin{align*}
    k \binom{\a}{k} &= \a {\a - 1 \choose k - 1} \\
    {\a \choose k} &= {\a - 1 \choose k - 1} + {\a - 1 \choose k}.
    \end{align*}</i>
</p>

<h3>Binomial Taylor polynomial</h3>

<p>
    <b>Proposition.</b> <i>The Taylor polynomial of degree $n$ around
    $0$ for

    $$f(x) = (1 + x)^\a$$

    is

    $$P_n(x) = \sum_{k=0}^n {\a \choose k} x^k,$$

    with Cauchy and Lagrange remainders

    \begin{align*}
    R_n(x) &= \a {\a-1 \choose n} \left( \frac{x-t}{1+t} \right)^n x (1+t)^{\a-1} && \text{ for some } t \in (0,x) \text{ or } (x,0) \\
    R_n(x) &= {\a \choose n+1} x^{n+1} (1+t)^{\a-n-1} && \text{ for some } t \in (0,x) \text{ or } (x,0). \\
    \end{align*} </i>
</p>

<h3>Newton's Generalised Binomial Theorem</h3>

<div class="aside">
    Very cool GIF of Isaac Newton. Gotta learn how to make stuff like
    this!
    <img src="images/1d92f6_139c131507b186fa32e4fcc31294a3a0.gif">
</div>
<p>
    <b>Theorem.</b> <i>For any $\a$ and $|x| < 1,$

    $$(1 + x)^\a = \sum_{k=1}^\infty {\a \choose k} x^k.$$</i>
</p>

<p>
    <i>Proof.</i> By the previous proposition,

    $$(1 + x)^\a = \sum_{k=0}^n {\a \choose k} x^k + R_n(x),$$

    where $R_n$ is the remainder in Cauchy or Lagrange form. We'll
    prove the theorem by showing that $R_n(x) \longrightarrow 0$ for
    $|x| < 1.$
</p>

<div class="aside">
    That's a neat trick: sometimes you can show that a sequence $a_n$
    goes to zero by showing that it's the terms of a convergent series
    and using a convergence test like the ratio test.
</div>

<p>
    By the ratio test, the series

    $$\sum_{k=0}^\infty {\a \choose k} x^k$$

    converges for $|x| < 1.$ In particular, for $|x| < 1,$

    \begin{align}
    \lim_{n\to\infty} {\a \choose n} x^n = 0.
    \label{iosjdofd34}
    \end{align}
</p>

<p>
    Suppose first that $0 \leq x < 1.$ Then the $t$ in Lagrange's form
    of the remainder satisfies $0 \leq t < x < 1,$ so

    $$0 < (1 + t)^{\a-n-1} \leq 1$$

    for $n+1 > \a.$ Therefore, by $\ref{iosjdofd34}$,

    $$R_n(x) = {\a \choose n+1} x^{n+1} (1+t)^{\a-n-1} \longrightarrow 0$$

    as $n\longrightarrow \infty.$
</p>

<p>
    Now suppose that $-1 < x < 0.$ The $t$ in Cauchy's form of the
    remainder satisfies $-1 < x < t \leq 0.$ It's easy to check that

    $$\left| \frac{x-t}{1+t} \right| = |x|\frac{1-t/x}{1+t} \leq |x| < 1$$

    and

    $$|x(1 + t)^{\a-1}| \leq |x|M = \text{constant},$$

    where $M = \max(1, (1+x)^{\a-1}).$ By $\ref{iosjdofd34}$ again,

    $$\lim_{n\to\infty} \a {\a-1 \choose n} \left( \frac{x-t}{1+t} \right)^n = 0,$$

    and therefore

    $$R_n(x) = \a {\a-1 \choose n} \left( \frac{x-t}{1+t} \right)^n x (1+t)^{\a-1} \longrightarrow 0.\qed$$
</p>

<p>
    <b>Question.</b> <i>Is there an interpretation for the generalized
    binomial theorem?</i>
</p>

<h3>Binomial Approximation</h3>

<p>
    <b>Corollary.</b> <i>If $x$ is a number close to $0$ and $\a$ is any real number, then

    $$(1 + x)^\a \approx 1 + \a x.$$</i>
</p>

<h3>Cauchy Condensation Theorem</h3>

<p>
    <b>Theorem.</b> <i>Suppose $a_n$ is decreasing and $a_n
    \longrightarrow 0.$ If $\sum\limits_{n=1}^\infty a_n$ converges,
    then $\sum\limits_{n=1}^\infty 2^n a_{2^n}$ also converges.</i>
</p>

<p>
    <i>Proof.</i> We'll show that

    $$\sum\limits_{n=1}^\infty 2^n a_{2^n} \leq 2 \sum\limits_{n=1}^\infty a_n.$$

    The first series is:

    \begin{align*}
    \sum\limits_{n=1}^\infty 2^n a_{2^n} &= (a_2 + \color{red}{a_2}) + (a_4 + a_4 + a_4 + \color{red}{a_4}) \\
        &+ (a_8 + a_8 + a_8 + a_8 + a_8 + a_8 + a_8 + \color{red}{a_8}) + \cdots
    \end{align*}

    and the second:

    \begin{align*}
    2 \sum\limits_{n=1}^\infty a_n &= (a_1 + \color{red}{a_1}) + (a_2 + a_3 + a_2 + \color{red}{a_3}) \\
        &+ (a_4 + a_5 + a_6 + a_7 + a_4 + a_5 + a_6 + \color{red}{a_7}) + \cdots
    \end{align*}

    Comparing the respective parenthetical groups in both series, we
    see that the second series's groups are at least as big as the
    first series's, since the smallest terms in the second groups are
    at least as big as the terms in the first groups, e.g.

    $$a_1 \geq a_2, \quad a_3 \geq a_4, \quad a_7 \geq a_8, \ldots. \qed$$
</p>

<h3>Bounded partial sums and vanishing terms do not imply convergence</h3>

<p>
    <b>Example.</b> If $\sum\limits_{n=1}^\infty a_n$ converges, then
    the partial sums are bounded, and $\lim\limits_{n\to\infty} a_n =
    0.$ The converse is not true, however, e.g. the partial sums of
    the series

    \begin{align*}
    1 - \left(\frac{1}{2} + \frac{1}{2}\right) + \left(\frac{1}{3} + \frac{1}{3} + \frac{1}{3}\right)
        - \left(\frac{1}{4} + \frac{1}{4} + \frac{1}{4} + \frac{1}{4}\right) + \cdots
    \end{align*}

    oscillate between 0 and 1, and the terms go to zero, but the
    series isn't convergent because the sums oscillate.
</p>

<h2>Infinite Products</h2>

<p>
    <b>Definition.</b> <i>Let $b_n \neq 0.$ We say that the infinite
    product $\prod_{n=1}^\infty b_n$ converges if the sequence
    of partial products $p_n = \prod_{n=1}^N b_n$ converges
    and $\lim_{n\to\infty} p_n \neq 0.$</i>
</p>

<h3>Infinite products are just infinite series</h3>

<p>
    <b>Corollary.</b> <i>$$\prod_{n=1}^\infty b_n
    \text{ converges iff } \sum_{n=1}^\infty \log b_n \text{ converges.}$$</i>
</p>

<p>
    <i>Proof.</i> Log turns products into sums. \qed
</p>

<p>
    Remark. The condition $\lim_{n\to\infty} p_n \neq 0$
    corresponds to $\sum_{n=1}^\infty \log b_n \neq -\infty.$
</p>

<p>
    <b>Example.</b> If $\prod_{n=1}^\infty (1 + a_n)$
    converges, then $\sum_{n=1}^\infty \log(1 + a_n)$
    converges, so $\log(1 + a_n) \longrightarrow 0,$ and therefore
    $a_n \longrightarrow 0.$
</p>

<h3>Upper and lower bounds for log</h3>

<p class="box">
    <b>Lemma.</b> <i>For $x > -1,$

    $$\frac{x}{1+x} \leq \log(1+x) \leq x.$$</i>
</p>

<div class="bside">
    <img src="images/logupperlowerbounds.png">
</div>
<p>
    <i>Proof.</i> We'll prove the upper bound first: since log is
    concave, its derivative decreases for increasing $x,$ so by the
    mean value theorem, for $x > 1,$

    \begin{align*}
    \frac{\log x - \log 1}{x - 1} &\leq D \log 1 = 1 \\
        \log x &\leq x - 1,
    \end{align*}

    Similarly for $x \in (0, 1),$ and equality clearly holds for $x =
    1.$ Therefore inequality holds for all $x > 0,$ or alternatively,
    for $x > -1,$

    $$\log(1+x) \leq x.$$

    For the lower bound, apply the upper bound to $1/x:$

    \begin{align*}
    \log \frac{1}{x} &\leq \frac{1}{x} - 1 \\
        1 - \frac{1}{x} &\leq \log x
    \end{align*}

    for $x > 0.$ Substituting $x + 1$ into $x$ yields the lower bound
    for $x > -1:$

    $$\frac{x}{1+x} \leq \log(1+x). \qed$$
</p>

<div class="bside">
    <img src="images/2log1x.png">
</div>
<p>
    <b>Lemma.</b> <i>For $x \in [0, 1),$

    $$x \leq 2 \log(1 + x).$$</i>
</p>

<p>
    <i>Proof.</i> Both sides are equal at $x = 0$ and the RHS has
    bigger derivative on $[0, 1).$ \qed
</p>

<p>
    <b>Proposition.</b> <i>If $a_n \geq 0,$ then

    $$\prod_{n=1}^\infty (1 + a_n) \text{ converges iff } \sum_{n=1}^\infty a_n \text{ converges.}$$</i>
</p>

<p>
    <i>Proof.</i> By a previous result, $\prod_{n=1}^\infty (1 + a_n)$
    converges iff $\sum_{n=1}^\infty \log(1 + a_n)$ converges. By the
    previous two lemmas, for $a_n \geq 0,$

    $$\log(1 + a_n) \leq a_n,$$

    and for $a_n \in [0, 1),$

    $$a_n \leq 2 \log(1 + a_n).$$

    If $\sum_{n=1}^\infty a_n$ converges, then the first inequality
    forces $\sum_{n=1}^\infty \log(1 + a_n)$ to converge. On the other
    hand, if $\sum_{n=1}^\infty \log(1 + a_n)$ converges, then $\log(1
    + a_n) \longrightarrow 0,$ so $a_n \longrightarrow 0,$ and so for
    sufficiently large $n,$ $a_n$ must be in $[0, 1),$ and the second
    inequality forces $\sum_{n=1}^\infty a_n$ to converge. \qed
</p>

<p>
    In a previous lemma we have the inequality

    $$\frac{x}{1+x} \leq x.$$

    By a constant factor we can also reverse the inequality for $x$ close to 0:
</p>

<p>
    <b>Lemma.</b> <i>By simple rearrangement, for $x \in [0, 1],$

    $$x \leq \frac{2x}{1 + x}.$$</i>
</p>

<p>
    Using both inequalities we have:
</p>

<p>
    <b>Corollary.</b> <i>If $a_n \geq 0,$ then

    $$\sum_{n=1}^\infty a_n \text{ converges iff } \sum_{n=1}^\infty \frac{a_n}{1+a_n} \text{ converges.}$$</i>
</p>

<p>
    This result can also be used to show the previous proposition,
    although we won't do it.
</p>

<h3>Some infinite products</h3>

<p>
    <b>Example.</b> <i>

    $$\prod_{n=2}^\infty \left( 1 - \frac{1}{n^2} \right) = \frac{1}{2}.$$
    </i>
</p>

<p>
    <i>Proof.</i> Writing it all out:

    \begin{align*}
    \prod_{n=2}^\infty \left( 1 - \frac{1}{n^2} \right) &= \prod_{n=2}^\infty \frac{n^2 - 1}{n^2} = \prod_{n=2}^\infty \frac{(n-1)(n+1)}{n^2} \\
        &= \frac{1 \cdot 3}{2 \cdot 2} \cdot \frac{2 \cdot 4}{3 \cdot 3} \cdot \frac{3 \cdot 5}{4 \cdot 4} \cdot \frac{4 \cdot 6}{5 \cdot 5} \cdot \frac{5 \cdot 7}{6 \cdot 6} \cdot \frac{6 \cdot 8}{7 \cdot 7} \cdots \\
        &= \frac{1}{2}. \qed
    \end{align*}
</p>

<p>
    <b>Example.</b> <i>For $|x| < 1,$

    $$\prod_{n=1}^\infty (1 + x^{2^n}) = \frac{1}{1-x^2}.$$
    </i>
</p>

<p>
    <i>Proof.</i> Multiply out all the factors:

    \begin{align*}
    \prod_{n=1}^\infty (1 + x^{2^n}) &= (1 + x^2)(1 + x^4)(1 + x^8)(1 + x^{16})\cdots \\
        &= 1 + x^2 + x^4 + x^6 + x^8 + x^{10} + x^{12} + x^{14} + \cdots,
    \end{align*}

    which is the geometric series with $r = x^2:$

    $$\prod_{n=1}^\infty (1 + x^{2^n}) = \frac{1}{1 - x^2}. \qed$$

    Nice relationship between powers of 2 and even numbers: every even
    number is a sum of distinct powers of 2. Actually every number is
    a sum of distinct powers of 2: 1 is also a power of 2.
</p>

<p>
    <b>Question.</b> <i>What is the relationship between say multiples
    of 3 and powers of 3?</i>
</p>

<p>
    <b>Theorem. </b> <i>Any positive rational number $x$ can be
    written as a finite sum of distinct numbers of the form $1/n.$</i>
</p>

<p>
    <i>Proof by example.</i> Say we want to write $x = \frac{27}{31}$
    as such a sum. Since

    \begin{align*}
    \frac{27}{31} - \frac{1}{2} &= \frac{23}{62} \\
    \frac{23}{62} - \frac{1}{3} &= \frac{7}{186} \\
    \frac{7}{186} &< \frac{1}{4}, \ldots, \frac{1}{26} \\
    \frac{7}{186} - \frac{1}{27} &= \frac{1}{1674}
    \end{align*}

    we have

    $$\frac{27}{31} = \frac{1}{2} + \frac{1}{3} + \frac{1}{27} + \frac{1}{1674}. \qed$$
</p>

<h1>Uniform Convergence and Power Series</h1>

<p>
    Counterexamples motivating notion of uniform convergence.
</p>

<div class="aside">
    <img src="images/p128.jpg">
</div>
<p>
    <b>Definition.</b> <i>Let $f_n$ be a sequence of functions defined
    on $A.$ Then a function $f$ on $A$ is called the uniform limit of
    $f_n$ if for every $\e > 0$ there is some $N$ such that for all $x$ in $A,$

    $$n > N \quad\text{implies}\quad |f(x) - f_n(x)| < \e.$$</i>
</p>

<p>
    In other words, for every $\e$-tube around $f,$ for all
    sufficiently large $n,$ $f_n$ lies inside this tube.
</p>

<h3>Uniform convergence respects integrals</h3>

<p>
    <b>Theorem.</b> <i>Let $f_n$ be a sequence of functions integrable
    on $[a, b]$ that converges uniformly to $f,$ which is also
    integrable on $[a, b].$ Then

    $$\int_a^b f = \lim_{n\to\infty} \int_a^b f_n.$$</i>
</p>

<h3>Uniform convergence preserves continuity</h3>

<p class="box">
    <b>Theorem.</b> <i>Suppose $f_n$ is a sequence of continuous
    functions on $[a, b]$ which converges uniformly to $f.$ Then $f$
    is also continuous on $[a, b].$</i>
</p>

<h3>Uniform convergence of derivatives</h3>

<p>
    <b>Theorem.</b> <i>Suppose that $f_n$ is a sequence of functions
    which are differentiable on $[a, b]$ with integrable derivatives
    $f_n',$ and that $f_n$ converges point wise to $f.$ Suppose also
    that $f_n'$ converges uniformly on $[a, b]$ to some continuous
    function $g.$ Then $f$ is differentiable and

    $$f'(x) = g(x) = \lim_{n\to\infty} f_n'(x).$$</i>
</p>

<p>
    <b>Question.</b> Does this mean $f_n$ converges uniformly to $f?$
    Whoa it does! A lemma first:
</p>

<p>
    <b>Lemma.</b> <i>Suppose that $g_n$ are differentiable functions
    on $[a,b]$ whose derivatives $g_n'$ converge uniformly to
    zero. Then $g_n$ converge uniformly to a constant function.</i>
</p>

<p>
    <i>Proof.</i> Derivatives converging uniformly to zero means that
    for any $\e > 0,$

    $$|g_n'(x)| = \left| \lim_{h\to 0} \frac{g_n(x+h) - g_n(x)}{h} \right| < \e$$

    for all $x\in[a,b]$ for sufficiently large $n.$ Then for all such
    $n,$ for sufficiently small $h$ we have

    $$\left| g_n(x+h) - g_n(x) \right| < \e h.$$

    Now let $y$ be any number in $[a,b].$ For simplicity assume $y >
    x$ and let $y = x+Mh$ for some positive integer $M.$ Consider the
    telescope difference

    \begin{align*}
    & \left| g_n(y) - g_n(x) \right| \\
        &= \left| g_n(x+Mh) - g_n(x+(M-1)h) + \cdots + g_n(x+h) - g_n(x) \right| \\
        &\leq \left| g_n(x+Mh) - g_n(x+(M-1)h) \right| + \cdots + \left| g_n(x+h) - g_n(x) \right| \\
        &< M \e h = \e(y - x) \leq \e(b - a).
    \end{align*}

    In other words, the difference $\left| g_n(y) - g_n(x) \right|$
    can be made as small as we like, uniformly on $[a,b],$ by choosing
    sufficiently large $n,$ i.e. $g_n$ converges uniformly to a
    constant function. \qed
</p>

<p>
    <b>Proposition.</b> <i>Suppose that $f_n$ is a sequence of
    functions which are differentiable on $[a, b],$ that $f_n$
    converges point wise to $f,$ and that $f_n'$ converges uniformly
    to $f'.$ Then $f_n$ converges uniformly to $f.$</i>
</p>

<p>
    <i>Proof.</i> Uniform convergence of $f_n'$ means that for any $\e
    > 0,$ for sufficiently large $n$ we have

    $$|f_n'(x) - f'(x)| < \e$$

    for all $x\in[a,b],$ i.e.

    \begin{align*}
    & \left| \lim_{h\to 0} \frac{f_n(x+h) - f_n(x)}{h} - \lim_{h\to 0} \frac{f(x+h) - f(x)}{h} \right| \\
        &= \left| \lim_{h\to 0} \frac{f_n(x+h) - f(x+h) - f_n(x) + f(x)}{h} \right| \\
        &= \left| g_n'(x) \right| \\
        &< \e,
    \end{align*}

    where $g_n(x) := f_n(x) - f(x).$ In other words, $g_n'$ converges
    uniformly to 0. By the previous lemma, $g_n$ converges uniformly
    to a constant function, and since $g_n$ converges point wise to
    zero, this means $g_n$ converges uniformly to zero. \qed
</p>

<p>
    <b>Proposition.</b> <i>Suppose that $f_n$ is a sequence of
    functions which are differentiable on $[a, b],$ that $f_n'$
    converges uniformly to some continuous function $g,$ and that
    $f_n(x_0)$ converges to a number $L$ for some $x_0\in[a,b].$ Then
    $f_n$ converges point wise to a function $f.$ By the previous
    proposition, $f_n$ then converges uniformly to $f.$</i>
</p>

<p>
    <i>Proof.</i> Uniform convergence of $f_n'$ means that for any $\e
    > 0,$ for sufficiently large $n$ we have

    $$|f_n'(x) - g(x)| = \left| \lim_{h\to 0} \frac{f_n(x+h) - f_n(x)}{h} - g(x) \right| < \e$$

    for all $x\in[a,b].$ Therefore for such $n,$ for sufficiently
    small $h$ we have

    $$\left| f_n(x+h) - f_n(x) - h g(x) \right| < \e h.$$

    Now let $x$ be any number in $[a,b].$ For simplicity assume $x =
    x_0 + Mh$ for some positive integer $M.$ Consider the telescope

    \begin{align*}
    & \left| f_n(x) - f_n(x_0) - hg(x_0+(M-1)h) - \cdots - hg(x_0) \right| \\
    &= | f_n(x_0+Mh) - f_n(x_0+(M-1)h) - hg(x_0+(M-1)h)
        + \cdots + f_n(x_0+h) - f_n(x_0) - hg(x_0) | \\
    &\leq | f_n(x_0+Mh) - f_n(x_0+(M-1)h) - hg(x_0+(M-1)h) |
        + \cdots + | f_n(x_0+h) - f_n(x_0) - hg(x_0) | \\
    &< M\e h = \e(x - x_0) \leq \e(b - a).
    \end{align*}

    In other words,

    $$|f_n(x) - f_n(x_0) - hg(x_0+(M-1)h) - \cdots - hg(x_0)|$$

    can be made as small as we like for sufficiently large $n$ and
    small $h.$ Since

    \begin{align*}
    f_n(x_0) &\longrightarrow L \\
    hg(x_0+(M-1)h) + \cdots + hg(x_0) &\longrightarrow \int_{x_0}^x g
    \end{align*}

    as $n\longrightarrow\infty$ and $h\longrightarrow 0,$ we have

    $$f_n(x) \longrightarrow L + \int_{x_0}^x g.$$

    (Note that this is the fundamental theorem of calculus.) \qed
</p>

<h3>Infinite Sum of Functions</h3>

<p>
    <b>Corollary.</b> <i>Let $\sum_{n=1}^\infty f_n$ converge
    uniformly to $f$ on $[a, b].$

    <ul>
    <li>If each $f_n$ is continuous on $[a, b],$ then so is $f.$</li>
    <li>If $f_n$ and $f$ are integrable on $[a, b],$ then

    $$\int_a^b f = \sum_{n=1}^\infty \int_a^b f_n.$$</li>
    <li>If $\sum_{n=1}^\infty f_n$ converges pointwise to $f$ and each
    $f_n$ has integrable derivative $f_n'$ and $\sum_{n=1}^\infty
    f_n'$ converges uniformly on $[a, b]$ to some continuous function $g,$
    then

    $$f'(x) = g(x) = \sum_{n=1}^\infty f_n'(x).$$</li>
    </ul></i>
</p>

<h3>Weierstrass M-test</h3>

<p>
    <b>Theorem.</b> <i>Let $f_n$ be a sequence of functions defined on
    $A,$ and suppose that $M_n$ is a sequence of numbers such that

    $$|f_n(x)| \leq M_n \quad\text{ for all }\quad x \in A.$$

    Suppose also that $\sum_{n=1}^\infty M_n$ converges. Then for each
    $x \in A$ the series $\sum_{n=1}^\infty f_n(x)$ converges
    absolutely, and $\sum_{n=1}^\infty f_n$ converges uniformly on
    $A$ to the function

    $$f(x) = \sum_{n=1}^\infty f_n(x).$$</i>
</p>

<h3>Radius of convergence of power series and its derivative</h3>

<p>
    <b>Theorem.</b> <i>Suppose that the series

    $$f(x_0) = \sum_{n=0}^\infty a_n x_0^n$$

    converges, and let $a \in (0, |x_0|).$ Then the series

    $$f(x) = \sum_{n=0}^\infty a_n x^n$$

    converges uniformly and absolutely on $[-a, a],$ as does the series

    $$g(x) = \sum_{n=0}^\infty n a_n x^{n-1}.$$ Moreover, $f$ is
    differentiable and

    $$f'(x) = g(x)$$

    for all $x \in [0, |x_0|).$</i>
</p>

<p>
    <b>Corollary.</b> <i>A function defined by a convergent power
    series in an interval $(-R, R)$ is infinitely differentiable in
    that interval. A convergent power series centered at zero is the
    Taylor series at zero of the function it defines.</i>
</p>

<p>
    <b>Corollary.</b> <i>A convergent power series in an interval
    $(-R, R)$ is continuous and uniformly and absolutely convergent in
    any interval $[-a, a]$ for $a < R.$</i>
</p>

<h3>Power series for $\sin x / x$</h3>

<p>
    <b>Exercise.</b> <i>If

    $$f(x) = \begin{cases}
    \frac{\sin x}{x} & \text{if $x \neq 0$}\\
    1 & \text{otherwise,}
    \end{cases}$$

    find $f^{(k)}(0).$ Hint: find the power series for $f.$</i>
</p>

<p>
    <b>Solution.</b> The power series for $\sin x$ is

    $$\sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \cdots,$$

    convergent for all $x,$ so

    $$\frac{\sin x}{x} = 1 - \frac{x^2}{3!} + \frac{x^4}{5!} - \cdots,$$

    convergent for all $x \neq 0.$ By the previous corollary this is
    the Taylor series of $f$ centered at zero, so

    \begin{align*}
    \frac{f^{(k)}(0)}{k!} &= a_k = \frac{(-1)^{k/2}}{(k+1)!} \\
    f^{(k)}(0) &= \frac{(-1)^{k/2}}{k+1}
    \end{align*}

    for even $k,$ and $f^{(k)}(0) = 0$ for odd $k.$ \qed
</p>

<h3>Alternative proof Newton's binomial theorem</h3>

<p>
    <b>Theorem.</b> <i>For any $\a$ and $|x| < 1,$

    $$(1 + x)^\a = \sum_{k=1}^\infty {\a \choose k} x^k.$$</i>
</p>

<p>
    <i>Proof.</i> Let

    $$f(x) = \sum_{k=1}^\infty {\a \choose k} x^k.$$

    First show that $f$ satisfies the first order linear differential
    equation

    $$(1 + x) f'(x) = \a f(x)$$

    by differentiating $f$ and use <a href="#bfvtr3491">these two
    binomial identities.</a>

    Then solve the equation using integrating factor and show that $f$
    must be of the form

    $$f(x) = c(1 + x)^\a$$

    for some $c,$ which must clearly be $1.$ \qed
</p>

<h3>Weierstrass M test example</h3>

<p>
    <b>Exercise.</b> <i>Prove that the series

    $$\sum_{n=1}^\infty \frac{x}{n(1 + nx^2)}$$

    converges uniformly on $\bR.$</i>
</p>

<p>
    <i>Solution.</i> Show that the functions

    $$f_n(x) = \frac{x}{n(1 + nx^2)}$$

    have max absolute values $1/2n^{3/2}$ at $x = 1/\sqrt{n}$ by
    solving

    $$f_n'(x) = 0,$$

    then use the Weierstrass M test and the fact that $\sum 1/n^s$
    converges iff $s > 1.$ \qed
</p>

<h3>Some functions power series cannot represent</h3>

<p>
    <b>Proposition.</b> <i>Suppose that $f(x) = \sum a_n x^n$ for all
    $x$ in some interval $I = (-R, R)$ and that $f(x) = 0$ for all $x
    \in I.$ Then $a_n = 0.$</i>
</p>

<p>
    <i>Proof.</i> Since $f = 0$ in $I,$ its derivatives are also all
    zero there, so

    $$a_n = \frac{f^{(n)}(0)}{n!} = 0. \qed$$
</p>

<p>
    <b>Proposition.</b> <i>Suppose that $f(x) = \sum a_n x^n$ for all
    $x$ in some interval $I = (-R, R)$ and that $f(x_m) = 0$ for some
    sequence $x_m \longrightarrow 0.$ Then $a_n = 0.$</i>
</p>

<p>
    <i>Proof.</i> By a previous corollary, $f$ is continuous in any
    interval $[-a, a]$ for $a < R,$ so

    $$a_0 = f(0) = \lim_{x_m \to 0} f(x_m) = 0.$$

    Since $f$ is zero at $x_m,$ by the mean value theorem there is a
    sequence $y_m \longrightarrow 0$ s.t.

    $$f'(y_m) = 0.$$

    Applying the first step again, we have

    $$a_1 = 0,$$

    and so on for $a_2, a_3, \ldots.$ \qed
</p>

<div class="bside">
    <img src="images/sin1overx.png">
</div>
<p>
    <b>Example.</b> The function

    $$f = \sin 1/x$$

    cannot be represented as a power series, because it crosses 0
    infinitely many times near the origin.
</p>

<p>
    <b>Example.</b> A power series also can't represent a function
    which is 0 for $x \leq 0$ and nonzero for $x > 0,$ e.g. it cannot
    describe the motion of a particle that remains at rest until time
    0 and then starts moving.
</p>

<h3>Uniqueness of power series representation</h3>

<p>
    <b>Corollary.</b> <i>Suppose that $f(x) = \sum a_n x^n$ and
    $g(x) = \sum b_n x^n$ converge for all $x$ in some interval
    containing zero, and that $f(x_m) = g(x_m)$ for some sequence $x_m
    \longrightarrow 0.$ Then $a_n = b_n$ for all $n.$</i>
</p>

<p class="box">
    <b>Corollary.</b> <i>A function can only have one power series
    representation centered at $0.$</i>
</p>

<h3>Even and odd power series</h3>

<p>
    <b>Corollary.</b> <i>If $\sum_{n=0}^\infty a_n x^n$ is an even
    function, then $a_n = 0$ for odd $n.$ If $\sum_{n=0}^\infty a_n
    x^n$ is an odd function, then $a_n = 0$ for even $n.$</i>
</p>

<h3>Power series for log</h3>

<p>
    <b>Proposition.</b> <i>The power series for $f(x) = \log(1 - x)$
    converges only for $x\in[-1,1),$ and the power series for $g(x) =
    \log\frac{1+x}{1-x}$ converges only for $x\in(-1,1).$</i>
</p>

<p>
    <i>Proof.</i> Recall that the power series for $\log(1 + x)$
    around zero is
</p>
<p class="box">
    $$\log(1 + x) = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \cdots,$$
</p>
<p>
    convergent for $x\in (-1,1].$ Replacing $x$ with $-x$, we have the
    power series for $\log(1-x):$

    $$\log(1 - x) = -x - \frac{x^2}{2} - \frac{x^3}{3} - \frac{x^4}{4} - \cdots,$$

    which converges for $x = -1$ because it's the same series as
    $\log(1 + x)$ for $x = 1.$ However it doesn't converge for $x = 1$
    because that's the same series as $\log(1 + x)$ for $x = -1,$ or
    alternatively by comparison with the harmonic series.
</p>

<p class="bside">
    <b>*</b> Another way to see that this series converges for
    $x\in(-1,1)$ is to compare it with the geometric series $\sum_n
    r^n,$ which converges for $r \in (-1,1).$
</p>
<p>
    Since these two series are absolutely convergent for $x\in(-1,1),$
    we can rearrange their terms to get

    \begin{align*}
    \log\frac{1+x}{1-x} &= \log(1+x) - \log(1-x) \\
        &= 2 \left( x + \frac{x^3}{3} + \frac{x^5}{5} + \frac{x^7}{7} + \cdots \right)
    \end{align*}

    for $x \in (-1,1).$<b>*</b> For $x = \pm 1,$ the series diverges by
    comparison with the harmonic series:

    $$2 + \frac{2}{3} + \frac{2}{5} + \frac{2}{7} + \cdots > 1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \cdots,$$

    because

    $$\frac{2}{2k - 1} > \frac{1}{k}. \qed$$
</p>

<h3>Formula for Fibonacci sequence</h3>

<p class="box">
    <b>Proposition.</b> <i>Recall that the Fibonacci sequence $a_n$ is
    defined as

    $$a_1 = a_2 = 1, \quad a_{n+1} = a_n + a_{n-1}.$$

    A closed form for $a_n$ is

    $$a_n = \frac{\left( \frac{1+\sqrt{5}}{2} \right)^n -
                  \left( \frac{1-\sqrt{5}}{2} \right)^n}{\sqrt{5}}.$$</i>
</p>

<p>
    <i>Proof.</i> Let

    $$f(x) = \sum_{n=1}^\infty a_n x^n-1 = 1 + x + 2x^2 + 3x^3 + 5x^5 + 8x^6 + \cdots$$

    be the power series having the Fibonacci numbers as
    coefficients. By the ratio test, $f$ converges for $x \in I :=
    (-1/2, 1/2).$ Since $f$ is absolutely convergent in $I,$ we can
    rearrange its terms to get

    $$f(x) - f(x) x - f(x) x^2 = 1,$$

    and so

    $$f(x) = \frac{-1}{x^2 + x - 1}$$

    for $x\in I.$ In this form, $f$ has the partial fractions
    decomposition

    $$f(x) = \frac{-1}{x^2 + x - 1} = \frac{1/\sqrt{5}}{x - \frac{-1-\sqrt{5}}{2}}
                                    - \frac{1/\sqrt{5}}{x - \frac{-1+\sqrt{5}}{2}}.$$

    Using the power series

    $$\frac{1}{x - a} = -\frac{1}{a} - \frac{x}{a^2} - \frac{x^2}{a^3} - \cdots,$$

    convergent for $x \in (-a, a)$ (by comparison with the geometric
    series), we can rewrite $f$ as

    \begin{align*}
    f(x) &= \frac{1}{\sqrt{5}}\left( - \frac{1}{\frac{-1-\sqrt{5}}{2}} - \frac{x}{\left(\frac{-1-\sqrt{5}}{2}\right)^2} - \frac{x^2}{\left(\frac{-1-\sqrt{5}}{2}\right)^3} - \cdots \right) \\
         &- \frac{1}{\sqrt{5}}\left( - \frac{1}{\frac{-1+\sqrt{5}}{2}} - \frac{x}{\left(\frac{-1+\sqrt{5}}{2}\right)^2} - \frac{x^2}{\left(\frac{-1+\sqrt{5}}{2}\right)^3} - \cdots \right) \\
         &= \sum_{n=1}^\infty \left[ \frac{1}{\sqrt{5}}\frac{1}{\left(\frac{-1+\sqrt{5}}{2}\right)^n} - \frac{1}{\sqrt{5}}\frac{1}{\left(\frac{-1-\sqrt{5}}{2}\right)^n} \right] x^{n-1},
    \end{align*}

    convergent for $x \in (-a, a),$ where $a = \min\left(\left|
    \frac{-1+\sqrt{5}}{2} \right|, \left| \frac{-1-\sqrt{5}}{2}
    \right|\right) \approx 0.618,$ in particular it's convergent for
    $x \in I.$
</p>
<p>
    In other words, we derived two power series for $f$ around 0, so
    by our previous uniqueness theorem they must be the same, with the
    same coefficients:

    $$a_n = \frac{1}{\sqrt{5}}\frac{1}{\left(\frac{-1+\sqrt{5}}{2}\right)^n} - \frac{1}{\sqrt{5}}\frac{1}{\left(\frac{-1-\sqrt{5}}{2}\right)^n},$$

    which simplifies to

    $$a_n = \frac{\left( \frac{1+\sqrt{5}}{2} \right)^n - \left( \frac{1-\sqrt{5}}{2} \right)^n}{\sqrt{5}}$$

    since

    $$\frac{2}{-1 \pm \sqrt{5}} = \frac{1 \pm \sqrt{5}}{2}. \qed$$
</p>

<img src="images/fibonacci-spirals-sunflower-pattern.jpg">

<h3>Abel's theorem on uniformly convergent power series</h3>

<p>
    <b>Theorem.</b> <i>Suppose that $\sum a_n$ converges. We know that
    the series $f(x) = \sum_{n=0}^\infty a_n x^n$ must converge
    uniformly on $(-a,a)$ for $a\in(0,1),$ but it may not converge
    uniformly on $[-1,1].$ In fact it may not even converge for $x =
    -1,$ e.g. when $f(x) = \log(1+x),$ which converges only for
    $x\in(-1, 1].$ However, $f$ does converge uniformly on
    $[0,1].$</i>
</p>

<p>
    <i>Proof.</i> Let $\e > 0.$ Since $\sum a_n$ converges, by
    Cauchy's condition its tail must vanish, specificially there is an
    $N$ s.t. for all $m,n > N,$

    $$|a_m + \cdots + a_n| < \e.$$

    By Abel's lemma,

    $$|a_m x^m + \cdots + a_n x^n| \leq 2 x^m \e \leq 2 \e,$$

    for all $x\in [0, 1].$ Therefore the tail of $f$ vanishes
    uniformly on $[0,1].$ \qed
</p>

<p>
    <b>Corollary.</b> <i>$f$ is continuous on $[0,1],$ and in particular

    $$\sum_{n=0}^\infty a_n = \lim_{x\uparrow 1}\sum_{n=0}^\infty a_n x^n.$$</i>
</p>

<h3>Abel summable</h3>

<p>
    <b>Definition.</b> <i>A sequence $a_n$ is called Abel summable if

    $$\lim_{x\uparrow 1}\sum_{n=0}^\infty a_n x^n$$

    exists.</i>
</p>

<div class="bside">
    <img src="images/oneoveroneplusxsquared.png">
</div>
<p>
    <b>Example.</b> Using this terminology, the previous results say
    that a summable sequence is Abel summable. However the converse is
    not true: an abel summable sequence is not necessarily
    summable. E.g. the series

    $$f(x) = 1 - x^2 + x^4 - x^6 + x^8 - \cdots$$

    is the power series for

    $$\arctan'(x) = \frac{1}{1+x^2}$$

    on $(-1,1).$ The sequence of coefficients is abel summable since

    $$\lim_{x\uparrow 1} f(x) = \lim_{x\uparrow 1} \frac{1}{1+x^2} = \frac{1}{2},$$

    but it is not summable because the terms don't vanish. \qed
</p>

<p>
    At the risk of going insane you can write this as
</p>
<p class="box">
    $$1 - 1 + 1 - 1 + \cdots = \frac{1}{2},$$
</p>
<p>
    which kinda makes sense: you can't decide whether you want 0 or a
    dollar so you expect fifty cents as a compromise.
</p>

<p>
    <b>Exercise.</b> <i>

    $$\frac{1}{1\cdot 2} - \frac{1}{2\cdot 3} + \frac{1}{3\cdot 4} - \frac{1}{4\cdot 5} + \cdots = 2\log 2 - 1.$$</i>
</p>

<p>
    <i>Proof.</i> Observing that

    $$\frac{1}{n(n+1)} = \frac{1}{n} - \frac{1}{n+1},$$

    we can rewrite

    \begin{align*}
    \frac{1}{1\cdot 2} - \frac{1}{2\cdot 3} + \frac{1}{3\cdot 4} - \frac{1}{4\cdot 5} + \cdots
        &= \sum_{n=1}^\infty (-1)^{n+1} \left( \frac{1}{n} - \frac{1}{n+1} \right) \\
        &= 1 - \frac{1}{2} - \frac{1}{2} + \frac{1}{3} + \frac{1}{3} - \frac{1}{4} - \frac{1}{4} + \cdots \\
        &= 2\left( \frac{1}{3} - \frac{1}{4} + \frac{1}{5} - \frac{1}{6} + \cdots \right) \\
        &= 2\left( \log(1+1) - 1 + \frac{1}{2} \right) \\
        &= 2 \log 2 - 1,
    \end{align*}

    since

    $$\log(1 + x) = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \cdots. \qed$$
    </i>
</p>

<h3>Uniform convergence preserves boundedness</h3>

<p>
    <b>Proposition.</b> <i>If $f_n$ is a sequence of bounded functions
    that converges uniformly to $f$ on $[a,b],$ then $f$ is also
    bounded.</i>
</p>

<p>
    <b>Nonexample.</b> If $f_n$ only converges point wise to $f$ then
    $f$ may be unbounded, e.g. $\tan x(1-1/n)$ are bounded functions
    that converge point wise to $\tan x,$ which is unbounded on
    $[-\frac{\pi}{2},\frac{\pi}{2}].$
</p>
<p>
    More examples: $\frac{1}{x + 1/n}$ are bounded and converge point
    wise to $1/x,$ and $\min(n, 1/x)$ are also bounded and converge
    point wise to $1/x,$ which is unbounded on $[0,1].$ \qed
</p>

<h3>Point wise limit of continuous functions not necessarily continuous</h3>

<p>
    <b>Exercise.</b> <i>Suppose $f$ is differentiable. Then $f'$ is
    the point wise limit of a sequence of continuous functions.</i>
</p>

<p>
    <i>Proof.</i> Let

    $$g_n(x) = \frac{f(x + 1/n) - f(x)}{1/n}.$$

    Then $g_n$ are continuous and

    $$\lim_{n\to\infty} g_n(x) = \lim_{h\to 0} \frac{f(x + h) - f(x)}{h} = f'(x). \qed$$
</p>

<div class="aside">
    <img src="images/bc0zD.png">
    <img src="images/U6wd6.png">
</div>
<p>
    <b>Example of a function with discontinuous derivative.</b> This
    shows that the point wise limit of continuous functions needs not
    be continuous: let

    $$f(x) = \begin{cases}
    x^2 \sin \frac{1}{x} & \text{if } x \neq 0 \\
    0 & \text{if } x = 0
    \end{cases}$$

    with derivative

    $$f'(x) = \begin{cases}
    2x \sin \frac{1}{x} - \cos \frac{1}{x} & \text{if } x \neq 0 \\
    0 & \text{if } x = 0,
    \end{cases}$$

    which is discontinuous at 0. \qed
</p>

<h3>Point wise limit of integrable functions not necessarily integrable</h3>

<p>
    <b>Example.</b> <i>Find a sequence of integrable functions on $[0,
    1]$ that converge to

    $$f(x) = \begin{cases}
    1 & \text{for rational } x \\
    0 & \text{for irrational } x.
    \end{cases}$$</i>

    Let

    $$f_n(x) = \begin{cases}
    0 & \text{everywhere except} \\
    1 & \text{if } x = 0, \frac{1}{n}, \frac{2}{n}, \ldots, \frac{n}{n}.
    \end{cases}$$

    Then $f_n$ have finitely many points of discontinuity and so are
    integrable, and for any rational number $x = \frac{p}{q},$ for all
    sufficiently large $n$ we have

    $$\tfrac{p}{q} \in \{ 0, \tfrac{1}{n}, \tfrac{2}{n}, \ldots, \tfrac{n}{n} \},$$

    and therefore

    $$\lim_{n\to\infty} f_n(\tfrac{p}{q}) = 1.$$

    On the other hand for irrational $x$ and all $n,$

    $$f_n(x) = 0. \qed$$
</p>

<h3>Uniform limit of integrable functions is integrable</h3>

<p>
    <b>Proposition.</b> <i>If $f_n$ are integrable functions on
    $[a,b]$ that converge uniformly to $f,$ then $f$ is
    integrable and

    $$\int_a^b f = \lim_{n\to\infty} \int_a^b f_n.$$</i>
</p>

<p>
    <i>Proof.</i> Inferring appropriate notations, $f_n$ integrable
    means that

    $$\lim_{\D_i\to 0} \sum_i f_n(t_i) \D_i$$

    exist. We want to show the same thing for $f,$ so let $\D_i$ be
    any partition of $[a,b]$ and consider the difference

    \begin{align*}
    \left| \sum_i f_n(t_i) \D_i - \sum_i f(t_i) \D_i \right|
        &\leq \sum_i \left| f_n(t_i) - f(t_i) \right| \D_i \\
        &< \e \sum_i \D_i = \e(b-a),
    \end{align*}

    where $\e > 0$ is chosen so that $\left| f_n(t_i) - f(t_i) \right|
    < \e$ for all $x\in[a,b]$ for sufficiently large $n.$ In other
    words, for any partition $\D_i$ and large enough $n,$ we can make
    this difference as small as we like, i.e.

    $$\lim_{\D_i\to 0} \sum_i f(t_i) \D_i = \lim_{n\to\infty} \lim_{\D_i\to 0} \sum_i f_n(t_i) \D_i,$$

    and $f$ is integrable with integral

    $$\int_a^b f = \lim_{n\to\infty} \int_a^b f_n. \qed$$
</p>

<h3>A variation</h3>

<p>
    <b>Exercise.</b> <i>Suppose that $f_n$ are continuous functions on
    $[0,1]$ converging uniformly to $f.$ Then

    $$\lim_{n\to\infty} \int_0^{1-1/n} f_n = \int_0^1 f.$$</i>
</p>

<p>
    <i>Proof.</i> Consider:

    \begin{align*}
    \left| \int_0^1 f - \int_0^{1-1/n} f_n \right|
        &= \left| \int_0^1 (f - f_n) - \int_{1-1/n}^1 f_n \right| \\
        &\leq \left| \int_0^1 (f - f_n) \right| + \left| \int_{1-1/n}^1 f_n \right|.
    \end{align*}

    The first term goes to zero by the previous proposition. Since
    $f_n$ are continous and converge uniformly to $f,$ $f$ is also
    continuous and thus bounded. Therefore for large $n,$ $f_n$ is
    also bounded, so the second term also goes to zero. \qed
</p>

<p>
    <b>Example.</b> If $f_n$ does not converge uniformly to $f,$ then
    it's not necessarily true that

    $$\lim_{n\to\infty} \int_0^{1-1/n} f_n = \int_0^1 f.$$

    E.g. let $f_n$ be zero on $[0,1-1/n]$ and look like a triangle of
    height $n$ on $[1-1/n,1].$ Then

    $$\lim_{n\to\infty} \int_0^{1-1/n} f_n = \frac{1}{2} \neq 0 = \int_0^1 f.$$
</p>

<h3>Example of uniformly convergent series on $[0,\infty)$</h3>

<p>
    <b>Exercise.</b> <i>The series

    $$f(x) = \sum_{n=1}^\infty \frac{(-1)^n}{x+n}$$

    converges uniformly on $[0,\infty).$</i>
</p>
<p>
    <i>Proof.</i> First note that $f(x)$ converges for all $x\in A :=
    [0,\infty)$ by Leibniz's alternating series theorem. To show
    uniform convergence we need to show that the sequence of partial
    sums

    $$f_m(x) = \sum_{n=1}^m \frac{(-1)^n}{x+n}$$

    converges uniformly to $f,$ i.e. the tail of the series vanishes,
    i.e. for any $\e > 0,$

    $$|f(x) - f_{n-1}(x)| = \left|\frac{(-1)^{n}}{x+n}
        + \frac{(-1)^{n+1}}{x+n+1} + \cdots \right| < \e$$

    for all $x\in A$ for sufficiently large $n.$ Since $f(0) = \sum
    \frac{(-1)^n}{n}$ converges, its tail does vanishes:

    $$\left|\frac{(-1)^{n}}{n} + \frac{(-1)^{n+1}}{n+1} + \cdots \right| < \e,$$

    so it's enough to show that

    $$\left|\frac{(-1)^{n}}{x+n} + \frac{(-1)^{n+1}}{x+n+1} + \cdots \right|
    \leq \left|\frac{(-1)^{n}}{n} + \frac{(-1)^{n+1}}{n+1} + \cdots \right|$$

    for all $x\in A.$
</p>
<p>
    For notational convenience and without loss of generality, assume
    that the tail starts with a positive sign, so we can drop the
    absolute value symbols and write

    $$\frac{1}{x+n} - \frac{1}{x+n+1} + \frac{1}{x+n+2} - \frac{1}{x+n+3} + \cdots.$$

    Note that the terms in this series have decreasing absolute
    values, and so successive difference pairs are positive: the first
    minus the second term is positive, the third minus the fourth is
    positive, fifth minus sixth, and so on. Therefore to show that

    $$\frac{1}{x+n} - \frac{1}{x+n+1} + \cdots \leq \frac{1}{n} - \frac{1}{n+1} + \cdots$$

    it's enough to show that these difference pairs satisfy

    $$\frac{1}{x+n+2k} - \frac{1}{x+n+2k+1} \leq \frac{1}{n+2k} - \frac{1}{n+2k+1}$$

    for all $k,$ which is clearly true:

    $$\frac{1}{(x+n+2k)(x+n+2k+1)} \leq \frac{1}{(n+2k)(n+2k+1)}. \qed$$
</p>

<h3>Example. Uniform limit of length is not length of the limit</h3>

<p>
    <i>Find a sequence $f_n$ approaching $f$ uniformly on $[0,1]$ s.t.

    $$\lim_{n\to\infty}(\text{length of $f_n$ on $[0,1]$}) \neq \text{length of $f.$}$$</i>
</p>
<p>
    Let $f_1$ be the triangle of height 1, $f_2$ be the two triangles
    of height $1/2,$ $f_3$ be the 4 triangles of height $1/4,$ and so
    on. Then each $f_n$ has length $\sqrt{5}$ and the sequence
    converges uniformly to $f=0,$ therefore

    $$\lim_{n\to\infty}(\text{length of $f_n$ on $[0,1]$})
        = \sqrt{5} \neq 0 = \text{length of $f.$} \qed$$
</p>

<h1>Complex Numbers</h1>

<h3>Roots of complex polynomial with real coefficients</h3>

<p>
    <b>Proposition.</b> <i>Let $f(z) = z^n + a_{n-1}z^{n-1} + \cdots +
    a_0$ be a complex polynomial with real coefficients and $z_0 = a +
    ib$ be one of its roots. Then $\bar{z_0} = a - ib$ is also a
    root. Thus nonreal roots of $f$ always appear in pairs, and the
    number of such roots is even.</i>
</p>

<p>
    <i>Proof.</i> Follows from conjugate rules for product, sum, and
    real numbers. \qed
</p>

<p>
    <b>Corollary.</b> <i>$f$ is divisible by $(z - z_0)(z - \bar{z_0})
    = z^2 - 2az + a^2 + b^2,$ a quadratic polynomial with real
    coefficients.</i>
</p>

<p>
    <b>Corollary.</b> <i>Every real polynomial $f(x) = x^n +
    a_{n-1}x^{n-1} + \cdots + a_0$ is divisible by the polynomial $x^2
    - 2ax + a^2 + b^2,$ where $a+ib$ is a complex root of $f.$</i>
</p>
<p>
    <b>Corollary.</b> <i>Every real polynomial $f(x) = x^n +
    a_{n-1}x^{n-1} + \cdots + a_0$ can be written as a product of
    linear factors of the form $x-a$ and irreducible quadratic factors
    of the form $x^2 - 2ax + a^2 + b^2.$</i>
</p>

<h3>Sum of polynomial squares</h3>

<p>
    <b>Definition.</b> <i>A real polynomial $f(x) = x^n +
    a_{n-1}x^{n-1} + \cdots + a_0$ is called a sum of squares if it
    can be written as $h_1^2 + \cdots + h_k^2$ for polynomials
    $h_i.$</i>
</p>
<p>
    <b>Corollary.</b> <i>If $f$ and $g$ are sums of squares, then so
    is $fg.$</i>
</p>
<p>
    <b>Proposition.</b> <i>$f$ is a sum of squares iff $f(x) \geq 0$
    for all $x.$</i>
</p>
<p>
    <i>Proof.</i> If $f$ is a sum of squares, then clearly $f(x) \geq
    0$ for all $x.$ Now suppose that $f \geq 0.$ By a previous
    corollary, $f$ can be written as a product of linear factors of
    the form $x-a$ and irreducible quadratic factors of the form $x^2
    - 2ax + a^2 + b^2$:

    $$f(x) = \underbrace{\prod_{a,k} \underbrace{(x - a)^k}_{\stackrel{?}{\geq} 0}}_{\geq 0}
        \prod_{a,b} \underbrace{(x^2 - 2ax + a^2 + b^2)}_{>0},$$

    where $k$ is the multiplicity of each root $a.$ Since the
    quadratic factors are irreducible, they have no roots, and since
    their leading coefficients are $1,$ they must all be positive for
    all $x.$ Since $f \geq 0,$ this implies $\prod_{a,k} (x - a)^k
    \geq 0.$
</p>
<p>
    Note that each quadratic factor is a sum of squares. We'll show
    that each $k$ is even, so that $(x-a)^k$ is a square, and apply
    the previous corollary to conclude that their product is also a
    sum of squares. We'll show this starting with the largest root and
    its multiplicity, and then down to the smallest root.
</p>
<p>
    Let $a$ be the largest root and $k$ be its multiplicity. (If $a$
    is the only root, then $(x-a)^k \geq 0$ implies that $k$ is even,
    and we're done.) Otherwise let $b$ be the second largest root and
    $l$ its multiplicity, let $c$ be etc. Then for $x \in (b, a),$

    \begin{align*}
    x - a &< 0 \\
    x - b &> 0 \\
    \vdots \\
    x - c &> 0.
    \end{align*}

    But

    $$(x - a)^k \underbrace{(x - b)^l \cdots (x - c)^m }_{>0} \geq 0$$

    implies that

    $$(x - a)^k \geq 0.$$

    Therefore $k$ must be even. Continue this way to the smallest root
    to find that all linear multiplicities are even, so $(x-a)^k$ are
    all squares for all roots $a.$ \qed
</p>

<h3>Primitive nth roots</h3>

<p>
    A number $w$ is called a primitive $n$-th root of 1 if
    $1,w,\ldots,w^{n-1}$ are all $n$-th roots of 1.
</p>

<p>
    <b>Proposition.</b> <i>There are $\f(n)$ primitive $n$-th roots of
    $1,$ where $\f$ is Euler's totient function counting the number of
    positive integers less than $n$ which are relatively prime to
    it.</i>
</p>

<p>
    <b>Proposition.</b> <i>Let $w \neq 1$ be an $n$-th root of $1.$
    Then

    $$\sum_{k=0}^{n-1} w^k = 0.$$</i>
</p>

<p>
    <i>Proof.</i> Let $w = \cos\frac{2\pi k}{n} + i \sin\frac{2\pi
    k}{n}$ and write out $\sum_{k=0}^{n-1} w^k.$ Then use Lagrange's
    trigonometric identities:

    \begin{align*}
    \frac{1}{2} + \sum_{k=1}^n \cos kx &= \frac{\sin(n + \frac{1}{2})x}{2\sin \frac{x}{2}} \\
    \sum_{k=1}^n \sin kx &= \frac{\sin \frac{n+1}{2}x \sin \frac{n}{2}x}{\sin \frac{x}{2}}. \qed
    \end{align*}
</p>

<p>
    <b>Conjecture.</b> <i>Conversely, suppose that $|w_0| = |w_1| =
    \ldots |w_{n-1}|$ and $\sum w_k = 0.$ Then $w_k$ are the vertices
    of an equilateral $n$-gon.</i>
</p>

<p>
    This is true for $n=3.$
</p>

<h3>Intermediate value theorem for real and complex valued complex functions</h3>

<p>
    <b>Exercise.</b> <i>Suppose that $f$ is a real valued function
    defined on a closed rectangle $R = [a,b]\times[c,d].$ If $f$ takes
    on the values $f(z)$ and $f(w)$ for $z,w\in R,$ then $f$ also
    takes on all values between $f(z)$ and $f(w).$</i>
</p>
<p>
    <i>However, if $f$ is complex valued, then this is not true:
    e.g. for $z = x+iy,$ define the complex exponential function

    $$f(z) = e^z = e^x e^{iy} = e^x(\cos y + i\sin y),$$

    continuous for all $z.$ Then $f$ takes on $f(i\pi/2) = i$ and
    $f(-i\pi/2) = -i,$ but doesn't take on the point zero on the line
    between $i$ and $-i.$</i>
</p>
<img src="images/complexexponential.png">

<h1>Complex Power Series</h1>

<h1>Reference</h1>

<ol>
<li>Spivak's Calculus.</li>
<li>Everything else from the web.</li>
</ol>

</div>
</body>
</html>
