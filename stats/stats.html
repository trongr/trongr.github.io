<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Probability and Statistics</title>
<link rel="stylesheet" href="../css/global.css">

<link rel="stylesheet" href="../css/obsidian.min.css">
<script src="../js/highlight.min.js"></script>

<!-- this config must be before MathJax.js: -->
<script src="../js/mathjax.config.js"></script>
<script src="../js/MathJax/MathJax.js"></script>
<!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script> -->

<script src="../js/jquery-3.1.0.min.js"></script>
<script src="../js/smartquotes.js"></script>
<script src="../js/global.js"></script>

</head>
<body>
<div id="content">

<h1>Probability and Statistics</h1>

<h2>Union Laws</h2>

<p>
    More generally, De Morgan's Laws also hold for any indexing set,
    whether it's countable or not:
</p>

<p>
    <b>Theorem.</b> <i>Let $I$ be any indexing set. Then

    \begin{align*}
    \bar{\Cup_{i \in I} A_i} &= \Cap_{i \in I} \bar{A_i} \\
    \bar{\Cap_{i \in I} A_i} &= \Cup_{i \in I} \bar{A_i}.
    \end{align*}
    </i>
</p>

<p>
    <i>Proof.</i> We'll just prove the first equality. If an element
    $x$ belongs to the set on the LHS, then it must not be in any
    $A_i,$ i.e. it belongs to all $\bar{A_i},$ and so it must belong to
    their intersection on the RHS. \qed
</p>

<h2>Sigma Algebra</h2>

<p>
    <b>Definition.</b> <i>A collection $\BB$ of subsets of $S$ is
    called a sigma algebra if it satisfies:

    <ul>
        <li>$\BB$ contains the empty set: $\0 \in \BB.$</li>
        <li>$\BB$ is closed under complementation: if $A \in \BB$ then $A^c \in \BB.$</li>
        <li>$\BB$ is closed under countable union: if $A_i \in \BB,$ then $\Cup_{i=1}^\infty A_i \in \BB.$</li>
    </ul></i>
</p>

<p>
    <b>Corollary.</b> <i>If $\BB$ is a sigma algebra, then it contains
    $S$ and also closed under countable intersection:

    <ul>
        <li>$S \in \BB.$</li>
        <li>$A_i \in \BB$ implies $\Cap_{i=1}^\infty A_i \in \BB.$</li>
    </ul></i>
</p>

<h2>Axioms of Probability</h2>

<!-- <div class="bside">
    The third axiom is also called the Axiom of Countable Additivity.
</div> -->
<p>
    <b>Definition.</b> <i>Let $S$ be a sample space with sigma algebra
    $\BB.$ A probability function on $\BB$ is a function
    $P:\BB\longmapsto [0, 1]$ s.t.

    <ol>
        <li>$P(A) \geq 0$ for all $A \in \BB.$</li>
        <li>$P(S) = 1.$</li>
        <li>If $A_1, A_2,\ldots \in \BB$ are pair wise disjoint, then $$P(\cup A_i) = \sum P(A_i).$$</li>
    </ol>
    </i>
</p>

<p>
    <b>Corollary.</b> <i>

    <ul>
        <li>$P(\0) = 0.$</li>
        <li>$P(\bar{A}) = 1 - P(A).$</li>
        <li>If $A\subset B$ then $P(A) \leq P(B).$</li>
        <li>$P(A \cup B) = P(A) + P(B) - P(A \cap B).$</li>
    </ul>
    </i>
</p>

<h2>Inclusion Exclusion Principle</h2>

<p>
    <b>Theorem.</b> <i>If $A_1,\ldots, A_n\in \BB$ then

    $$P(\cup_i A_i) = \sum_{\L_1} P(\L_1) - \sum_{\L_2} P(\L_2) + \cdots \pm \sum_{\L_n} P(\L_n),$$

    where $\L_k$ denotes any $k$-intersection, e.g. $A_1 \cap \cdots
    \cap A_k,$ and the sum $\sum_{\L_k} $is over all such possible
    $k$-intersections.</i>
</p>

<p>
    <b>Theorem.</b> <i>If $P$ is a probability function, then

    <ul>
    <li>$P(A) = \sum\limits_{i=1}^\infty P(A \cap C_i)$ for any partition $C_i$ of $\O.$</li>
    <li>Boole's Inequality: $P(\Cup_{i=1}^\infty A_i) \leq \sum\limits_{i=1}^\infty A_i$ for any sets $A_i.$</li>
    </ul></i>
</p>

<p>
    <b>Remark.</b> Can get a more general version of Bonferroni from
    Boole:

    $$P(\cap_{i=1}^n A_i) \geq \sum_{i=1}^n P(A_i) - (n-1).$$
</p>

<h2>Counting</h2>

<p>
    The number of ways of choosing $k$ balls from a collection of $n$
    balls, provided the order matters or not and you can replace
    selected balls or not:
</p>

<table>
  <tr>
    <td></td>
    <td>With replacement</td>
    <td>Without replacement</td>
  </tr>
  <tr>
    <td>Ordered</td>
    <td>$$n^k$$</td>
    <td>$$\frac{n!}{(n-k)!} = {n \choose k} k!$$</td>
  </tr>
  <tr>
    <td>Unordered</td>
    <td class="yellow">$${n - 1 + k \choose k}$$</td>
    <td>$${n \choose k}$$</td>
  </tr>
</table>

<p>
    <i>Example. The number of shortest polygonal paths (with horizontal and
    vertical segments) joining the lower left (A) and upper right (B) squares of
    a chess board is ${ 14 \choose 7 }.$</i> In order to go from A to B, you
    have to go right 7 steps (R) and up 7 steps (U). Therefore these paths
    correspond one-to-one to the set of 14-letter sequences consisting of 7 R's
    and 7 U's in some order, e.g. RRRRRRRUUUUUUU is the path where you go all
    the way right and then all the way up. Recast in this form, it's easy to
    count these sequences: we have to choose 7 places to put the R's; after that
    the U's are determined: hence there are ${ 14 \choose 7 }$ such sequences.
</p>

<h3>The sampling process matters</h3>

<p>
    <i>In order to correctly calculate the probability of an outcome,
    you have to take into account the whole sequence of events leading
    up to the outcome and not just the end result.</i>
</p>

<h3>Multinomial coefficient</h3>

<p>
    <b>Proposition.</b> <i>The number of ways of choosing $n$ numbers
    from $m$ different numbers repeated $k_1,\ldots,k_m$ times is

    $$\binom{n}{k_1,\ldots,k_m} = \frac{n!}{k_1!\cdots k_m!}.$$</i>
</p>

<p>
    Note that the $k_i$'s have to add up to $n.$
</p>

<h2>Conditional probability and independence</h2>

<p>
    <b>Definition.</b> <i>If $A$ and $B$ are events in $S,$ and $P(B)
    > 0,$ then the conditional probability of $A$ given $B$ is

    $$P(A|B) = \frac{P(A\cap B)}{P(B)}.$$</i>
</p>

<h1>The center of a probability distribution</h1>

<h3>Mean for discrete distribution</h3>

<p>
  $$
    \bar{x} = \frac{1}{n} \sum_{i} x_i,
  $$

  or alternatively:

  $$
    \bar{x} = \frac{1}{n} \sum_{x} x n(x),
  $$
where $ n(x) $ is the number of occurrences of $ x. $ This becomes $$
  \bar{x} = \sum_{x} x \frac{n(x)}{n} = \sum_{x} x p(x),
$$
where $ p(x) := \frac{n(x)}{n} $ is the observed frequency of $ x. $ This mean approaches the true mean $$
  \mu = \sum_{x} x P(x),
$$
where $ P(x) $ is the true probability of $ x. $
</p>

<h3>Mean for continuous distribution</h3>

<p>
  The probability function $ P(x) $ is replaced by the infinitesimal $ f(x) dx, $ where $ f(x) $ is the probability density function, and the sum is replaced by the integral: $$
    \mu = \int x f(x) dx.
  $$
</p>

<h1>Measures of dispersion of a distribution</h1>

<h2>Deviation from the mean</h2>

<p>
  The average of deviations from the mean of all samples: $$
    d = \sum_{x} \left| x - \mu \right| P(x)
  $$
  for the discrete case, and $$
    d = \int \left| x - \mu \right| f(x) dx
  $$
  for the continuous case.
</p>

</div>
</body>
</html>
