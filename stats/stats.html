<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Probability and Statistics</title>
<link rel="stylesheet" href="../css/global.css">

<link rel="stylesheet" href="../css/obsidian.min.css">
<script src="../js/highlight.min.js"></script>

<!-- this config must be before MathJax.js: -->
<script src="../js/mathjax.config.js"></script>
<script src="../js/MathJax/MathJax.js"></script>
<!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script> -->

<script src="../js/jquery-3.1.0.min.js"></script>
<script src="../js/smartquotes.js"></script>
<script src="../js/global.js"></script>

</head>
<body>
<div id="content">

<h1>Probability and Statistics</h1>

<!-- <img src="images/Monkey-typing.jpg">
<div class="epigraph">
     <div class="quote">
        Given an infinite length of time, a chimpanzee punching at random on a
        typewriter would almost surely type out all of Shakespeare's plays.
     </div><hr><div class="author">Infinite Monkey Theorem</div>
</div>
<div class="clearboth"></div> -->

<h2>Union Laws</h2>

<!-- <img src="images/unionlaws.png"> -->

<div class="bside">
    <pre><code>!(a && b && c) == !a || !b || !c
!(a || b || c) == !a && !b && !c</code></pre>
    <img src="images/600px-Demorganlaws.svg.png">
</div>

<p>
    More generally, De Morgan's Laws also hold for any indexing set,
    whether it's countable or not:
</p>

<p>
    <b>Theorem.</b> <i>Let $I$ be any indexing set. Then

    \begin{align*}
    \bar{\Cup_{i \in I} A_i} &= \Cap_{i \in I} \bar{A_i} \\
    \bar{\Cap_{i \in I} A_i} &= \Cup_{i \in I} \bar{A_i}.
    \end{align*}
    </i>
</p>

<p>
    <i>Proof.</i> We'll just prove the first equality. If an element
    $x$ belongs to the set on the LHS, then it must not be in any
    $A_i,$ i.e. it belongs to all $\bar{A_i},$ and so it must belong to
    their intersection on the RHS. \qed
</p>

<h2>Sigma Algebra</h2>

<p>
    <b>Definition.</b> <i>A collection $\BB$ of subsets of $S$ is
    called a sigma algebra if it satisfies:

    <ul>
        <li>$\BB$ contains the empty set: $\0 \in \BB.$</li>
        <li>$\BB$ is closed under complementation: if $A \in \BB$ then $A^c \in \BB.$</li>
        <li>$\BB$ is closed under countable union: if $A_i \in \BB,$ then $\Cup_{i=1}^\infty A_i \in \BB.$</li>
    </ul></i>
</p>

<p>
    <b>Corollary.</b> <i>If $\BB$ is a sigma algebra, then it contains
    $S$ and also closed under countable intersection:

    <ul>
        <li>$S \in \BB.$</li>
        <li>$A_i \in \BB$ implies $\Cap_{i=1}^\infty A_i \in \BB.$</li>
    </ul></i>
</p>

<h2>Axioms of Probability</h2>

<div class="bside">
    The third axiom is also called the Axiom of Countable Additivity.
</div>
<p>
    <b>Definition.</b> <i>Let $S$ be a sample space with sigma algebra
    $\BB.$ A probability function on $\BB$ is a function
    $P:\BB\longmapsto [0, 1]$ s.t.

    <ol>
        <li>$P(A) \geq 0$ for all $A \in \BB.$</li>
        <li>$P(S) = 1.$</li>
        <li>If $A_1, A_2,\ldots \in \BB$ are pair wise disjoint, then $$P(\cup A_i) = \sum P(A_i).$$</li>
    </ol>
    </i>
</p>

<p>
    <b>Corollary.</b> <i>

    <ul>
        <li>$P(\0) = 0.$</li>
        <li>$P(\bar{A}) = 1 - P(A).$</li>
        <li>If $A\subset B$ then $P(A) \leq P(B).$</li>
        <li>$P(A \cup B) = P(A) + P(B) - P(A \cap B).$</li>
    </ul>
    </i>
</p>

<img src="images/bonferroniineq3.png">

<div class="aside">
    Contour and 3D plot of Bonferroni.
    <img src="images/bonferroniineq2.png">
    <img src="images/bonferroniineq.png">
</div>

<div class="clearboth"></div>

<h2>Inclusion Exclusion Principle</h2>

<p>
    <b>Theorem.</b> <i>If $A_1,\ldots, A_n\in \BB$ then

    $$P(\cup_i A_i) = \sum_{\L_1} P(\L_1) - \sum_{\L_2} P(\L_2) + \cdots \pm \sum_{\L_n} P(\L_n),$$

    where $\L_k$ denotes any $k$-intersection, e.g. $A_1 \cap \cdots
    \cap A_k,$ and the sum $\sum_{\L_k} $is over all such possible
    $k$-intersections.</i>
</p>

<p>
    <b>Theorem.</b> <i>If $P$ is a probability function, then

    <ul>
    <li>$P(A) = \sum\limits_{i=1}^\infty P(A \cap C_i)$ for any partition $C_i$ of $\O.$</li>
    <li>Boole's Inequality: $P(\Cup_{i=1}^\infty A_i) \leq \sum\limits_{i=1}^\infty A_i$ for any sets $A_i.$</li>
    </ul></i>
</p>

<p>
    <b>Remark.</b> Can get a more general version of Bonferroni from
    Boole:

    $$P(\cap_{i=1}^n A_i) \geq \sum_{i=1}^n P(A_i) - (n-1).$$
</p>

<h2>Counting</h2>

<p>
    The number of ways of choosing $k$ balls from a collection of $n$
    balls, provided the order matters or not and you can replace
    selected balls or not:
</p>

<table>
  <tr>
    <td></td>
    <td>With replacement</td>
    <td>Without replacement</td>
  </tr>
  <tr>
    <td>Ordered</td>
    <td>$$n^k$$</td>
    <td>$$\frac{n!}{(n-k)!} = {n \choose k} k!$$</td>
  </tr>
  <tr>
    <td>Unordered</td>
    <td class="yellow">$${n - 1 + k \choose k}$$</td>
    <td>$${n \choose k}$$</td>
  </tr>
</table>

<p>
    <i>Example. The number of shortest polygonal paths (with horizontal and
    vertical segments) joining the lower left (A) and upper right (B) squares of
    a chess board is ${ 14 \choose 7 }.$</i> In order to go from A to B, you
    have to go right 7 steps (R) and up 7 steps (U). Therefore these paths
    correspond one-to-one to the set of 14-letter sequences consisting of 7 R's
    and 7 U's in some order, e.g. RRRRRRRUUUUUUU is the path where you go all
    the way right and then all the way up. Recast in this form, it's easy to
    count these sequences: we have to choose 7 places to put the R's; after that
    the U's are determined: hence there are ${ 14 \choose 7 }$ such sequences.
</p>

<p>
    <img src="images/unorderedwithreplacement.png">
</p>

<h3>The sampling process matters</h3>

<p>
    <i>In order to correctly calculate the probability of an outcome,
    you have to take into account the whole sequence of events leading
    up to the outcome and not just the end result:</i>
</p>

<p>
    <img src="images/samplingwithreplacement.png">
</p>

<h3>Multinomial coefficient</h3>

<p>
    <b>Proposition.</b> <i>The number of ways of choosing $n$ numbers
    from $m$ different numbers repeated $k_1,\ldots,k_m$ times is

    $$\binom{n}{k_1,\ldots,k_m} = \frac{n!}{k_1!\cdots k_m!}.$$</i>
</p>

<p>
    Note that the $k_i$'s have to add up to $n.$
</p>

<h2>Conditional probability and independence</h2>

<p>
    <b>Definition.</b> <i>If $A$ and $B$ are events in $S,$ and $P(B)
    > 0,$ then the conditional probability of $A$ given $B$ is

    $$P(A|B) = \frac{P(A\cap B)}{P(B)}.$$</i>
</p>

<h3>Simulating de Mere's Bets</h3>

<pre><code>bool throws_a_pair_of_sixes() {
    int r = rand() % 6 + 1;
    int s = rand() % 6 + 1;
    if (r == 6 && s == 6) {
        return true;
    } else {
        return false;
    }
}

bool throws_a_pair_of_sixes_in_n_throws(int n) {
    for (int i = 0; i < n; i++) {
        if (throws_a_pair_of_sixes()) {
            return true;
        }
    }
    return false;
}

int main() {
    srand(time(NULL)); // should only be called once

    int NUM_RUNS = 100000;
    int NUM_THROWS = 24;
    int win_count = 0;

    for (int i = 0; i < NUM_RUNS; i++) {
        if (throws_a_pair_of_sixes_in_n_throws(NUM_THROWS)) {
            win_count++;
        }
    }
    printf("Win count: %d. Num runs: %d\n", win_count, NUM_RUNS);

    return 0;
}</code></pre>



<h1>Reference</h1>

<ol>
<li>Statistical Inference by Casella and Berger.</li>
<li>Intro to Probability by Charles M. Grinstead.</li>
<li>An Intro to Probability Theory and its Applications by William Feller.</li>
</ol>

</div>
</body>
</html>
