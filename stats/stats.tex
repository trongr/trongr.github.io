\documentclass[12pt]{article}
\usepackage{geometry}
\geometry{letterpaper}
\usepackage{float}
\usepackage{graphicx}
\graphicspath{{images/}}

\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{epstopdf}
\usepackage{epigraph}
\usepackage{url}
\usepackage{mathtools}

\usepackage{pdfrender,xcolor}

\usepackage{lmodern}
\usepackage[T1]{fontenc}

% Computer Concrete
% \usepackage{concmath}
% \usepackage[T1]{fontenc}

% Times variants
%
% \usepackage{mathptmx}
% \usepackage[T1]{fontenc}
%
% \usepackage[T1]{fontenc}
% \usepackage{stix}
%
% Needs to typeset using LuaLaTeX:
% \usepackage{unicode-math}
% \setmainfont{XITS}
% \setmathfont{XITS Math}

% garamond
% \usepackage[cmintegrals,cmbraces]{newtxmath}
% \usepackage{ebgaramond-maths}
% \usepackage[T1]{fontenc}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{question}[theorem]{Question}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{keywords}{Keywords}
\newtheorem{reference}{Reference}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{note}[theorem]{Note}

%\newcommand{\defeq}{\coloneqq}
\newcommand*{\defeq}{\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}
                     =}

\newcommand{\N}{\mathbf N}
\newcommand{\Q}{\mathbf Q}
\newcommand{\R}{\mathbf R}

\title{Statistics}
\author{Trong Truong}
\date{\today}

\begin{document}
\pdfrender{StrokeColor=black,TextRenderingMode=2,LineWidth=0.3pt}
\sloppy
\maketitle

\section{Conditional probability}

If the conditional probability of A given B is equal to the (unconditional)
probability of B, we say that A and B are statistically independent. This means
occurrence of B does not change the probability of A occurring. This means that
\begin{align*}
P(A) = P ( A | B ) &= \frac { P ( A \cap B ) } { P ( B ) } \\
P(A) P(B) &= P(A \cap B) \quad \text{(Law of multiplication).}
\end{align*}

\section{Mendel's Laws of Heredity}

It is amazing that you can deduce a law about the \emph{discrete} nature of
heredity (genes) from the statistics of plant breeding, specifically from the
proportion $$
0.749 \approx 0.75 = \frac{3}{4}
$$ of offspring of hybrid peas showing dominant traits. Whenever such neat
ratios are found in nature, they almost surely indicate a discreteness in the
underlying phenomenon.

\section{General Law of Addition / Inclusion Exclusion Principle}

\begin{theorem}[Inclusion Exclusion Principle]
Let $ E_1, \ldots, E_n $ be any events. Then $$
  P(E_1 + \cdots + E_n) = \sum_i P(E_i) - \sum_{i<j} P(E_i E_j) + \sum_{i<j<k} P(E_i E_j E_k) - \cdots (-1)^{n-1} P(E_1 \cdots E_n).
$$
\end{theorem}

\begin{proposition}
  Let $ E_1, \ldots, E_n $ be any events. Then $$
  \sum_i P(E_i) - \sum_{i<j} P(E_i E_j) \leq P(E_1 + \cdots + E_n) \leq \sum_i P(E_i).
  $$
\end{proposition}

\begin{note}
  Note that $ E_1 + \cdots + E_n $ is the event where at least one of the $ E_i $'s occurs.
\end{note}

\begin{proof}
This is obvious if we think of the events $ E_i $ as regions in a plane and $ P $ as the area function. For example, in this case $ P(E_1 + \cdots + E_n) $ represents the area $A$ covered by all the $ E_i $'s; $ \sum_i P(E_i) - \sum_{i<j} P(E_i E_j) $ represents the area covered by all the $ E_i $'s except the intersections of any pair $ E_i, E_j, $ which is clearly a smaller area than $A.$
\end{proof}

\begin{definition}[Pairwise and mutual independence]
  The events $ E_1, \ldots, E_n $ are called pairwise independent if $$
    P(E_i E_j) = P(E_i) P(E_j)
  $$ for any pair $ i \neq j. $ They are called mutually independent if they also satisfy
    \begin{align*}
      P(E_i E_j E_k) &= P(E_i) P(E_j) P(E_k) \\
      &\cdots \\
      P(E_1 \cdots E_n) &= P(E_1) \cdots P(E_n)
    \end{align*}
  for any set of distinct $ i, j, \ldots. $
\end{definition}

\begin{example}
  Pairwise independence does not imply mutual independence. E.g. Consider the
  experiment of throwing two dice, one red and one black, and let $ E_1, E_2,
  E_3 $ be the events of throwing odd on red, odd on black, and an odd sum on
  both dice, respectively. Then it's easy to verify that $ E_i $ are pairwise independent, but they are not mutually independent because $$
    P(E_1 E_2 E_3) = 0 \neq \frac{1}{8} = P(E_1) P(E_2) P(E_3).
  $$
\end{example}

\section{Random Variables and Probability Distribution}



\end{document}
