<!DOCTYPE html>
<html>

<head>
<meta charset="utf-8">
<title>Artificial Intelligence II</title>
<link rel="stylesheet" href="../css/global.css">

<link rel="stylesheet" href="../css/obsidian.min.css">
<script src="../js/highlight.min.js"></script>

<!-- this config must be before MathJax.js: -->
<script src="../js/mathjax.config.js"></script>
<script src="../js/MathJax/MathJax.js"></script>
<!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script> -->

<script src="../js/jquery-3.1.0.min.js"></script>
<script src="../js/smartquotes.js"></script>
<script src="../js/global.js"></script>

</head>

<body>
<div id="content">

<h1>Artificial Intelligence II</h1>

<h1>RNN: Gated Recurrent Units (GRU)</h1>

<p>
    \begin{align*}
        z_t &= \s(W_z x_t + U_z h_{t-1}) & \textrm{Update Gate} \\
        r_t &= \s(W_r x_t + U_r h_{t-1}) & \textrm{Reset Gate} \\
        \tilde h_t &= \tanh(r_t \circ U h_{t-1} + W x_t) & \textrm{New Memory} \\
        h_t &= (1 - z_t) \circ \tilde h_t + z_t \circ h_{t-1} & \textrm{Hidden State}
    \end{align*}
</p>

<h1>RNN: Long Short-Term Memory (LSTM)</h1>

<p>
    \begin{align*}
        i_t &= \s(W_i x_t + U_i h_{t-1}) & \textrm{Input Gate} \\
        f_t &= \s(W_f x_t + U_f h_{t-1}) & \textrm{Forget Gate} \\
        o_t &= \s(W_o x_t + U_o h_{t-1}) & \textrm{Output gate} \\
        \tilde c_t &= \tanh(W_c x_t + U_c h_{t-1}) & \textrm{New Memory} \\
        c_t &= f_t \circ \tilde c_{t-1} + i_t \circ \tilde c_t & \textrm{Final Memory} \\
        h_t &= o_t \circ \tanh(c_t) & \textrm{Hidden State}
    \end{align*}
</p>

<p>
    The interpretation is as follows: the new input $x_t$ is combined with the
    old hidden state $h_{t-1}$ to create the new memory $\tilde c_t.$ Then the
    new memory $\tilde c_t$ is combined with the previous "new memory" $\tilde
    c_{t-1}$ to create the final memory $c_t.$ Question is why not use the
    previous final memory $c_{t-1}$ instead of $\tilde c_{t-1}$? I.e.

    $$c_t = f_t \circ c_{t-1} + i_t \circ \tilde c_t.$$
</p>

<p>
    Is $h_t$ like the short-term memory and $c_t$ the long-term memory?
</p>

<h1>Recursive Neural Networks (RecNN)</h1>

<h1>Dynamic Convolutional Neural Network</h1>

<p>
    Ref. A Convolutional Neural Network for Modelling Sentences. Nal
    Kalchbrenner, Edward Grefenstette, Phil Blunsom.
</p>

<h3>Word Embedding and Sentence Construction</h3>

Given an input sentence, let $w_i \in \bR^d$ be the $i$-th word in the sentence
and $s = [w_1, \ldots, w_s] \in \bR^{d\times s}$ be the sentence matrix.

<h3>Wide Convolution</h3>

<p>
    Let $m \in \bR^{d\times m}$ be a filter (weight) matrix. Then apply a wide
    convolution of $s$ with $m$:

    $$m * s = c \sim d \times (s + m - 1).$$
</p>

<h3>$k$-Max Pooling</h3>

<p>
    Apply static $k$-max pooling to the last convolutional layer before the fully
    connected layer. Every intermediate conv layer uses dynamic $k_{l}$-max pooling.
    A simple heuristic for $k_l$ is

    $$k_l = \max(k_{\rm top}, \lceil \frac{L - l}{L} s \rceil),$$

    where $k_{\rm top}$ is the $k$-max value at the top pooling layer just
    before FC, $L$ is the number of conv layer in the network that uses pooling,
    $l$ is the current pooling layer, and $s$ is the length of the input
    sentence.
</p>

<p>
    Using the heuristic, we can calculate

    $$k_0 = \max(k_{\rm top}, \lceil \frac{L - 0}{L} s \rceil) = s,$$

    assuming $s \geq k_{\rm top}.$ Therefore an output of a $k$-max operation
    in the first layer has dimension $d \times s.$
</p>

<h3>Nonlinear activation function</h3>

<p>
    Next apply a nonlinear activation function, e.g. sigmoid, tanh, or ReLU.
</p>

<h3>Multiple feature maps</h3>

<p>
    Denote the input sentence $s$ as $F^0.$ Instead of using only one filter $m$
    to produce only a single feature map $F^1$ in the first layer, we can use
    multiple filters $m^1_1, m^1_2, m^1_3,$ to produce multiple feature maps
    $F^1_1, F^1_2, F^1_3,$ say.
</p>
<p>
    For illustration, suppose we want 3 filters (and therefore 3 feature maps in
    the first layer), let's calculate the feature maps $F^1_1, F^1_2, F^1_3:$

    \begin{align*}
    F^1_1 &= m^1_1 * F^0 \\
    F^1_2 &= m^1_2 * F^0 \\
    F^1_3 &= m^1_3 * F^0. \\
    \end{align*}

    Then apply dynamic pooling and nonlinearity on these 3 feature maps
    separately to get

    \begin{align*}
    F^1_1 &= g \circ p (F^1_1) \\
    F^1_2 &= g \circ p (F^1_2) \\
    F^1_3 &= g \circ p (F^1_3). \\
    \end{align*}

    Next suppose we also want 3 feature maps in the second layer. Then we'd need 9 filters:

    \begin{matrix}
    m^2_{11} & m^2_{12} & m^2_{13} \\
    m^2_{21} & m^2_{22} & m^2_{23} \\
    m^2_{31} & m^2_{32} & m^2_{33}. \\
    \end{matrix}

    Then the feature maps are

    \begin{align*}
    F^2_1 &= m^2_{11} * F^1_1 + m^2_{12} * F^1_2 + m^2_{13} * F^1_3 \\
    F^2_2 &= m^2_{21} * F^1_1 + m^2_{22} * F^1_2 + m^2_{23} * F^1_3 \\
    F^2_3 &= m^2_{31} * F^1_1 + m^2_{32} * F^1_2 + m^2_{33} * F^1_3. \\
    \end{align*}

    To produce the final feature maps apply pooling and nonlinearity:

    \begin{align*}
    F^2_1 &= g \circ p (F^2_1) \\
    F^2_2 &= g \circ p (F^2_2) \\
    F^2_3 &= g \circ p (F^2_3). \\
    \end{align*}

    And so on for higher order features.
</p>
<p>
    <i>Question. Why do you need to sum up the wide convolutions of each of the
    lower layer features, instead of simply having one wide conv? Like this:</i>

    \begin{align*}
    F^2_1 &= m^2_1 * F^1_1 \\
    F^2_2 &= m^2_2 * F^1_2 \\
    F^2_3 &= m^2_3 * F^1_3, \\
    \end{align*}

    instead of this

    \begin{align*}
    F^2_1 &= m^2_{11} * F^1_1 + m^2_{12} * F^1_2 + m^2_{13} * F^1_3 \\
    F^2_2 &= m^2_{21} * F^1_1 + m^2_{22} * F^1_2 + m^2_{23} * F^1_3 \\
    F^2_3 &= m^2_{31} * F^1_1 + m^2_{32} * F^1_2 + m^2_{33} * F^1_3. \\
    \end{align*}

    Answer. Let's back paddle to the first layer. We produced 3 feature maps
    $F^1_1, F^1_2, F^1_3$ from a single input feature map $F^0.$ This represents
    first level features. Next, we'd like to learn higher level features,
    meaning how these lower level features combine and relate to each other.
    That's where the summing comes in. If we only had

    \begin{align*}
    F^2_1 &= m^2_1 * F^1_1 \\
    F^2_2 &= m^2_2 * F^1_2 \\
    F^2_3 &= m^2_3 * F^1_3, \\
    \end{align*}

    then the second level features would simply be learning from each
    corresponding lower level feature, e.g. $F^2_1$ would only re-represent
    information already present in $F^1_1,$ $F^2_2$ would only re-represent
    information already present in $F^1_2,$ and $F^2_3$ would only re-represent
    information already present in $F^1_3.$
</p>
<p>
    Instead what we'd like is for $F^2_1$ to learn how $F^1_1, F^1_2,$ and
    $F^1_3$ combine. And we want $F^2_2$ to learn another way they combine, and
    $F^2_3$ yet another.
</p>

<h3>Folding</h3>

<p>
    (Optionally) Apply folding between each wide conv and pooling layer to
    introduce dependencies among rows of feature maps.
</p>

<h3>Fully connected layer and softmax</h3>

<p>
    Finally join the last set of feature maps along columns, e.g.

    \begin{matrix}
    [F^z_1 & F^z_2 & F^z_3] \sim d \times ?
    \end{matrix}

    and run them through a fully connected layer, then softmax to predict the
    next word, and minimize cross entropy loss.
</p>

<h1>Autoencoders</h1>

<h2>Contractive autoencoder</h2>

<p>
    The contractive autoencoder adds a regularization term to the loss function:

    $$ L = || X - \hat X ||^2 + \l || J_h(X) ||^2_F, $$

    where $ || J_h(X) ||^2_F = \sum_{ij} \left(\frac{\dd h_j (X)}{\dd
    X_i}\right)^2 $ is the Frobenius inner product. Making the Jacobian small
    means that small changes in the input map to small changes in the hidden
    layer. How does this help?
</p>

<h1>Sequence to sequence model</h1>

<h1>Word Embedding</h1>


</div>
</body>
</html>