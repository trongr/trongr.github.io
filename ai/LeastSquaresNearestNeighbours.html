<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />
<title>A.I.</title>
<link rel="stylesheet" href="../css/global.css" />
<link rel="stylesheet" href="../css/obsidian.min.css" />
<script src="../js/highlight.min.js"></script>
<!-- this config must be before MathJax.js: -->
<script src="../js/mathjax.config.js"></script>
<script src="../js/MathJax/MathJax.js"></script>
<!--
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
-->
<script src="../js/jquery-3.1.0.min.js"></script>
<script src="../js/smartquotes.js"></script>
<script src="../js/global.js"></script>
</head>
<body>
<div id="content">
<h1>Statistics: Least Squares and Nearest Neighbours</h1>


<h1>Reference</h1>

<ol>
  <li>Elements of statistical learning</li>
</ol>

<h1>Linear models and least squares</h1>

<p>
  Given the input $ X^T = (X_1, \ldots, X_p), $ where each $ X_i $ is a real
  number, we want to predict the output $ Y $ via the model

  $$
    Y = \beta_0 + \sum_{j=1}^p X_j \beta_j.
  $$

  By including the constant $ 1 $ in the first entry of $ X, $ i.e.

  $$
    X^T = (1, X_1, \ldots, X_p)
  $$

  we can rewrite this as the inner product

  $$
  Y = X^T \beta.
  $$

  Slightly more generally, $ Y $ can be a $ k $-vector, in which case $ \b $
  will have to be a $ p\times k $ matrix (let's say because $ X $'s dimension is
  fixed, because it's data that is given to us):

  $$
  \begin{align*}
  Y^T &= X^T \beta \\
  (Y_1,\ldots, Y_k) &= (X_1, \ldots, X_p) \begin{bmatrix}
  \b_{11} & \cdots & \b_{1k} \\
  \vdots & & \vdots \\
  \b_{p1} & \cdots & \b_{pk} \\
  \end{bmatrix}.
  \end{align*}

  $$

</p>

<h1>Gradient and the direction of steepest ascent</h1>

<p>
  Suppose $ Y $ is just a real number for now. Then the function $ f(X) = X^T \b
  $ is linear from $ \bR^p \longrightarrow \bR, $  and its gradient $ f'(X) = \b
  $ is the vector in $ \bR^p $ that points in the direction of steepest ascent,
  i.e. the direction you go in in order to maximize the change in $ f. $
</p>

<h1>Hyperplanes</h1>

<p>
  The equation

  $$
  Y = \beta_0 + \sum_{j=1}^p X_j \beta_j = X^T \beta
  $$

  defines a "hyperplane" in the space $ \bR^p \times \bR, $ in the same way the
  linear equation

  $$
    y = a + bx
  $$

  defines a line in the space $ \bR \times \bR. $  By fitting a linear model $
  \b $ to the data $ X $ we mean that we want to find $ \b $ that will make $ Y
  $ satisfy some constraint, for example perhaps we want to minimize the
  residual sum of squares

  $$
    {\rm RSS} (\b) = \sum_{i=1}^N (y_i - x_i^T \b)^2.
  $$

  This is called the Method of Least Squares. Note that each $ x_i $ in this
  expression is like the vector $ X \in \bR^p, $ and each $ y_i $ is like the
  real number $ Y \in \bR, $ and that the vector $ \b $ is reused in each case $
  1..N. $ What's happening is that we're taking multiple values of $ X $'s and $
  Y $'s and we're finding a single $ \b $ that will minimize $ {\rm RSS}. $ The
  analogy in $ \bR^2 $ is that we're taking together many values of $ x_i $ and
  $ y_i, $ which define points in $ \bR^2, $ and we're trying to find a line
  through them that best fits them all, i.e. we're finding the best $ \b = (a,
  b) $ that defines the line $$ y = a + bx. $$
</p>

</div>
</body>
</html>
