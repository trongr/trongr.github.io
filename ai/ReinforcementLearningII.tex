% Converting a PDF into JPGs: gs -dFirstPage=26 -dLastPage=33 -dBATCH -dNOPAUSE -sDEVICE=jpeg -r666 -dUseCropBox -o images/OUT-%04d.jpg IN.pdf

% \documentclass[14pt]{extarticle}
\documentclass[20pt]{extarticle}
\usepackage[margin=1cm,paperwidth=7in,paperheight=7in]{geometry}

\usepackage{float}
\usepackage{graphicx}
\graphicspath{{images/}}

\usepackage{listings}
\lstset{basicstyle=\ttfamily, aboveskip=\bigskipamount, belowskip=\bigskipamount}

\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{epstopdf}
\usepackage{epigraph}
\usepackage{url}
\usepackage{mathtools}
\usepackage{pdfrender,xcolor}

\usepackage{hyperref}
\hypersetup{
  colorlinks,
  citecolor=red,
  filecolor=red,
  linkcolor=red,
  urlcolor=red
}

\usepackage{lmodern}
\usepackage[T1]{fontenc}

% Computer Concrete
% \usepackage{concmath}
% \usepackage[T1]{fontenc}

% Times variants
%
% \usepackage{mathptmx}
% \usepackage[T1]{fontenc}
%
% \usepackage[T1]{fontenc}
% \usepackage{stix}
%
% Needs to typeset using LuaLaTeX:
% \usepackage{unicode-math}
% \setmainfont{XITS}
% \setmathfont{XITS Math}

% garamond
% \usepackage[cmintegrals,cmbraces]{newtxmath}
% \usepackage{ebgaramond-maths}
% \usepackage[T1]{fontenc}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

% Redefine enumerate environment to have less spacing between items
\let\oldBeginEnumerate=\enumerate
\let\oldEndEnumerate=\endenumerate
\renewenvironment{enumerate}{
\oldBeginEnumerate
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\oldEndEnumerate}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{question}[theorem]{Question}
\newtheorem{definition}[theorem]{Definition}

\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{keywords}{Keywords}
\newtheorem{reference}{Reference}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{note}[theorem]{Note}

\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}

\DeclareMathOperator{\argmax}{argmax}

\newcommand{\bE}{\mathbf E}
\newcommand{\bN}{\mathbf N}
\newcommand{\bQ}{\mathbf Q}
\newcommand{\bR}{\mathbf R}

% Blackboard (hollow) math

\newcommand{\hE}{\mathbb E}
\newcommand{\hP}{\mathbb P}

% Greek and misc symbols

\newcommand{\0}{\varnothing}

\renewcommand{\a}{\alpha}
\renewcommand{\b}{\beta}
\newcommand{\e}{\varepsilon}
\newcommand{\f}{\varphi}
\newcommand{\g}{\gamma}
\newcommand{\m}{\mu}
\newcommand{\s}{\sigma}
\renewcommand{\th}{\theta}

\newcommand{\D}{\Delta}

% Delimiters

%\newcommand{\defeq}{\coloneqq}
\newcommand*{\defeq}{\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
           \hbox{\scriptsize.}\hbox{\scriptsize.}}}
           =}
\newcommand{\<}{\langle}
\renewcommand{\>}{\rangle}

\title{Reinforcement Learning}
\author{Trong}
\date{November 1, 2018---\today}

\begin{document}
\pdfrender{StrokeColor=black,TextRenderingMode=2,LineWidth=0.5pt}
\sloppy
\maketitle
\thispagestyle{empty}
\pagestyle{empty}
% \usepackage{hyperref} for hyper links in table of contents. Remember to
% compile twice to update table of contents
\tableofcontents

\section{DOOM: Health Gathering in Acid Lake}

Agent seems to converge to a specific behavior, e.g. if in the beginning it tends to turn left, it will gradually turn tighter and tighter corners until it's spinning in one place. I suspect this is a property of the loss function
\begin{align*}
  L &= - \frac{1}{n} \sum_{i} \< A, \log s(\pi) \> \cdot G \\
  G &= \frac{[G_0 \ \cdots \ G_n] - \m}{\s}
\end{align*}
and not the network architecture. This is because the loss function's main goal via the \( \< A, \log s(\pi) \> \) factor is to create actions that conform with previous behavior. Adding randomness in the loss function, e.g. in the way we generate the discounted rewards, seems to help, but that only delays the pathological behavior.

\break
\section{Actor Critic}

Kinda similar to GAN's. The Critic network measures the value of the actions, while the Actor network outputs actions. The Actor Critic Model updates parameters at each step instead of waiting for the entire episode to finish.

Recall the update rule
\[
\D \th = \a G_t \nabla_{\th} \ln \pi(a_t|s_t),
\]
where $ G $ is the reward function. In Actor Critic, we want the Critic to learn $ G $ instead of soft-coding it. The loss function then becomes: \[
\D \th = \a q(s_t, a_t) \nabla_{\th} \ln \pi(a_t|s_t),
\]
where $ q $ is the value of taking action $ a_t $ at $ s_t. $

In addition to updating $ \th $ for $ \pi $ during training, we must also independently train and update \[
\D w = \b (q_{\rm target} - q(s_t, a_t)) \nabla_{w} q(s_t, a_t),
\]
where \[
q_{\rm target} = R(s_t, a_t) + \g q(s_{t+1}, a_{t+1})
\]
is the target value we've seen before. The difference \[
q_{\rm target} - q(s_t, a_t)
\]
is also called the Temporal Difference Error.

\section{Advantage Function}

\begin{definition}
Define the Advantage Function to be \[
A(s, a) = Q(s, a) - V(s),
\]
where $ V(s) $ is the average value of state $ s, $ i.e. the average value of all actions at $ s. $ In other words, $ A(s, a) $ measures how well action $ a $ performs compared to all other actions at $ s. $
\end{definition}

Turns out that the Advantage Function can be approximated by the Temporal Difference Error we just saw: \begin{align*}
    A(s, a) &= Q(s, a) - V(s) \\
    &\approx R(s_t, a_t) + \g q(s_{t+1}, a_{t+1}) - q(s_t, a_t).
\end{align*}
The interpretation is that in a minibatch, on average \begin{align*}
    V(s) &\approx q(s_t, a_t),
\end{align*}
and
\begin{align*}
    Q(s, a) &\approx R(s_t, a_t) + \g q(s_{t+1}, a_{t+1}).
\end{align*}
IOW the value of taking action $ a $ at $ s $ is the sum of the immediate reward plus the discounted expected future reward.

\section{A2C and A3C}

In A3C / Asynchronous Advantage Actor Critic, multiple agents execute in parallel, each updating the global model asynchronously. In contrast, in A2C / Advantage Actor Critic, we wait until all the agents have finished calculating their gradients before averaging them and updating the network all at once.

\end{document}


