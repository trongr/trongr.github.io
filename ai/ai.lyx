#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "ccfonts" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
A.I.
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Section*
Simplified Softmax Loss for GAN
\end_layout

\begin_layout Paragraph*
\begin_inset Formula 
\[
\begin{aligned}Z & =\sum_{x\in B}e^{-\mu(x)}=\sum_{x\in B_{+}}e^{-D(x)}+\sum_{x\in B_{-}}e^{-D\circ G(x)}\\
L_{D} & =\sum_{x\in B_{+}}\frac{1}{|B_{+}|}D(x)+\log Z\\
L_{G} & =\sum_{x\in B_{-}}\frac{1}{|B_{-}|}D\circ G(x)+\log Z
\end{aligned}
\]

\end_inset


\end_layout

\begin_layout Standard

\emph on
QUESTION
\emph default
.
 What happens when D is very good at telling real and fake images apart?
\end_layout

\begin_layout Section*

\emph on
The cost function in linear regression by gradient descent is convex
\end_layout

\begin_layout Standard

\series bold
Proposition
\series default
.
\emph on
The cost function in linear regression by gradient descent is convex.
\end_layout

\begin_layout Standard

\emph on
Proof
\emph default
.
 In linear regression by gradient descent we have the cost function
\begin_inset Formula 
\[
J(\theta)=\frac{1}{2m}\sum_{k=1}^{m}(h_{\theta}(x^{(k)})-y^{(k)})^{2},
\]

\end_inset

where
\begin_inset Formula 
\[
h_{\theta}(x)=\theta^{T}x=\theta_{0}x_{0}+\theta_{1}x_{1}+\cdots+\theta_{n}x_{n}
\]

\end_inset

is the hypothesis function.
\end_layout

\begin_layout Standard
In order to show that J is convex we’re gonna use a well known fact from
 linear algebra that tells us whether a function J is convex or not by looking
 at its Hessian matrix of second order partial derivatives
\begin_inset Formula 
\[
H=\left[\frac{\partial^{2}J}{\partial\theta_{i}\partial\theta_{j}}\right].
\]

\end_inset

In particular it says that: A continuous, twice differentiable function
 of several variables is convex on a convex set if and only if its Hessian
 matrix is positive semidefinite on the interior of the convex set.
\end_layout

\begin_layout Standard
[https://en.wikipedia.org/wiki/Convex_function]
\end_layout

\begin_layout Standard
By definition H is positive semidefinite iff for any nonzero
\begin_inset Formula $\theta,$
\end_inset

 we have
\begin_inset Formula 
\[
\theta^{T}H\theta\geq0.
\]

\end_inset


\end_layout

\begin_layout Standard
[https://en.wikipedia.org/wiki/Positive-definite_matrix]
\end_layout

\begin_layout Standard
So let’s see what H looks like by calculating the first partial derivatives
 of J:
\begin_inset Formula 
\[
\frac{\partial J}{\partial\theta_{j}}=\frac{1}{m}\sum_{k}(h_{\theta}(x^{(k)})-y^{(k)})x_{j}^{(k)},
\]

\end_inset

and the seconds:
\begin_inset Formula 
\[
H_{ij}=\frac{\partial^{2}J}{\partial\theta_{i}\partial\theta_{j}}=\frac{1}{m}\sum_{k}x_{i}^{(k)}x_{j}^{(k)}.
\]

\end_inset


\end_layout

\begin_layout Standard
Looking at the formula for
\begin_inset Formula $H_{ij},$
\end_inset

 we notice that H is precisely
\begin_inset Formula $\frac{1}{m}X^{T}X,$
\end_inset

 where
\begin_inset Formula $X=[x_{i}^{(k)}]$
\end_inset

 is the matrix of samples k by features i.
 Using H in matrix form makes it a lot easier to see that it’s positive
 semidefinite: note that
\begin_inset Formula $\theta^{T}H\theta$
\end_inset

 is just
\begin_inset Formula 
\[
\frac{1}{m}\theta^{T}X^{T}X\theta=\frac{1}{m}(X\theta)^{T}X\theta=\frac{1}{m}|X\theta|^{2}\geq0,
\]

\end_inset

so H is positive semidefinite and J is convex.
\end_layout

\begin_layout Standard
And finally the reason we care about convexity is that it guarantees we
 can always find a global minimum for the cost function in linear regression.
\end_layout

\begin_layout Section*
Encoding and triggering a GAN
\end_layout

\begin_layout Standard
TODO.
 Would be interesting to see if different runs cause the generator to prefer
 different digits.
 That
\emph on
would
\emph default
be interesting, but not very useful because we want consistency between
 runs, so that we can encode different digits on the random distribution
\begin_inset Formula $z.$
\end_inset


\end_layout

\begin_layout Section*
Enter the Matrix / Live coding in the real world
\end_layout

\begin_layout Standard
LyX is quite enlightened in its philosophy of bridging / removing the gap
 between model (LaTeX code) and view (math), or representation and reality.
 This is a much more natural way of working than having to type and read
 LaTeX code, because the working mathematician doesn't care about LaTeX:
 what she cares about is the final math expressions.
\end_layout

\begin_layout Standard
Perhaps we can apply this principle to other fields, e.g.
 in programming, where the code you write is very different / removed from
 the final product.
 Imagine a time when instead of having to go into an editor to write your
 code, compile, deploy, and watch it appear out there in the real world,
 you can edit your code directly in the real world (a sandbox environment
 simulating the real world) and watch it change instantly: where the real
 world is
\emph on
 made
\emph default
 of code.
 We already have something like this in modern browsers, where you can click
 on an element and change its styles, or dynamically inject JavaScript into
 a live page.
 It's not quite so far fetched.
\end_layout

\begin_layout Section
GAN Generator noise dimension
\end_layout

\begin_layout Standard
Can't tell the difference when lowering the random noise input dimension
 into a generator from 96 to 32.
\end_layout

\begin_layout Section
Wasserstein GANs
\end_layout

\begin_layout Standard
Wasserstein / Kantorovich-Monge-Rubinstein / Earth mover distance.
\end_layout

\begin_layout Standard
WGAN-GP (Gradient Penalty)
\end_layout

\begin_layout Section
Fully Connected VS Convolutional Neural NetworksT20
\end_layout

\begin_layout Section
Generator power and modal collapse
\end_layout

\begin_layout Standard

\series bold
Question.
\series default
\emph on
Are fully connected and convolutional networks equivalent? Specifically
 can an FC network learn anything a conv network can, given enough training
 time and layers?
\end_layout

\begin_layout Section
The AI problem
\end_layout

\begin_layout Standard
The central problem in AI is how to build small, dumb AI's that become smarter
 when combined with others of their kind.
 Put another way, how to create AI's that individually perform simple tasks,
 and when combined performs arbitrarily complex tasks, such that the more
 AI agents we have the more complex the tasks they can perform.
\end_layout

\begin_layout Section

\emph on
A Super Neural Network / Neural Super Network? or just Super Network
\end_layout

\begin_layout Standard
A network made of many smaller networks and the structure of the whole network
 is learned, e.g.
\emph on
which neuron talks to which neuron,
\emph default
 how they talk, how much data to pass back and forth, when, etc.
\end_layout

\begin_layout Standard

\emph on
Conjecture
\emph default
.

\emph on
 The more powerful a generator is, the more chance of modal collapse.
\end_layout

\begin_layout Standard

\emph on
Question.

\emph default
 Powerful in what sense specifically?
\end_layout

\begin_layout Section
GAN as mode evolution
\end_layout

\begin_layout Standard

\emph on
Network
\emph default
.
 Convolutional (integer, non-fractional stride) generator with convolutional
 (also integer) discriminator and simplified softmax.
\end_layout

\begin_layout Standard
(The generator has an FC layer at the end.
 Without the last FC layer we would have to somehow merge the color channels
 into one, i.e.
\begin_inset Formula $(N,28,28,C)\longrightarrow(N,28,28,1)$
\end_inset

, e.g.
 by averaging, and the generated images would have greater grayscale variation
 instead of more black and white, at least in the beginning, as we saw before,
 and the digits would be more wispy.)
\end_layout

\begin_layout Standard

\emph on
Conjecture.
\emph default
Whatever digits the network happens to generate in the beginning, it’ll
 tend to learn and generate digits most similar to those.
 E.g.
 in the beginning the generator settles on 9-looking images, and it seems
 later digits “evolved” from that, e.g.
 4, 7, 8, 1, 2, in order of how closely they resemble ones that went before.
 In this case it's easy to transform the rough-looking 9 into 4 than into
 1.
 It only starts learning 1's after having gone through 7.
 Kinda makes sense.
\end_layout

\begin_layout Standard

\emph on
Conjecture: existence of attractors.
\emph default
Once the network starts generating 1's, they'll take over, because it's
 easier to turn other digits into 1's than to turn 1's into other digits.
 E.g.
 a narrow 9 looks a lot like a 1, a narrow 8 looks like a 1, a badly written
 1 looks like a 1, a badly written 7 looks like a 1.
 One's are like black holes.
\end_layout

\begin_layout Standard

\emph on
Conjecture.
\emph default
Some digits never get generated.
 There might not be any clear path from the existing digits to the missing
 ones, e.g.
 it's hard to go to 6 from the other digits.
\end_layout

\begin_layout Standard

\emph on
Question.
\emph default
How do you stop that from happening?
\end_layout

\begin_layout Standard
One solution might be to train multiple generators, each learning a few
 digits, that together generate all the digits.
 Drop out?
\end_layout

\begin_layout Section
Modal collapse is natural
\end_layout

\begin_layout Standard
It's not really surprising that a GAN collapses into modes: that happens
 in real life when humans learn (to generate) things too: we focus on the
 things we're good at learning / generating, and ignore the vast majority
 of other things.
\end_layout

\begin_layout Section
Adding more randomness to the network to perturb the stagnant
\begin_inset Quotes eld
\end_inset

gene pool
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard

\emph on
Question.
\emph default
How do you do that exactly?
\end_layout

\begin_layout Section
Having some trouble scaling
\end_layout

\begin_layout Standard
the network from MNIST to CelebA, cause the images are a lot bigger and
 the network doesn't seem to learn.
 Keeps generating black images.
\end_layout

\begin_layout Section
Cooperative Networks and emergence
\end_layout

\begin_layout Standard

\emph on
Question.
\emph default
How do you make networks that cooperate? How do you compose networks together?
 More generally, how do you compose simple things to make complex things?
\end_layout

\begin_layout Section
TODO.
 GAN Generalization
\end_layout

\begin_layout Standard
More than two networks, each contributing to none(?), one, or more loss
 functions.
\end_layout

\begin_layout Standard
Training a GAN to generate faces is not unlike Necromancy.
 Some of these faces are terrifying.
\end_layout

\begin_layout Section
Machine and Human Learning
\end_layout

\begin_layout Standard
In two-player games, e.g.
 Ping Pong, Chess, Generative Adversarial Networks, etc., you won't be able
 to improve as well if your opponent is much better than you and plays without
 mercy.
 The most learning occurs for both players when they are more or less evenly
 matched; then they can learn and improve together in tandem, like two feet
 walking.
\end_layout

\begin_layout Section
Teacher and Student Ensembles
\end_layout

\begin_layout Standard
Applying human learning to machine learning, we might create a network composed
 of many teachers / discriminators and many students / generators.
 The teachers introduce a variety of subjects, or things to be learned,
 and the students can come up with a variety of solutions.
\end_layout

\begin_layout Standard

\series bold
Conjecture.

\series default
\emph on
 Teacher and student ensembles perform better than a single all-knowing
 teacher and all-talented student.
 Architecturally it also scales better, because we can have lots of smaller
 teacher and student networks instead of a single monolithic system, e.g.
 teachers can take sabbaticals and a substitute can replace them, and so
 on.
\end_layout

\begin_layout Section
I need a bigger machine
\end_layout

\begin_layout Section
Asking questions and coming up with insights
\end_layout

\begin_layout Standard
The habit of constantly asking questions teaches you to ask better questions,
 and the habit of coming up with insights teaches to you to come up with
 better insights.
 And, along the way, the little questions and insights add up.
\end_layout

\begin_layout Section
Convolutional layers VS FC layers
\end_layout

\begin_layout Standard
Earlier I made a .
 .
 .
\end_layout

\begin_layout Quote

\emph on
Conjecture.

\emph default
 Convolutional layers are highly sensitive to local changes in space, which
 is the reason they're used in image recognition tasks to detect local patterns.
 I suspect that
\emph on
 they're also sensitive to local changes in time, in the sense that they're
 easily influenced by the most recently seen training examples.

\emph default
 E.g.
 in a GAN discriminator made of convolutional layers, at least early on
 in the training (and much later on as well), generated images in each batch
 are almost identical to each other, and very different from those from
 even the previous batches.
 This suggests that the training images in the current batch are enough
 to force the entire convolutional network to 
\begin_inset Quotes eld
\end_inset

change its mind.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
I think this is wrong, at least for the much later on part.
 I think generators will tend to converge towards adversarial examples,
 and as soon as the discriminiator learns those examples, G changes to another
 set of adversarial examples.
\end_layout

\begin_layout Section
Behold! Shallow Conv Nets
\end_layout

\begin_layout Standard
If your conv net isn't deep enough, it will learn local features but fail
 to learn how they combine together.
 The consequence is it'll put those local patterns in the wrong places,
 e.g.
 when training a discriminator on faces with two conv layers, the generator
 will put eyes all over the face.
\end_layout

\begin_layout Standard
Adding FC layers after those conv layers should help prevent that.
 Do you need more FC layers the more conv layers you have in front? How
 about interleaving conv and FC layers?
\end_layout

\begin_layout Section
Generative models are powerful
\end_layout

\begin_layout Standard
because if you can build a machine that can generate simple things like
 text, images, music, etc.
 very soon it'll be able to come up with more complicated ideas, e.g.
 novels, math, how to navigate a city, how to grow and improve itself, i.e.
 a machine that can think (compute at a deeper level).
\end_layout

\begin_layout Section
How can we make machines that think at a deeper level?
\end_layout

\begin_layout Standard
I think it's (simply!) by 
\emph on
adding more connections and enabling more complex interactions to occur
 more quickly among those connections.

\emph default
 If we give the machine more ability to twist and 
\begin_inset Quotes eld
\end_inset

wrap
\begin_inset Quotes erd
\end_inset

 its mind around ideas, it will naturally do it.
 The problem then becomes scale (space) and speed (time), without compromising
 either.
\end_layout

\begin_layout Section
GAN: the more powerful a model is, the more likely modal collapse will happen
\end_layout

\begin_layout Standard
E.g.
 a conv GAN will converge faster and collapse faster than a GAN made of
 only FC layers.
\end_layout

\begin_layout Quote

\emph on
In programming you don't understand things: you just use them.
\end_layout

\begin_layout Section
Been having trouble with GANs running out of MEM
\end_layout

\begin_layout Standard
last few days moving from MNIST to CelebA.
 Turns out was pretty easy to solve.
 Moral is don't have FC layers mapping 218 * 178 * 3 = 100K units to 100K
 other units.
 That's 100K * 100K = 10M neurons.
 With float32 that's 10GB of MEM! You need to reduce the input sizes before
 feeding them into FC layers.
 Laptop crashed several times trying to allocate memory.
\end_layout

\begin_layout Section
Conv networks and entropy?
\end_layout

\begin_layout Standard
Conv Generator, conv Discriminator, and softmax loss.
 G and D also have ending FC layers.
 Much cleaner images than an FC network with similar number of layers.
 However, after about 30K iterations, the network starts losing control
 and the colors start bleeding.
 Don't know why yet, but I expect that a more powerful / deeper network
 will be able to sustain its resolution longer.
\end_layout

\begin_layout Section
Generative Model Learning from Experience
\end_layout

\begin_layout Standard

\series bold
Experiment.

\series default
\emph on
 Something we can try is to feed the output of the generator back into part
 of its input, so that say half the entries of 
\begin_inset Formula $z$
\end_inset

 are drawn randomly and the other half are the generated images from the
 last batch.
 
\emph default
I have no idea what this will do but it might give some interesting behavior
 around seeding the generator with images you want it to create....
 This doesn't work so good.
\end_layout

\begin_layout Standard

\series bold
Experiment.
 
\series default
\emph on
Another thing to try is to use an RNN to generate the images instead of
 a convolutional or fully connected network.
 TODO.
\end_layout

\begin_layout Section
Generative Adversarial Networks are .
 .
 .
 well, adversarial
\end_layout

\begin_layout Standard
I've just realized something: 
\end_layout

\begin_layout Standard

\emph on
Conjecture.
 
\emph default
The saturated images we saw previously (when you train a GAN for a long
 time) is not due to the generator 
\emph on
failing
\emph default
 to learn: it's because the generator learned so well that it's coming up
 with adversarial examples to fool the discriminator.
 
\end_layout

\begin_layout Standard
We've seen this happening before, where you can trick a convolutional network
 trained to classify images by tweaking the input image.
 Here the discriminator is just a binary classifier, and we have a generator
 trained to fool it! The problem is in the objective function and the discrimina
tor: 
\emph on
the generator's job is not to come up with photorealistic faces; its job
 is to trick the discriminator into 
\emph default
thinking 
\emph on
they're photorealistic faces, whether they are or not.
 
\emph default
That's a subtle but big difference.
\end_layout

\begin_layout Standard
There're two problems here, but actually just one:
\end_layout

\begin_layout Enumerate
We don't know how to create an objective function that tells the generator
 to create realistic faces.
 We don't know what a realistic face means.
 The best we can do is to train a discriminator to decide for us what is
 and isn't a realistic face.
 That's what a realistic face is, is whatever the discriminator says.
 Which leads us to the real problem:
\end_layout

\begin_layout Enumerate
The discriminator is vulnerable to adversarial attacks.
 Once we fix that the generator shoulcond be able to learn to create more
 reasonable pictures.
\end_layout

\begin_layout Section
When you change the objective function, you change the game
\end_layout

\begin_layout Section
Recursive GANs don't work so well
\end_layout

\begin_layout Address
Tried feeding the generated images back to the front of the generator, worked
 OK the first epoch, but fails after that.
 Also tried feeding it near the end of the generator, but fails right off
 the bat.
 Let's put this one on the back burner for now.
\end_layout

\begin_layout Section
Jensen-Shannon divergence & Kullback–Leibler divergence
\end_layout

\begin_layout Standard
Ah so this is similar to the earth mover metric in that it lets you measure
 the distance between two probability distributions.
 
\end_layout

\begin_layout Standard
Related: Kullback–Leibler divergence:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
D_{\text{KL}}(P\|Q)=-\sum_{i}P(i)\log\frac{Q(i)}{P(i)}=\sum_{i}P(i)\log\frac{P(i)}{Q(i)}.
\]

\end_inset

For distributions P and Q of a continuous random variable,
\begin_inset Formula 
\[
D_{\text{KL}}(P\|Q)=\int_{-\infty}^{\infty}p(x)\log\frac{p(x)}{q(x)}dx.
\]

\end_inset


\emph on
Question.
 
\emph default
What does this mean and How does it relate to the Shannon Entropy
\begin_inset Formula 
\[
\text{H}=-\sum_{i}p_{i}\ln p_{i}.
\]

\end_inset


\end_layout

\begin_layout Standard
We have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
D_{\text{KL}}(P\|Q) & =\sum_{i}P(i)\log\frac{P(i)}{Q(i)}\\
 & =-\sum_{i}P(i)\log Q(i)+\sum_{i}P(i)\log P(i)\\
 & =H(P,Q)-H(P)
\end{align*}

\end_inset

where 
\begin_inset Formula $H(P,Q)$
\end_inset

 is the cross entropy of P and Q.
 Whoa, it's this guy.
 But what does it mean? TODO.
\end_layout

\begin_layout Subsection

\emph on
Conditional entropy
\end_layout

\begin_layout Standard
TODO.
 There is also the related quantity called the 
\emph on
conditional entropy
\emph default
 of two events X and Y:
\begin_inset Formula 
\[
\text{H}(X|Y)=-\sum_{i,j}p\left(x_{i},y_{j}\right)\log\frac{p\left(x_{i},y_{j}\right)}{p\left(y_{j}\right)},
\]

\end_inset

representing the amount of randomness in the random variable X given the
 event Y.
\end_layout

\begin_layout Section
Shannon Entropy
\end_layout

\begin_layout Standard
Entropy is defined as the expected value of the amount of information contained
 in an event.
 It's the mean of the log probabilities of the component events, weighted
 by the probabilities of the events:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\text{H}(X)=\mathbb{E}[\text{I}(X)]=\mathbb{E}[-\ln(P(X))]=\sum_{i=1}^{n}P\left(x_{i}\right)\text{I}\left(x_{i}\right)=-\sum_{i=1}^{n}P\left(x_{i}\right)\ln P\left(x_{i}\right).
\]

\end_inset

So this is the reason AI people like to use log probabilities!
\end_layout

\begin_layout Standard

\emph on
These are Times of High Entropy.
 LOL
\end_layout

\begin_layout Section
Shannon's Theorem and why compressing an already compressed file makes it
 bigger than the original
\end_layout

\begin_layout Quote

\emph on
However, the problem can still arise even in everyday use when applying
 a compression algorithm to already compressed data: for example, making
 a ZIP file of music, pictures or videos that are already in a compressed
 format such as FLAC, MP3, WebM, AAC, PNG or JPEG will generally result
 in a ZIP file that is slightly larger than the source file(s).
\end_layout

\begin_layout Section
Information
\end_layout

\begin_layout Standard
Properties of Information:
\end_layout

\begin_layout Enumerate
An increase in the probability of an event decreases the information from
 an observed event, and vice versa: 
\begin_inset Formula $I(p)$
\end_inset

 is monotonically decreasing in 
\begin_inset Formula $p.$
\end_inset

 (More information is contained in a message about an unlikely event.
 This is why fake news go viral more often than real news: people like to
 relay messages that contain more information to each other.
 It's only natural.)
\end_layout

\begin_layout Enumerate
Information is a non-negative quantity: 
\begin_inset Formula $I(p)\geq0.$
\end_inset

 
\end_layout

\begin_layout Enumerate
Events that always occur do not communicate information: 
\begin_inset Formula $I(1)=0.$
\end_inset


\end_layout

\begin_layout Enumerate
Information due to independent events is additive: 
\begin_inset Formula $I\left(p_{1}p_{2}\right)=I\left(p_{1}\right)+I\left(p\right).$
\end_inset

 
\end_layout

\begin_layout Standard
Define 
\begin_inset Formula 
\[
I(p)=\log(1/p)=-\log(p).
\]

\end_inset


\end_layout

\end_body
\end_document
