#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
AI
\end_layout

\begin_layout Author
Trong
\end_layout

\begin_layout Date
2018
\end_layout

\begin_layout Part
February
\end_layout

\begin_layout Section*
S180217.
 Simplified Softmax Loss for GAN
\end_layout

\begin_layout Paragraph*
\begin_inset Formula 
\[
\begin{aligned}Z & =\sum_{x\in B}e^{-\mu(x)}=\sum_{x\in B_{+}}e^{-D(x)}+\sum_{x\in B_{-}}e^{-D\circ G(x)}\\
L_{D} & =\sum_{x\in B_{+}}\frac{1}{|B_{+}|}D(x)+\log Z\\
L_{G} & =\sum_{x\in B_{-}}\frac{1}{|B_{-}|}D\circ G(x)+\log Z
\end{aligned}
\]

\end_inset


\end_layout

\begin_layout Standard

\emph on
QUESTION
\emph default
.
 What happens when D is very good at telling real and fake images apart?
\end_layout

\begin_layout Section*
O180218
\emph on
.
 The cost function in linear regression by gradient descent is convex
\end_layout

\begin_layout Standard

\series bold
Proposition
\series default
.
 
\emph on
The cost function in linear regression by gradient descent is convex.
\end_layout

\begin_layout Standard

\emph on
Proof
\emph default
.
 In linear regression by gradient descent we have the cost function 
\begin_inset Formula 
\[
J(\theta)=\frac{1}{2m}\sum_{k=1}^{m}(h_{\theta}(x^{(k)})-y^{(k)})^{2},
\]

\end_inset

where 
\begin_inset Formula 
\[
h_{\theta}(x)=\theta^{T}x=\theta_{0}x_{0}+\theta_{1}x_{1}+\cdots+\theta_{n}x_{n}
\]

\end_inset

is the hypothesis function.
\end_layout

\begin_layout Standard
In order to show that J is convex we’re gonna use a well known fact from
 linear algebra that tells us whether a function J is convex or not by looking
 at its Hessian matrix of second order partial derivatives 
\begin_inset Formula 
\[
H=\left[\frac{\partial^{2}J}{\partial\theta_{i}\partial\theta_{j}}\right].
\]

\end_inset

In particular it says that: A continuous, twice differentiable function
 of several variables is convex on a convex set if and only if its Hessian
 matrix is positive semidefinite on the interior of the convex set.
 
\end_layout

\begin_layout Standard
[https://en.wikipedia.org/wiki/Convex_function]
\end_layout

\begin_layout Standard
By definition H is positive semidefinite iff for any nonzero 
\begin_inset Formula $\theta,$
\end_inset

 we have 
\begin_inset Formula 
\[
\theta^{T}H\theta\geq0.
\]

\end_inset


\end_layout

\begin_layout Standard
[https://en.wikipedia.org/wiki/Positive-definite_matrix]
\end_layout

\begin_layout Standard
So let’s see what H looks like by calculating the first partial derivatives
 of J: 
\begin_inset Formula 
\[
\frac{\partial J}{\partial\theta_{j}}=\frac{1}{m}\sum_{k}(h_{\theta}(x^{(k)})-y^{(k)})x_{j}^{(k)},
\]

\end_inset

and the seconds: 
\begin_inset Formula 
\[
H_{ij}=\frac{\partial^{2}J}{\partial\theta_{i}\partial\theta_{j}}=\frac{1}{m}\sum_{k}x_{i}^{(k)}x_{j}^{(k)}.
\]

\end_inset


\end_layout

\begin_layout Standard
Looking at the formula for 
\begin_inset Formula $H_{ij},$
\end_inset

 we notice that H is precisely 
\begin_inset Formula $\frac{1}{m}X^{T}X,$
\end_inset

 where 
\begin_inset Formula $X=[x_{i}^{(k)}]$
\end_inset

 is the matrix of samples k by features i.
 Using H in matrix form makes it a lot easier to see that it’s positive
 semidefinite: note that 
\begin_inset Formula $\theta^{T}H\theta$
\end_inset

 is just 
\begin_inset Formula 
\[
\frac{1}{m}\theta^{T}X^{T}X\theta=\frac{1}{m}(X\theta)^{T}X\theta=\frac{1}{m}|X\theta|^{2}\geq0,
\]

\end_inset

so H is positive semidefinite and J is convex.
\end_layout

\begin_layout Standard
And finally the reason we care about convexity is that it guarantees we
 can always find a global minimum for the cost function in linear regression.
 
\end_layout

\begin_layout Section*
M180219.
 Encoding and triggering a GAN
\end_layout

\begin_layout Standard
TODO.
 Would be interesting to see if different runs cause the generator to prefer
 different digits.
 That 
\emph on
would 
\emph default
be interesting, but not very useful because we want consistency between
 runs, so that we can encode different digits on the random distribution
 
\begin_inset Formula $z.$
\end_inset


\end_layout

\begin_layout Section*
Enter the Matrix / Live coding in the real world
\end_layout

\begin_layout Standard
LyX is quite enlightened in its philosophy of bridging / removing the gap
 between model (LaTeX code) and view (math), or representation and reality.
 This is a much more natural way of working than having to type and read
 LaTeX code, because the working mathematician doesn't care about LaTeX:
 what she cares about is the final math expressions.
 
\end_layout

\begin_layout Standard
Perhaps we can apply this principle to other fields, e.g.
 in programming, where the code you write is very different / removed from
 the final product.
 Imagine a time when instead of having to go into an editor to write your
 code, compile, deploy, and watch it appear out there in the real world,
 you can edit your code directly in the real world (a sandbox environment
 simulating the real world) and watch it change instantly: where the real
 world is
\emph on
 made
\emph default
 of code.
 We already have something like this in modern browsers, where you can click
 on an element and change its styles, or dynamically inject JavaScript into
 a live page.
 It's not quite so far fetched.
\end_layout

\begin_layout Section
W21.
 GAN Generator noise dimension
\end_layout

\begin_layout Standard
Can't tell the difference when lowering the random noise input dimension
 into a generator from 96 to 32.
\end_layout

\begin_layout Section
Wasserstein GANs
\end_layout

\begin_layout Standard
Wasserstein / Kantorovich-Monge-Rubinstein / Earth mover distance.
\end_layout

\begin_layout Standard
WGAN-GP (Gradient Penalty)
\end_layout

\begin_layout Section
R22.
 Fully Connected VS Convolutional Neural NetworksT20 
\end_layout

\begin_layout Section
Generator power and modal collapse
\end_layout

\begin_layout Standard

\series bold
Question.
 
\series default
\emph on
Are fully connected and convolutional networks equivalent? Specifically
 can an FC network learn anything a conv network can, given enough training
 time and layers?
\end_layout

\begin_layout Section
The Neuralist
\end_layout

\begin_layout Section
The AI problem
\end_layout

\begin_layout Standard
The central problem in AI is how to build small, dumb AI's that become smarter
 when combined with others of their kind.
 Put another way, how to create AI's that individually perform simple tasks,
 and when combined performs arbitrarily complex tasks, such that the more
 AI agents we have the more complex the tasks they can perform.
\end_layout

\begin_layout Section

\emph on
A Super Neural Network / Neural Super Network? or just Super Network
\end_layout

\begin_layout Standard
A network made of many smaller networks and the structure of the whole network
 is learned, e.g.
 
\emph on
which neuron talks to which neuron,
\emph default
 how they talk, how much data to pass back and forth, when, etc.Conjecture.
 
\emph on
The more powerful a generator is, the more chance of modal collapse.
\end_layout

\begin_layout Standard

\emph on
Question.
 
\emph default
Powerful in what sense specifically?
\end_layout

\begin_layout Section
W28.
 GAN as mode evolution
\end_layout

\begin_layout Standard

\emph on
Network
\emph default
.
 Convolutional (integer, non-fractional stride) generator with convolutional
 (also integer) discriminator and simplified softmax.
 
\end_layout

\begin_layout Standard
(The generator has an FC layer at the end.
 Without the last FC layer we would have to somehow merge the color channels
 into one, i.e.
 
\begin_inset Formula $(N,28,28,C)\longrightarrow(N,28,28,1)$
\end_inset

, e.g.
 by averaging, and the generated images would have greater grayscale variation
 instead of more black and white, at least in the beginning, as we saw before,
 and the digits would be more wispy.)
\end_layout

\begin_layout Standard

\emph on
Conjecture.
 
\emph default
Whatever digits the network happens to generate in the beginning, it’ll
 tend to learn and generate digits most similar to those.
 E.g.
 in the beginning the generator settles on 9-looking images, and it seems
 later digits “evolved” from that, e.g.
 4, 7, 8, 1, 2, in order of how closely they resemble ones that went before.
 In this case it's easy to transform the rough-looking 9 into 4 than into
 1.
 It only starts learning 1's after having gone through 7.
 Kinda makes sense.
\end_layout

\begin_layout Standard

\emph on
Conjecture: existence of attractors.
 
\emph default
Once the network starts generating 1's, they'll take over, because it's
 easier to turn other digits into 1's than to turn 1's into other digits.
 E.g.
 a narrow 9 looks a lot like a 1, a narrow 8 looks like a 1, a badly written
 1 looks like a 1, a badly written 7 looks like a 1.
 One's are like black holes.
\end_layout

\begin_layout Standard

\emph on
Conjecture.
 
\emph default
Some digits never get generated.
 There might not be any clear path from the existing digits to the missing
 ones, e.g.
 it's hard to go to 6 from the other digits.
\end_layout

\begin_layout Standard

\emph on
Question.
 
\emph default
How do you stop that from happening?
\end_layout

\begin_layout Standard
One solution might be to train multiple generators, each learning a few
 digits, that together generate all the digits.
 Drop out?
\end_layout

\end_body
\end_document
